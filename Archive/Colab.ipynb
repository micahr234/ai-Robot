{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Run.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micahr123/continuous_control/blob/master/Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hiQG_ASGHPF",
        "colab_type": "code",
        "outputId": "3637b47c-8c2d-4ff5-e509-71b512993222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
        "!sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget http://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb\n",
        "!sudo dpkg -i cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb\n",
        "!sudo apt-key add /var/cuda-repo-10-2-local-10.2.89-440.33.01/7fa2af80.pub\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install cuda"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-14 10:31:59--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 192.229.232.112, 2606:2800:247:2063:46e:21d:825:102e\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|192.229.232.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 56 [application/octet-stream]\n",
            "Saving to: ‘cuda-ubuntu1804.pin’\n",
            "\n",
            "\rcuda-ubuntu1804.pin   0%[                    ]       0  --.-KB/s               \rcuda-ubuntu1804.pin 100%[===================>]      56  --.-KB/s    in 0s      \n",
            "\n",
            "2020-02-14 10:31:59 (2.11 MB/s) - ‘cuda-ubuntu1804.pin’ saved [56/56]\n",
            "\n",
            "--2020-02-14 10:32:05--  http://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 192.229.232.112, 2606:2800:247:2063:46e:21d:825:102e\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|192.229.232.112|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1896270068 (1.8G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb’\n",
            "\n",
            "cuda-repo-ubuntu180 100%[===================>]   1.77G  49.6MB/s    in 16s     \n",
            "\n",
            "2020-02-14 10:32:21 (111 MB/s) - ‘cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb’ saved [1896270068/1896270068]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01.\n",
            "(Reading database ... 145113 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb ...\n",
            "Unpacking cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01 (1.0-1) ...\n",
            "Setting up cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01 (1.0-1) ...\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  InRelease\n",
            "Ign:1 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  InRelease\n",
            "Get:2 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  Release [574 B]\n",
            "Get:2 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  Release [574 B]\n",
            "Get:3 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  Release.gpg [833 B]\n",
            "Get:3 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  Release.gpg [833 B]\n",
            "Get:4 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  Packages [23.8 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:16 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:17 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [83.1 kB]\n",
            "Get:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [36.9 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [818 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,345 kB]\n",
            "Get:22 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,765 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [7,064 B]\n",
            "Get:24 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [817 kB]\n",
            "Get:25 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [27.5 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,104 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [11.1 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [41.2 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4,252 B]\n",
            "Get:30 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [852 kB]\n",
            "Fetched 7,204 kB in 8s (935 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  cuda-10-2 cuda-command-line-tools-10-2 cuda-compiler-10-2 cuda-cudart-10-2\n",
            "  cuda-cudart-dev-10-2 cuda-cufft-10-2 cuda-cufft-dev-10-2 cuda-cuobjdump-10-2\n",
            "  cuda-cupti-10-2 cuda-cupti-dev-10-2 cuda-curand-10-2 cuda-curand-dev-10-2\n",
            "  cuda-cusolver-10-2 cuda-cusolver-dev-10-2 cuda-cusparse-10-2\n",
            "  cuda-cusparse-dev-10-2 cuda-demo-suite-10-2 cuda-documentation-10-2\n",
            "  cuda-driver-dev-10-2 cuda-gdb-10-2 cuda-libraries-10-2\n",
            "  cuda-libraries-dev-10-2 cuda-license-10-2 cuda-memcheck-10-2\n",
            "  cuda-misc-headers-10-2 cuda-npp-10-2 cuda-npp-dev-10-2 cuda-nsight-10-2\n",
            "  cuda-nsight-compute-10-2 cuda-nsight-systems-10-2 cuda-nvcc-10-2\n",
            "  cuda-nvdisasm-10-2 cuda-nvgraph-10-2 cuda-nvgraph-dev-10-2 cuda-nvjpeg-10-2\n",
            "  cuda-nvjpeg-dev-10-2 cuda-nvml-dev-10-2 cuda-nvprof-10-2 cuda-nvprune-10-2\n",
            "  cuda-nvrtc-10-2 cuda-nvrtc-dev-10-2 cuda-nvtx-10-2 cuda-nvvp-10-2\n",
            "  cuda-runtime-10-2 cuda-samples-10-2 cuda-sanitizer-api-10-2\n",
            "  cuda-toolkit-10-2 cuda-tools-10-2 cuda-visual-tools-10-2 libcublas-dev\n",
            "  libcublas10\n",
            "The following NEW packages will be installed:\n",
            "  cuda cuda-10-2 cuda-command-line-tools-10-2 cuda-compiler-10-2\n",
            "  cuda-cudart-10-2 cuda-cudart-dev-10-2 cuda-cufft-10-2 cuda-cufft-dev-10-2\n",
            "  cuda-cuobjdump-10-2 cuda-cupti-10-2 cuda-cupti-dev-10-2 cuda-curand-10-2\n",
            "  cuda-curand-dev-10-2 cuda-cusolver-10-2 cuda-cusolver-dev-10-2\n",
            "  cuda-cusparse-10-2 cuda-cusparse-dev-10-2 cuda-demo-suite-10-2\n",
            "  cuda-documentation-10-2 cuda-driver-dev-10-2 cuda-gdb-10-2\n",
            "  cuda-libraries-10-2 cuda-libraries-dev-10-2 cuda-license-10-2\n",
            "  cuda-memcheck-10-2 cuda-misc-headers-10-2 cuda-npp-10-2 cuda-npp-dev-10-2\n",
            "  cuda-nsight-10-2 cuda-nsight-compute-10-2 cuda-nsight-systems-10-2\n",
            "  cuda-nvcc-10-2 cuda-nvdisasm-10-2 cuda-nvgraph-10-2 cuda-nvgraph-dev-10-2\n",
            "  cuda-nvjpeg-10-2 cuda-nvjpeg-dev-10-2 cuda-nvml-dev-10-2 cuda-nvprof-10-2\n",
            "  cuda-nvprune-10-2 cuda-nvrtc-10-2 cuda-nvrtc-dev-10-2 cuda-nvtx-10-2\n",
            "  cuda-nvvp-10-2 cuda-runtime-10-2 cuda-samples-10-2 cuda-sanitizer-api-10-2\n",
            "  cuda-toolkit-10-2 cuda-tools-10-2 cuda-visual-tools-10-2\n",
            "The following packages will be upgraded:\n",
            "  libcublas-dev libcublas10\n",
            "2 upgraded, 50 newly installed, 0 to remove and 89 not upgraded.\n",
            "Need to get 0 B/1,428 MB of archives.\n",
            "After this operation, 3,272 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-license-10-2 10.2.89-1 [16.4 kB]\n",
            "Get:2 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-misc-headers-10-2 10.2.89-1 [1,111 kB]\n",
            "Get:3 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nvcc-10-2 10.2.89-1 [37.4 MB]\n",
            "Get:4 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-cuobjdump-10-2 10.2.89-1 [88.5 kB]\n",
            "Get:5 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nvprune-10-2 10.2.89-1 [39.5 kB]\n",
            "Get:6 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-compiler-10-2 10.2.89-1 [2,530 B]\n",
            "Get:7 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nvdisasm-10-2 10.2.89-1 [22.2 MB]\n",
            "Get:8 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-gdb-10-2 10.2.89-1 [2,769 kB]\n",
            "Get:9 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nvprof-10-2 10.2.89-1 [1,651 kB]\n",
            "Get:10 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-sanitizer-api-10-2 10.2.89-1 [2,161 kB]\n",
            "Get:11 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-memcheck-10-2 10.2.89-1 [139 kB]\n",
            "Get:12 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-cudart-10-2 10.2.89-1 [111 kB]\n",
            "Get:13 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-driver-dev-10-2 10.2.89-1 [11.8 kB]\n",
            "Get:14 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-cudart-dev-10-2 10.2.89-1 [491 kB]\n",
            "Get:15 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-cupti-10-2 10.2.89-1 [8,169 kB]\n",
            "Get:16 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-cupti-dev-10-2 10.2.89-1 [2,197 kB]\n",
            "Get:17 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nvtx-10-2 10.2.89-1 [38.9 kB]\n",
            "Get:18 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-command-line-tools-10-2 10.2.89-1 [27.0 kB]\n",
            "Get:19 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nsight-10-2 10.2.89-1 [2,582 B]\n",
            "Get:20 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nvvp-10-2 10.2.89-1 [2,532 B]\n",
            "Get:21 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nvrtc-10-2 10.2.89-1 [6,413 kB]\n",
            "Get:22 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nvrtc-dev-10-2 10.2.89-1 [8,822 B]\n",
            "Get:23 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-cusolver-10-2 10.2.89-1 [85.6 MB]\n",
            "Get:24 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-cusolver-dev-10-2 10.2.89-1 [15.2 MB]\n",
            "Get:25 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  libcublas10 10.2.2.89-1 [42.2 MB]\n",
            "Get:26 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  libcublas-dev 10.2.2.89-1 [42.3 MB]\n",
            "Get:27 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-cufft-10-2 10.2.89-1 [87.8 MB]\n",
            "Get:28 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-cufft-dev-10-2 10.2.89-1 [164 MB]\n",
            "Get:29 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-curand-10-2 10.2.89-1 [38.9 MB]\n",
            "Get:30 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-curand-dev-10-2 10.2.89-1 [39.1 MB]\n",
            "Get:31 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-cusparse-10-2 10.2.89-1 [59.2 MB]\n",
            "Get:32 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-cusparse-dev-10-2 10.2.89-1 [59.7 MB]\n",
            "Get:33 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-npp-10-2 10.2.89-1 [56.7 MB]\n",
            "Get:34 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-npp-dev-10-2 10.2.89-1 [57.6 MB]\n",
            "Get:35 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nvml-dev-10-2 10.2.89-1 [53.8 kB]\n",
            "Get:36 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nvjpeg-10-2 10.2.89-1 [1,274 kB]\n",
            "Get:37 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nvjpeg-dev-10-2 10.2.89-1 [1,213 kB]\n",
            "Get:38 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nsight-compute-10-2 10.2.89-1 [3,712 B]\n",
            "Get:39 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nsight-systems-10-2 10.2.89-1 [3,130 B]\n",
            "Get:40 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nvgraph-10-2 10.2.89-1 [44.5 MB]\n",
            "Get:41 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-nvgraph-dev-10-2 10.2.89-1 [35.2 MB]\n",
            "Get:42 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-visual-tools-10-2 10.2.89-1 [389 MB]\n",
            "Get:43 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-tools-10-2 10.2.89-1 [2,496 B]\n",
            "Get:44 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-samples-10-2 10.2.89-1 [65.6 MB]\n",
            "Get:45 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-documentation-10-2 10.2.89-1 [54.1 MB]\n",
            "Get:46 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-libraries-dev-10-2 10.2.89-1 [2,614 B]\n",
            "Get:47 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-libraries-10-2 10.2.89-1 [2,584 B]\n",
            "Get:48 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-toolkit-10-2 10.2.89-1 [2,830 B]\n",
            "Get:49 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-runtime-10-2 10.2.89-1 [2,532 B]\n",
            "Get:50 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-demo-suite-10-2 10.2.89-1 [3,880 kB]\n",
            "Get:51 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda-10-2 10.2.89-1 [2,558 B]\n",
            "Get:52 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  cuda 10.2.89-1 [2,514 B]\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 52.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package cuda-license-10-2.\n",
            "(Reading database ... 145201 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-license-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-license-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-misc-headers-10-2.\n",
            "Preparing to unpack .../01-cuda-misc-headers-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-misc-headers-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvcc-10-2.\n",
            "Preparing to unpack .../02-cuda-nvcc-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvcc-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cuobjdump-10-2.\n",
            "Preparing to unpack .../03-cuda-cuobjdump-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cuobjdump-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvprune-10-2.\n",
            "Preparing to unpack .../04-cuda-nvprune-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvprune-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-compiler-10-2.\n",
            "Preparing to unpack .../05-cuda-compiler-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-compiler-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvdisasm-10-2.\n",
            "Preparing to unpack .../06-cuda-nvdisasm-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvdisasm-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-gdb-10-2.\n",
            "Preparing to unpack .../07-cuda-gdb-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-gdb-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvprof-10-2.\n",
            "Preparing to unpack .../08-cuda-nvprof-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvprof-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-sanitizer-api-10-2.\n",
            "Preparing to unpack .../09-cuda-sanitizer-api-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-sanitizer-api-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-memcheck-10-2.\n",
            "Preparing to unpack .../10-cuda-memcheck-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-memcheck-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cudart-10-2.\n",
            "Preparing to unpack .../11-cuda-cudart-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-10-2.\n",
            "Preparing to unpack .../12-cuda-driver-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-10-2.\n",
            "Preparing to unpack .../13-cuda-cudart-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cupti-10-2.\n",
            "Preparing to unpack .../14-cuda-cupti-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cupti-dev-10-2.\n",
            "Preparing to unpack .../15-cuda-cupti-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvtx-10-2.\n",
            "Preparing to unpack .../16-cuda-nvtx-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvtx-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-10-2.\n",
            "Preparing to unpack .../17-cuda-command-line-tools-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nsight-10-2.\n",
            "Preparing to unpack .../18-cuda-nsight-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvvp-10-2.\n",
            "Preparing to unpack .../19-cuda-nvvp-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvvp-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-10-2.\n",
            "Preparing to unpack .../20-cuda-nvrtc-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-10-2.\n",
            "Preparing to unpack .../21-cuda-nvrtc-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-10-2.\n",
            "Preparing to unpack .../22-cuda-cusolver-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-dev-10-2.\n",
            "Preparing to unpack .../23-cuda-cusolver-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-dev-10-2 (10.2.89-1) ...\n",
            "Preparing to unpack .../24-libcublas10_10.2.2.89-1_amd64.deb ...\n",
            "Unpacking libcublas10 (10.2.2.89-1) over (10.2.1.243-1) ...\n",
            "Preparing to unpack .../25-libcublas-dev_10.2.2.89-1_amd64.deb ...\n",
            "Unpacking libcublas-dev (10.2.2.89-1) over (10.2.1.243-1) ...\n",
            "Selecting previously unselected package cuda-cufft-10-2.\n",
            "Preparing to unpack .../26-cuda-cufft-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cufft-dev-10-2.\n",
            "Preparing to unpack .../27-cuda-cufft-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-curand-10-2.\n",
            "Preparing to unpack .../28-cuda-curand-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-curand-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-curand-dev-10-2.\n",
            "Preparing to unpack .../29-cuda-curand-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-curand-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-10-2.\n",
            "Preparing to unpack .../30-cuda-cusparse-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-dev-10-2.\n",
            "Preparing to unpack .../31-cuda-cusparse-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-npp-10-2.\n",
            "Preparing to unpack .../32-cuda-npp-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-npp-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-npp-dev-10-2.\n",
            "Preparing to unpack .../33-cuda-npp-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-npp-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-10-2.\n",
            "Preparing to unpack .../34-cuda-nvml-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvjpeg-10-2.\n",
            "Preparing to unpack .../35-cuda-nvjpeg-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvjpeg-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvjpeg-dev-10-2.\n",
            "Preparing to unpack .../36-cuda-nvjpeg-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvjpeg-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nsight-compute-10-2.\n",
            "Preparing to unpack .../37-cuda-nsight-compute-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-compute-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nsight-systems-10-2.\n",
            "Preparing to unpack .../38-cuda-nsight-systems-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-systems-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-10-2.\n",
            "Preparing to unpack .../39-cuda-nvgraph-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-dev-10-2.\n",
            "Preparing to unpack .../40-cuda-nvgraph-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-10-2.\n",
            "Preparing to unpack .../41-cuda-visual-tools-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-tools-10-2.\n",
            "Preparing to unpack .../42-cuda-tools-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-tools-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-samples-10-2.\n",
            "Preparing to unpack .../43-cuda-samples-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-samples-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-documentation-10-2.\n",
            "Preparing to unpack .../44-cuda-documentation-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-10-2.\n",
            "Preparing to unpack .../45-cuda-libraries-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-libraries-10-2.\n",
            "Preparing to unpack .../46-cuda-libraries-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-10-2.\n",
            "Preparing to unpack .../47-cuda-toolkit-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-runtime-10-2.\n",
            "Preparing to unpack .../48-cuda-runtime-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-10-2.\n",
            "Preparing to unpack .../49-cuda-demo-suite-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-10-2.\n",
            "Preparing to unpack .../50-cuda-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda.\n",
            "Preparing to unpack .../51-cuda_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda (10.2.89-1) ...\n",
            "Setting up cuda-license-10-2 (10.2.89-1) ...\n",
            "*** LICENSE AGREEMENT ***\n",
            "By using this software you agree to fully comply with the terms and \n",
            "conditions of the EULA (End User License Agreement). The EULA is located\n",
            "at /usr/local/cuda-10.2/doc/EULA.txt. The EULA can also be found at\n",
            "http://docs.nvidia.com/cuda/eula/index.html. If you do not agree to the\n",
            "terms and conditions of the EULA, do not use the software.\n",
            "\n",
            "Setting up cuda-nvgraph-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvprune-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvrtc-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvtx-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvjpeg-10-2 (10.2.89-1) ...\n",
            "Setting up libcublas10 (10.2.2.89-1) ...\n",
            "Setting up libcublas-dev (10.2.2.89-1) ...\n",
            "Setting up cuda-cufft-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nsight-compute-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cusparse-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cuobjdump-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-sanitizer-api-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvjpeg-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cusolver-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-misc-headers-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvvp-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-curand-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cudart-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-npp-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cufft-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-libraries-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-memcheck-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvrtc-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-driver-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-npp-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nsight-systems-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nsight-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvdisasm-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvml-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvgraph-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvcc-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvprof-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cusparse-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-compiler-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-runtime-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-curand-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cusolver-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-demo-suite-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-gdb-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cudart-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-libraries-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-visual-tools-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-samples-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cupti-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-documentation-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cupti-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-command-line-tools-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-tools-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-toolkit-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-10-2 (10.2.89-1) ...\n",
            "Setting up cuda (10.2.89-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU1CaPVJG6F6",
        "colab_type": "code",
        "outputId": "16a8b28c-9d05-4ef8-ec3d-7346b911d1b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "#!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install torch torchvision\n",
        "!pip install gym\n",
        "#!pip install pyglet\n",
        "#!pip install piglet\n",
        "#!pip install pyopengl\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "Display().start()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 89 not upgraded.\n",
            "Need to get 783 kB of archives.\n",
            "After this operation, 2,266 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.3 [783 kB]\n",
            "Fetched 783 kB in 3s (298 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 157617 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.3_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.2.2)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.6)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (0.2.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.2.10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1024x768x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1024x768x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62-5uK8CIVmc",
        "colab_type": "code",
        "outputId": "b8376d5c-daeb-443e-caa5-de3caf42a2b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "user = 'micahr123'\n",
        "password = getpass('Enter Password: ')\n",
        "os.environ['BITBUCKET_AUTH'] = user + ':' + password\n",
        "\n",
        "!rm -rf continuous_control\n",
        "!git clone https://$BITBUCKET_AUTH@github.com/micahr123/continuous_control.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter Password: ··········\n",
            "Cloning into 'continuous_control'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 51 (delta 27), reused 29 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (51/51), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzqAK4OdHhqZ",
        "colab_type": "code",
        "outputId": "ecbd566d-dbf1-4364-9507-0727b38c4d22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ./continuous_control/train.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "Creating agent test30\n",
            "Using device: cuda\n",
            "Building value network\n",
            "Loading value network from file test30_value.pt\n",
            "Building policy network\n",
            "Loading policy network from file test30_policy.pt\n",
            "Building max policy network\n",
            "Loading max policy network from file test30_max_policy.pt\n",
            "Loading experience buffer from file test30.npz\n",
            "Episode 0 finished after 200 timesteps - cumulative reward = -896.5435282054942\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:663.9748239986232 \t\tpolicy loss:1.208337773065098 \t\tavg entropy:1.3901116186637814 \t\tstd entropy:0.3562981318566964\n",
            "Epoch: 2 \t\tvalue loss:658.0772009677574 \t\tpolicy loss:1.3347525059199723 \t\tavg entropy:1.3272703282985576 \t\tstd entropy:0.36631772427050874\n",
            "Epoch: 3 \t\tvalue loss:658.560981125128 \t\tpolicy loss:1.439264753802878 \t\tavg entropy:1.3678215974995067 \t\tstd entropy:0.3420980315865566\n",
            "Epoch: 4 \t\tvalue loss:669.6298137727331 \t\tpolicy loss:1.3660573822553042 \t\tavg entropy:1.3653937461893029 \t\tstd entropy:0.33751477022323434\n",
            "Epoch: 5 \t\tvalue loss:695.3775149486104 \t\tpolicy loss:1.5470679548920179 \t\tavg entropy:1.3699174028199377 \t\tstd entropy:0.3411754975951307\n",
            "Epoch: 6 \t\tvalue loss:685.4352477026767 \t\tpolicy loss:1.3400885961094842 \t\tavg entropy:1.37668370029657 \t\tstd entropy:0.3521765362438713\n",
            "Epoch: 7 \t\tvalue loss:665.9792285356365 \t\tpolicy loss:1.4120252083559506 \t\tavg entropy:1.3498964028322815 \t\tstd entropy:0.35155992239011186\n",
            "Epoch: 8 \t\tvalue loss:645.7342899510118 \t\tpolicy loss:1.5027561080260354 \t\tavg entropy:1.3995632157274258 \t\tstd entropy:0.3381162928827823\n",
            "Epoch: 9 \t\tvalue loss:654.0596973856941 \t\tpolicy loss:1.3436173396032365 \t\tavg entropy:1.3938764518172637 \t\tstd entropy:0.3363985936655776\n",
            "Epoch: 10 \t\tvalue loss:702.661958788262 \t\tpolicy loss:1.6103407066376483 \t\tavg entropy:1.4004262994571381 \t\tstd entropy:0.3385836225711965\n",
            "Episode 1 finished after 200 timesteps - cumulative reward = -1188.6642440764463\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:720.7728776775423 \t\tpolicy loss:1.5309009405433154 \t\tavg entropy:1.423127324993642 \t\tstd entropy:0.3440900994731098\n",
            "Epoch: 2 \t\tvalue loss:704.9508261758773 \t\tpolicy loss:1.4001273757121602 \t\tavg entropy:1.4183875993565218 \t\tstd entropy:0.33125995615689696\n",
            "Epoch: 3 \t\tvalue loss:670.9829716916944 \t\tpolicy loss:1.5467086717730663 \t\tavg entropy:1.4490938641061846 \t\tstd entropy:0.3283852982090794\n",
            "Epoch: 4 \t\tvalue loss:643.6763635854252 \t\tpolicy loss:1.4372951847607973 \t\tavg entropy:1.4501687398572631 \t\tstd entropy:0.32868357686318916\n",
            "Epoch: 5 \t\tvalue loss:665.1430098736872 \t\tpolicy loss:1.4906995374648297 \t\tavg entropy:1.4519302115298265 \t\tstd entropy:0.32196638413562584\n",
            "Epoch: 6 \t\tvalue loss:696.9036490018251 \t\tpolicy loss:1.5101620203158894 \t\tavg entropy:1.4463239580244813 \t\tstd entropy:0.3368445790965074\n",
            "Epoch: 7 \t\tvalue loss:702.2075645571849 \t\tpolicy loss:1.3574524336173885 \t\tavg entropy:1.419425742151524 \t\tstd entropy:0.34931900842148106\n",
            "Epoch: 8 \t\tvalue loss:694.8822551789831 \t\tpolicy loss:1.4702271271924503 \t\tavg entropy:1.4294389620487462 \t\tstd entropy:0.3493322764803484\n",
            "Epoch: 9 \t\tvalue loss:661.1654452964908 \t\tpolicy loss:1.4647531753680745 \t\tavg entropy:1.4497471051184547 \t\tstd entropy:0.3433723391256954\n",
            "Epoch: 10 \t\tvalue loss:651.7014480340677 \t\tpolicy loss:1.444878225443793 \t\tavg entropy:1.448832747453688 \t\tstd entropy:0.333818708461325\n",
            "Episode 2 finished after 200 timesteps - cumulative reward = -1040.7623992870751\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:668.6510184866484 \t\tpolicy loss:1.487014661069776 \t\tavg entropy:1.456662031253769 \t\tstd entropy:0.32286720136854113\n",
            "Epoch: 2 \t\tvalue loss:667.8164082511527 \t\tpolicy loss:1.2904557185094865 \t\tavg entropy:1.430816158429034 \t\tstd entropy:0.347328933836107\n",
            "Epoch: 3 \t\tvalue loss:663.846965852331 \t\tpolicy loss:1.2895527296378964 \t\tavg entropy:1.3864798167701995 \t\tstd entropy:0.3554963909547002\n",
            "Epoch: 4 \t\tvalue loss:651.9478609679176 \t\tpolicy loss:1.4068063751595918 \t\tavg entropy:1.406334545025141 \t\tstd entropy:0.3561729195390595\n",
            "Epoch: 5 \t\tvalue loss:643.8969746574027 \t\tpolicy loss:1.4204566693696818 \t\tavg entropy:1.4061076162140755 \t\tstd entropy:0.3502396120825262\n",
            "Epoch: 6 \t\tvalue loss:672.5241128890241 \t\tpolicy loss:1.5060890854382125 \t\tavg entropy:1.4228853826554302 \t\tstd entropy:0.3309176469939245\n",
            "Epoch: 7 \t\tvalue loss:678.8821506187564 \t\tpolicy loss:1.386018370995756 \t\tavg entropy:1.430603481038372 \t\tstd entropy:0.33782073698227627\n",
            "Epoch: 8 \t\tvalue loss:661.0058823882556 \t\tpolicy loss:1.2806024600247867 \t\tavg entropy:1.379479534274871 \t\tstd entropy:0.35299835200330987\n",
            "Epoch: 9 \t\tvalue loss:647.0641604564229 \t\tpolicy loss:1.5042182597957674 \t\tavg entropy:1.4039214023849358 \t\tstd entropy:0.34682110315308284\n",
            "Epoch: 10 \t\tvalue loss:632.7428413766329 \t\tpolicy loss:1.3642283912564888 \t\tavg entropy:1.411868628885093 \t\tstd entropy:0.3452571839600337\n",
            "Episode 3 finished after 200 timesteps - cumulative reward = -873.9779572761694\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:662.6659085633324 \t\tpolicy loss:1.4703046200705356 \t\tavg entropy:1.4050007673833322 \t\tstd entropy:0.3321123297494217\n",
            "Epoch: 2 \t\tvalue loss:702.7897764112129 \t\tpolicy loss:1.6186207757621516 \t\tavg entropy:1.429034315796667 \t\tstd entropy:0.3356320065541439\n",
            "Epoch: 3 \t\tvalue loss:691.2900265552959 \t\tpolicy loss:1.3737763469336464 \t\tavg entropy:1.433346069414953 \t\tstd entropy:0.3453418704719641\n",
            "Epoch: 4 \t\tvalue loss:672.5518103427574 \t\tpolicy loss:1.4733931920567498 \t\tavg entropy:1.4250412859306916 \t\tstd entropy:0.33369399964695345\n",
            "Epoch: 5 \t\tvalue loss:644.3155837762552 \t\tpolicy loss:1.5080162833948605 \t\tavg entropy:1.459019305061941 \t\tstd entropy:0.3319644103386665\n",
            "Epoch: 6 \t\tvalue loss:639.3830521380315 \t\tpolicy loss:1.4034503737434012 \t\tavg entropy:1.4478809519817954 \t\tstd entropy:0.33077969438912275\n",
            "Epoch: 7 \t\tvalue loss:685.8783199122695 \t\tpolicy loss:1.5497095154934242 \t\tavg entropy:1.4467836958846372 \t\tstd entropy:0.3267941906820552\n",
            "Epoch: 8 \t\tvalue loss:711.7623165943584 \t\tpolicy loss:1.5534540815431563 \t\tavg entropy:1.4543702497637194 \t\tstd entropy:0.338664414387606\n",
            "Epoch: 9 \t\tvalue loss:700.4475377817623 \t\tpolicy loss:1.3408370995130696 \t\tavg entropy:1.4358238546814965 \t\tstd entropy:0.33691457085223586\n",
            "Epoch: 10 \t\tvalue loss:679.4828606277216 \t\tpolicy loss:1.474140703678131 \t\tavg entropy:1.4535729975802334 \t\tstd entropy:0.32725538357831346\n",
            "Episode 4 finished after 200 timesteps - cumulative reward = -867.6673007943822\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:644.4459253530033 \t\tpolicy loss:1.4024202159193695 \t\tavg entropy:1.4556383335062715 \t\tstd entropy:0.32895314361326095\n",
            "Epoch: 2 \t\tvalue loss:646.0342577324539 \t\tpolicy loss:1.4273319859973719 \t\tavg entropy:1.4384506308201883 \t\tstd entropy:0.3242051889574732\n",
            "Epoch: 3 \t\tvalue loss:675.5047297243212 \t\tpolicy loss:1.5063656705324766 \t\tavg entropy:1.441961195761063 \t\tstd entropy:0.32560615093451206\n",
            "Epoch: 4 \t\tvalue loss:680.5858069247887 \t\tpolicy loss:1.4154321140930302 \t\tavg entropy:1.432546685149435 \t\tstd entropy:0.34294527837070843\n",
            "Epoch: 5 \t\tvalue loss:675.9141180319864 \t\tpolicy loss:1.3159079639638056 \t\tavg entropy:1.4071004771013729 \t\tstd entropy:0.3341932274473259\n",
            "Epoch: 6 \t\tvalue loss:659.545280581615 \t\tpolicy loss:1.4092932108972893 \t\tavg entropy:1.424927523737071 \t\tstd entropy:0.32756383664877065\n",
            "Epoch: 7 \t\tvalue loss:637.1357171730917 \t\tpolicy loss:1.3723005554715142 \t\tavg entropy:1.4187022692746802 \t\tstd entropy:0.32788878604918437\n",
            "Epoch: 8 \t\tvalue loss:651.0153818599513 \t\tpolicy loss:1.4202732228841939 \t\tavg entropy:1.4136553202320317 \t\tstd entropy:0.3229623905911768\n",
            "Epoch: 9 \t\tvalue loss:666.0919449602972 \t\tpolicy loss:1.4019027332790563 \t\tavg entropy:1.411901786015659 \t\tstd entropy:0.3341598250503494\n",
            "Epoch: 10 \t\tvalue loss:657.5262901431224 \t\tpolicy loss:1.2203242661523037 \t\tavg entropy:1.3733358678915462 \t\tstd entropy:0.3494293699742343\n",
            "Episode 5 finished after 200 timesteps - cumulative reward = -846.9366661342424\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:649.4051858225176 \t\tpolicy loss:1.318574454515211 \t\tavg entropy:1.33925955036789 \t\tstd entropy:0.3507850155353026\n",
            "Epoch: 2 \t\tvalue loss:624.0351626488471 \t\tpolicy loss:1.3481562887468646 \t\tavg entropy:1.3641209304678479 \t\tstd entropy:0.34997999533175894\n",
            "Epoch: 3 \t\tvalue loss:620.9230041503906 \t\tpolicy loss:1.4143147208998281 \t\tavg entropy:1.3565247349406964 \t\tstd entropy:0.33862041117545455\n",
            "Epoch: 4 \t\tvalue loss:654.7005526634955 \t\tpolicy loss:1.5621156730959493 \t\tavg entropy:1.3959075059016155 \t\tstd entropy:0.31990511576714353\n",
            "Epoch: 5 \t\tvalue loss:657.1259785313761 \t\tpolicy loss:1.3379947350871177 \t\tavg entropy:1.4128491690073137 \t\tstd entropy:0.331560346960561\n",
            "Epoch: 6 \t\tvalue loss:639.3863948698967 \t\tpolicy loss:1.2310405710051138 \t\tavg entropy:1.3629133431418659 \t\tstd entropy:0.33937824772043634\n",
            "Epoch: 7 \t\tvalue loss:615.9756602625692 \t\tpolicy loss:1.383537410728393 \t\tavg entropy:1.359008943009513 \t\tstd entropy:0.34899146186488156\n",
            "Epoch: 8 \t\tvalue loss:594.5792398760395 \t\tpolicy loss:1.2698123493502218 \t\tavg entropy:1.3299880043251646 \t\tstd entropy:0.3681631038714551\n",
            "Epoch: 9 \t\tvalue loss:631.7506073982485 \t\tpolicy loss:1.5144044843412214 \t\tavg entropy:1.3559840742727511 \t\tstd entropy:0.3220982245725096\n",
            "Epoch: 10 \t\tvalue loss:670.9741639168031 \t\tpolicy loss:1.5618991871033945 \t\tavg entropy:1.4076068019389716 \t\tstd entropy:0.31427784336491266\n",
            "Episode 6 finished after 200 timesteps - cumulative reward = -904.1183138253213\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:660.4011535644531 \t\tpolicy loss:1.2806553456091112 \t\tavg entropy:1.4073976065873712 \t\tstd entropy:0.32652843863759584\n",
            "Epoch: 2 \t\tvalue loss:639.7316451534148 \t\tpolicy loss:1.3801943794373543 \t\tavg entropy:1.3856265948494404 \t\tstd entropy:0.3219103335457228\n",
            "Epoch: 3 \t\tvalue loss:604.9726385301159 \t\tpolicy loss:1.3454003766659768 \t\tavg entropy:1.3828989851367979 \t\tstd entropy:0.3408083694427432\n",
            "Epoch: 4 \t\tvalue loss:607.7239606303554 \t\tpolicy loss:1.32435151069395 \t\tavg entropy:1.3523247428214122 \t\tstd entropy:0.3382509790022554\n",
            "Epoch: 5 \t\tvalue loss:662.4653842064643 \t\tpolicy loss:1.5947977535186275 \t\tavg entropy:1.3923553260122137 \t\tstd entropy:0.3075476232128255\n",
            "Epoch: 6 \t\tvalue loss:677.6523974018712 \t\tpolicy loss:1.465206188540305 \t\tavg entropy:1.4208725446948012 \t\tstd entropy:0.3172470333041435\n",
            "Epoch: 7 \t\tvalue loss:659.3870790543094 \t\tpolicy loss:1.252289802797379 \t\tavg entropy:1.4008781413457292 \t\tstd entropy:0.30948903640753533\n",
            "Epoch: 8 \t\tvalue loss:626.7088455692415 \t\tpolicy loss:1.3656848995916304 \t\tavg entropy:1.395269449962855 \t\tstd entropy:0.3128521489609045\n",
            "Epoch: 9 \t\tvalue loss:600.0181678033645 \t\tpolicy loss:1.208029974852839 \t\tavg entropy:1.3514611871661892 \t\tstd entropy:0.3319883702900805\n",
            "Epoch: 10 \t\tvalue loss:631.3571452479208 \t\tpolicy loss:1.4750737730533845 \t\tavg entropy:1.352320168790095 \t\tstd entropy:0.3102788910800843\n",
            "Episode 7 finished after 200 timesteps - cumulative reward = -993.6408868611486\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:660.952399469191 \t\tpolicy loss:1.4654882156079816 \t\tavg entropy:1.3884785444086247 \t\tstd entropy:0.3110071899409605\n",
            "Epoch: 2 \t\tvalue loss:655.7940545851185 \t\tpolicy loss:1.289966921652517 \t\tavg entropy:1.3746848622119272 \t\tstd entropy:0.32393118430631485\n",
            "Epoch: 3 \t\tvalue loss:643.7175952542212 \t\tpolicy loss:1.3061220059471745 \t\tavg entropy:1.3655700774336017 \t\tstd entropy:0.308196553893185\n",
            "Epoch: 4 \t\tvalue loss:613.5232243691721 \t\tpolicy loss:1.2761618120055045 \t\tavg entropy:1.352771569654926 \t\tstd entropy:0.3199049432789121\n",
            "Epoch: 5 \t\tvalue loss:615.3314361572266 \t\tpolicy loss:1.3898012157409423 \t\tavg entropy:1.3332526464586134 \t\tstd entropy:0.3199940418061342\n",
            "Epoch: 6 \t\tvalue loss:649.9277442193801 \t\tpolicy loss:1.5155137623510053 \t\tavg entropy:1.3695112772634277 \t\tstd entropy:0.31157350068926104\n",
            "Epoch: 7 \t\tvalue loss:653.9269788188319 \t\tpolicy loss:1.3562119603157043 \t\tavg entropy:1.381059018073531 \t\tstd entropy:0.32418532790288695\n",
            "Epoch: 8 \t\tvalue loss:641.1680484894783 \t\tpolicy loss:1.2348335654504838 \t\tavg entropy:1.3556670777161013 \t\tstd entropy:0.31361067462637027\n",
            "Epoch: 9 \t\tvalue loss:616.2861500401651 \t\tpolicy loss:1.3176787687886147 \t\tavg entropy:1.3526702555878596 \t\tstd entropy:0.318685595111859\n",
            "Epoch: 10 \t\tvalue loss:605.5672253024194 \t\tpolicy loss:1.2512858135084952 \t\tavg entropy:1.3178599001147917 \t\tstd entropy:0.32942733251104656\n",
            "Episode 8 finished after 200 timesteps - cumulative reward = -956.9720211845358\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:642.9991322178995 \t\tpolicy loss:1.5087360864685428 \t\tavg entropy:1.342614621610703 \t\tstd entropy:0.3095960566713199\n",
            "Epoch: 2 \t\tvalue loss:659.1069951211252 \t\tpolicy loss:1.4279845405009486 \t\tavg entropy:1.3747106440929533 \t\tstd entropy:0.31828122159572436\n",
            "Epoch: 3 \t\tvalue loss:645.6685973136656 \t\tpolicy loss:1.2515077273691855 \t\tavg entropy:1.3523567232344915 \t\tstd entropy:0.31765378130843636\n",
            "Epoch: 4 \t\tvalue loss:628.098648563508 \t\tpolicy loss:1.3609267146356645 \t\tavg entropy:1.3624328849973417 \t\tstd entropy:0.30802788594533925\n",
            "Epoch: 5 \t\tvalue loss:607.4395417244203 \t\tpolicy loss:1.2369822100285561 \t\tavg entropy:1.3397849159275443 \t\tstd entropy:0.3168497582305265\n",
            "Epoch: 6 \t\tvalue loss:633.8965621456023 \t\tpolicy loss:1.4684949036567443 \t\tavg entropy:1.3330294749343279 \t\tstd entropy:0.31041725945895027\n",
            "Epoch: 7 \t\tvalue loss:662.5401571950605 \t\tpolicy loss:1.4782139735837136 \t\tavg entropy:1.3707493383242089 \t\tstd entropy:0.3171348531802262\n",
            "Epoch: 8 \t\tvalue loss:651.7509421071699 \t\tpolicy loss:1.2671803409053433 \t\tavg entropy:1.3612863849947368 \t\tstd entropy:0.324900963836781\n",
            "Epoch: 9 \t\tvalue loss:637.2891624204574 \t\tpolicy loss:1.340345658602253 \t\tavg entropy:1.3593513196050928 \t\tstd entropy:0.3075824578379568\n",
            "Epoch: 10 \t\tvalue loss:610.2281380930254 \t\tpolicy loss:1.3077121819219282 \t\tavg entropy:1.358024131151272 \t\tstd entropy:0.31042139321566875\n",
            "Episode 9 finished after 200 timesteps - cumulative reward = -967.7561419843091\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:619.9910731161795 \t\tpolicy loss:1.4229686048723036 \t\tavg entropy:1.3409193647724966 \t\tstd entropy:0.3095399690714092\n",
            "Epoch: 2 \t\tvalue loss:659.1983686877835 \t\tpolicy loss:1.5541010075999844 \t\tavg entropy:1.3749398971469171 \t\tstd entropy:0.3141563512841341\n",
            "Epoch: 3 \t\tvalue loss:660.476073234312 \t\tpolicy loss:1.352992667305854 \t\tavg entropy:1.3855962840299452 \t\tstd entropy:0.3268024382778662\n",
            "Epoch: 4 \t\tvalue loss:646.4996820265247 \t\tpolicy loss:1.2889596291126744 \t\tavg entropy:1.3729592727268896 \t\tstd entropy:0.30634199314786886\n",
            "Epoch: 5 \t\tvalue loss:617.5310122582221 \t\tpolicy loss:1.3910599216338126 \t\tavg entropy:1.3853041449948664 \t\tstd entropy:0.30518971622293517\n",
            "Epoch: 6 \t\tvalue loss:610.5643039826424 \t\tpolicy loss:1.360420084768726 \t\tavg entropy:1.361078664174003 \t\tstd entropy:0.3106427142771409\n",
            "Epoch: 7 \t\tvalue loss:651.4081017278855 \t\tpolicy loss:1.5231514034732696 \t\tavg entropy:1.3778535493391175 \t\tstd entropy:0.3125575197701953\n",
            "Epoch: 8 \t\tvalue loss:666.8553624306955 \t\tpolicy loss:1.4364412744199075 \t\tavg entropy:1.394550698832158 \t\tstd entropy:0.3266580555284226\n",
            "Epoch: 9 \t\tvalue loss:654.8856737690587 \t\tpolicy loss:1.2965723708752663 \t\tavg entropy:1.3755680462064281 \t\tstd entropy:0.3183179095263454\n",
            "Epoch: 10 \t\tvalue loss:631.0692680112777 \t\tpolicy loss:1.3922285956721152 \t\tavg entropy:1.3952386323761556 \t\tstd entropy:0.3086557900211844\n",
            "Episode 10 finished after 200 timesteps - cumulative reward = -868.5235088750351\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:609.9842383975074 \t\tpolicy loss:1.2697223491138883 \t\tavg entropy:1.369392833805544 \t\tstd entropy:0.3123706744018847\n",
            "Epoch: 2 \t\tvalue loss:638.4063875713045 \t\tpolicy loss:1.4958655512522137 \t\tavg entropy:1.3662332685009075 \t\tstd entropy:0.31030097676639734\n",
            "Epoch: 3 \t\tvalue loss:661.5800553579179 \t\tpolicy loss:1.4975303477711148 \t\tavg entropy:1.3987168203979443 \t\tstd entropy:0.3228516170304831\n",
            "Epoch: 4 \t\tvalue loss:652.6562141539558 \t\tpolicy loss:1.2829889702418493 \t\tavg entropy:1.3846430998101495 \t\tstd entropy:0.32815845130643406\n",
            "Epoch: 5 \t\tvalue loss:639.601314968533 \t\tpolicy loss:1.3696885676611037 \t\tavg entropy:1.3913312431939928 \t\tstd entropy:0.31221778288778695\n",
            "Epoch: 6 \t\tvalue loss:612.392815968347 \t\tpolicy loss:1.3965768000436207 \t\tavg entropy:1.3924329760325684 \t\tstd entropy:0.3126834077376209\n",
            "Epoch: 7 \t\tvalue loss:626.867679172092 \t\tpolicy loss:1.4334798161945645 \t\tavg entropy:1.3704016850740197 \t\tstd entropy:0.3143738765958218\n",
            "Epoch: 8 \t\tvalue loss:656.8029479980469 \t\tpolicy loss:1.575470494845557 \t\tavg entropy:1.4006848180418612 \t\tstd entropy:0.3263852935823235\n",
            "Epoch: 9 \t\tvalue loss:652.6921183268229 \t\tpolicy loss:1.2903940441116455 \t\tavg entropy:1.4004523884675126 \t\tstd entropy:0.3393684920578848\n",
            "Epoch: 10 \t\tvalue loss:640.6202770414807 \t\tpolicy loss:1.3600861600467138 \t\tavg entropy:1.3893148658647414 \t\tstd entropy:0.3224097257735857\n",
            "Episode 11 finished after 200 timesteps - cumulative reward = -749.4630448168579\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:611.2596067398314 \t\tpolicy loss:1.4179891574950445 \t\tavg entropy:1.408628739448121 \t\tstd entropy:0.31530132048979453\n",
            "Epoch: 2 \t\tvalue loss:612.8174932570685 \t\tpolicy loss:1.4052225682470534 \t\tavg entropy:1.3796257089699309 \t\tstd entropy:0.31901952508795256\n",
            "Epoch: 3 \t\tvalue loss:655.3077818855406 \t\tpolicy loss:1.61537428129287 \t\tavg entropy:1.3991810013439794 \t\tstd entropy:0.33109756858764583\n",
            "Epoch: 4 \t\tvalue loss:664.9560861738902 \t\tpolicy loss:1.4208443495962355 \t\tavg entropy:1.4241729581824099 \t\tstd entropy:0.3426720396926018\n",
            "Epoch: 5 \t\tvalue loss:648.9983491443452 \t\tpolicy loss:1.3143292466799419 \t\tavg entropy:1.4055521189383207 \t\tstd entropy:0.3305347441412952\n",
            "Epoch: 6 \t\tvalue loss:620.1239590115017 \t\tpolicy loss:1.382139247561258 \t\tavg entropy:1.4214762158281147 \t\tstd entropy:0.3189949372556823\n",
            "Epoch: 7 \t\tvalue loss:601.7350909520709 \t\tpolicy loss:1.3155762770819286 \t\tavg entropy:1.3898763523537379 \t\tstd entropy:0.32386124014557244\n",
            "Epoch: 8 \t\tvalue loss:638.9169466533358 \t\tpolicy loss:1.5048418858694652 \t\tavg entropy:1.3858902040248116 \t\tstd entropy:0.32792381670999476\n",
            "Epoch: 9 \t\tvalue loss:667.5256875658793 \t\tpolicy loss:1.4962759358542306 \t\tavg entropy:1.4126364895882897 \t\tstd entropy:0.3420517877617856\n",
            "Epoch: 10 \t\tvalue loss:654.1806931268601 \t\tpolicy loss:1.251367496119605 \t\tavg entropy:1.3925501520759784 \t\tstd entropy:0.3479861169279866\n",
            "Episode 12 finished after 200 timesteps - cumulative reward = -883.0392558344256\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:633.3474561903212 \t\tpolicy loss:1.3488096547505213 \t\tavg entropy:1.396707407669328 \t\tstd entropy:0.32808418799250794\n",
            "Epoch: 2 \t\tvalue loss:602.794677249969 \t\tpolicy loss:1.2956694061793979 \t\tavg entropy:1.3845224948870107 \t\tstd entropy:0.32563663487112\n",
            "Epoch: 3 \t\tvalue loss:616.6044757176959 \t\tpolicy loss:1.4247096587741186 \t\tavg entropy:1.3596736778066563 \t\tstd entropy:0.3302821526667529\n",
            "Epoch: 4 \t\tvalue loss:651.3961389935206 \t\tpolicy loss:1.559220963054233 \t\tavg entropy:1.3867348504342591 \t\tstd entropy:0.34466369519322176\n",
            "Epoch: 5 \t\tvalue loss:650.0181056431362 \t\tpolicy loss:1.3371085827312772 \t\tavg entropy:1.395158942735995 \t\tstd entropy:0.35544168032031664\n",
            "Epoch: 6 \t\tvalue loss:635.6073283846416 \t\tpolicy loss:1.2954459899947757 \t\tavg entropy:1.3840716003133846 \t\tstd entropy:0.3345434976358749\n",
            "Epoch: 7 \t\tvalue loss:606.4693768213666 \t\tpolicy loss:1.3222687651240637 \t\tavg entropy:1.3921900052584397 \t\tstd entropy:0.3222453372494611\n",
            "Epoch: 8 \t\tvalue loss:599.9386838882689 \t\tpolicy loss:1.3417443642540583 \t\tavg entropy:1.3566060227641283 \t\tstd entropy:0.3303728011624969\n",
            "Epoch: 9 \t\tvalue loss:638.067123170883 \t\tpolicy loss:1.5169590125008234 \t\tavg entropy:1.3680752762580832 \t\tstd entropy:0.343373046607459\n",
            "Epoch: 10 \t\tvalue loss:652.0372474307105 \t\tpolicy loss:1.4098228339164975 \t\tavg entropy:1.3894093135042123 \t\tstd entropy:0.3566700667481323\n",
            "Episode 13 finished after 200 timesteps - cumulative reward = -861.2180396356446\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:637.4015338231646 \t\tpolicy loss:1.2720185120900471 \t\tavg entropy:1.3704268081429278 \t\tstd entropy:0.3493743628906469\n",
            "Epoch: 2 \t\tvalue loss:615.555907718719 \t\tpolicy loss:1.3242547218761747 \t\tavg entropy:1.3836683688915459 \t\tstd entropy:0.32941088424611936\n",
            "Epoch: 3 \t\tvalue loss:593.1447172619048 \t\tpolicy loss:1.2413608498043485 \t\tavg entropy:1.3515814288169334 \t\tstd entropy:0.33193716220825625\n",
            "Epoch: 4 \t\tvalue loss:618.5070079016307 \t\tpolicy loss:1.4693866059893654 \t\tavg entropy:1.3428877399444201 \t\tstd entropy:0.33439478569027664\n",
            "Epoch: 5 \t\tvalue loss:644.898681640625 \t\tpolicy loss:1.4628183283503093 \t\tavg entropy:1.375136606262368 \t\tstd entropy:0.34720646726453386\n",
            "Epoch: 6 \t\tvalue loss:636.4361906505767 \t\tpolicy loss:1.2405696776178148 \t\tavg entropy:1.3630948261679359 \t\tstd entropy:0.3514035683858466\n",
            "Epoch: 7 \t\tvalue loss:621.9111633300781 \t\tpolicy loss:1.2968013844792805 \t\tavg entropy:1.3564512090024294 \t\tstd entropy:0.33369156007793976\n",
            "Epoch: 8 \t\tvalue loss:593.3521849617125 \t\tpolicy loss:1.2662819661791362 \t\tavg entropy:1.348185296116551 \t\tstd entropy:0.32979922973818426\n",
            "Epoch: 9 \t\tvalue loss:600.5323156932044 \t\tpolicy loss:1.411147345626165 \t\tavg entropy:1.3253224629567117 \t\tstd entropy:0.33395382596602013\n",
            "Epoch: 10 \t\tvalue loss:633.1387968517486 \t\tpolicy loss:1.5360729637600126 \t\tavg entropy:1.3597422556944523 \t\tstd entropy:0.3471899308703138\n",
            "Episode 14 finished after 200 timesteps - cumulative reward = -956.0699191640441\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:635.3441380092075 \t\tpolicy loss:1.3098739045006889 \t\tavg entropy:1.3754171917868985 \t\tstd entropy:0.3541952195134057\n",
            "Epoch: 2 \t\tvalue loss:622.12645757766 \t\tpolicy loss:1.252867549184769 \t\tavg entropy:1.3587570524736057 \t\tstd entropy:0.33386628022255277\n",
            "Epoch: 3 \t\tvalue loss:593.6353604755705 \t\tpolicy loss:1.2895232599879067 \t\tavg entropy:1.3568707862950506 \t\tstd entropy:0.3258761593111003\n",
            "Epoch: 4 \t\tvalue loss:586.9347950768849 \t\tpolicy loss:1.3234824216555034 \t\tavg entropy:1.3192973239497532 \t\tstd entropy:0.3341466723806527\n",
            "Epoch: 5 \t\tvalue loss:624.7835112072173 \t\tpolicy loss:1.5377544486333454 \t\tavg entropy:1.3446019229321253 \t\tstd entropy:0.3414353390180395\n",
            "Epoch: 6 \t\tvalue loss:638.7878122481089 \t\tpolicy loss:1.3848082981412373 \t\tavg entropy:1.3779277397976035 \t\tstd entropy:0.3509292917881956\n",
            "Epoch: 7 \t\tvalue loss:627.3249124193949 \t\tpolicy loss:1.2524806090763636 \t\tavg entropy:1.362538270856653 \t\tstd entropy:0.33627749322784495\n",
            "Epoch: 8 \t\tvalue loss:603.4731769864521 \t\tpolicy loss:1.3094685522336809 \t\tavg entropy:1.36915090287394 \t\tstd entropy:0.3224873280994707\n",
            "Epoch: 9 \t\tvalue loss:584.2946060422867 \t\tpolicy loss:1.2348917533480932 \t\tavg entropy:1.326615658681544 \t\tstd entropy:0.33052747063573856\n",
            "Epoch: 10 \t\tvalue loss:614.1471237909226 \t\tpolicy loss:1.4970367172407726 \t\tavg entropy:1.3293129638187469 \t\tstd entropy:0.3344978839106902\n",
            "Episode 15 finished after 200 timesteps - cumulative reward = -896.8871113045938\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:636.8217697143555 \t\tpolicy loss:1.4103312231600285 \t\tavg entropy:1.3664343837592043 \t\tstd entropy:0.34678700738533835\n",
            "Epoch: 2 \t\tvalue loss:622.4151644706726 \t\tpolicy loss:1.3073319206014276 \t\tavg entropy:1.3550502254568701 \t\tstd entropy:0.3443353014403204\n",
            "Epoch: 3 \t\tvalue loss:607.7694706916809 \t\tpolicy loss:1.4342782273888588 \t\tavg entropy:1.3912354202229011 \t\tstd entropy:0.31810912874527997\n",
            "Epoch: 4 \t\tvalue loss:588.442723274231 \t\tpolicy loss:1.5728950379416347 \t\tavg entropy:1.4041344184419022 \t\tstd entropy:0.3125176096164371\n",
            "Epoch: 5 \t\tvalue loss:611.2639904022217 \t\tpolicy loss:1.4219244979321957 \t\tavg entropy:1.3925895449217363 \t\tstd entropy:0.3337864832797324\n",
            "Epoch: 6 \t\tvalue loss:644.1114687919617 \t\tpolicy loss:1.5113918203860521 \t\tavg entropy:1.402376771117973 \t\tstd entropy:0.35385756967571186\n",
            "Epoch: 7 \t\tvalue loss:636.1149883270264 \t\tpolicy loss:1.3038421496748924 \t\tavg entropy:1.396641516788285 \t\tstd entropy:0.35385775817851595\n",
            "Epoch: 8 \t\tvalue loss:616.4958882331848 \t\tpolicy loss:1.3999544996768236 \t\tavg entropy:1.413164310859917 \t\tstd entropy:0.3295362308075463\n",
            "Epoch: 9 \t\tvalue loss:590.4512133598328 \t\tpolicy loss:1.557158007286489 \t\tavg entropy:1.4290066628497613 \t\tstd entropy:0.31688016265845287\n",
            "Epoch: 10 \t\tvalue loss:608.0873851776123 \t\tpolicy loss:1.3950644312426448 \t\tavg entropy:1.4056290661373847 \t\tstd entropy:0.33313167780314684\n",
            "Episode 16 finished after 200 timesteps - cumulative reward = -748.4684887407691\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:643.5250105857849 \t\tpolicy loss:1.5552120842039585 \t\tavg entropy:1.4142599055819707 \t\tstd entropy:0.35145803282407617\n",
            "Epoch: 2 \t\tvalue loss:636.6423115730286 \t\tpolicy loss:1.2991934912279248 \t\tavg entropy:1.4098042273098361 \t\tstd entropy:0.3570337247308307\n",
            "Epoch: 3 \t\tvalue loss:614.1621451377869 \t\tpolicy loss:1.3998692939057946 \t\tavg entropy:1.414374694659695 \t\tstd entropy:0.3460574048881994\n",
            "Epoch: 4 \t\tvalue loss:584.8681201934814 \t\tpolicy loss:1.5511123165488243 \t\tavg entropy:1.442198069871224 \t\tstd entropy:0.3211557517954963\n",
            "Epoch: 5 \t\tvalue loss:597.5743536949158 \t\tpolicy loss:1.3685611495748162 \t\tavg entropy:1.4114212035878975 \t\tstd entropy:0.3346307820792815\n",
            "Epoch: 6 \t\tvalue loss:639.6079459190369 \t\tpolicy loss:1.620700191706419 \t\tavg entropy:1.4183444813857506 \t\tstd entropy:0.3519515341091024\n",
            "Epoch: 7 \t\tvalue loss:637.5805683135986 \t\tpolicy loss:1.3319510417059064 \t\tavg entropy:1.4236185496603653 \t\tstd entropy:0.36221327578467316\n",
            "Epoch: 8 \t\tvalue loss:616.448721408844 \t\tpolicy loss:1.405274878256023 \t\tavg entropy:1.4239651261265345 \t\tstd entropy:0.3516422362315338\n",
            "Epoch: 9 \t\tvalue loss:586.7129716873169 \t\tpolicy loss:1.5546575570479035 \t\tavg entropy:1.462233062167461 \t\tstd entropy:0.3213922749502817\n",
            "Epoch: 10 \t\tvalue loss:592.1286296844482 \t\tpolicy loss:1.3473919602110982 \t\tavg entropy:1.421995726273827 \t\tstd entropy:0.3348921716107428\n",
            "Episode 17 finished after 200 timesteps - cumulative reward = -876.9690354115986\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:638.6517195701599 \t\tpolicy loss:1.697634780779481 \t\tavg entropy:1.4248545076684007 \t\tstd entropy:0.3519191219281936\n",
            "Epoch: 2 \t\tvalue loss:647.6013336181641 \t\tpolicy loss:1.459498105570674 \t\tavg entropy:1.4517556029361374 \t\tstd entropy:0.36210359509845397\n",
            "Epoch: 3 \t\tvalue loss:628.6137471199036 \t\tpolicy loss:1.39593275077641 \t\tavg entropy:1.4515786964732147 \t\tstd entropy:0.34626286788040744\n",
            "Epoch: 4 \t\tvalue loss:597.6358833312988 \t\tpolicy loss:1.4666546136140823 \t\tavg entropy:1.4814077290142857 \t\tstd entropy:0.3242159987861222\n",
            "Epoch: 5 \t\tvalue loss:588.9995985031128 \t\tpolicy loss:1.3917849948629737 \t\tavg entropy:1.4438647413122578 \t\tstd entropy:0.329735305250183\n",
            "Epoch: 6 \t\tvalue loss:630.81156873703 \t\tpolicy loss:1.668828096240759 \t\tavg entropy:1.4419972472886244 \t\tstd entropy:0.3452115598263775\n",
            "Epoch: 7 \t\tvalue loss:648.9141564369202 \t\tpolicy loss:1.5010904455557466 \t\tavg entropy:1.4668229919943794 \t\tstd entropy:0.3583398762467297\n",
            "Epoch: 8 \t\tvalue loss:632.6333422660828 \t\tpolicy loss:1.3621174739673734 \t\tavg entropy:1.4579649576760314 \t\tstd entropy:0.34524956459212314\n",
            "Epoch: 9 \t\tvalue loss:605.7486038208008 \t\tpolicy loss:1.4205054584890604 \t\tavg entropy:1.4782975202480202 \t\tstd entropy:0.3261729779029748\n",
            "Epoch: 10 \t\tvalue loss:583.8096013069153 \t\tpolicy loss:1.4467260260134935 \t\tavg entropy:1.4541815534243419 \t\tstd entropy:0.3252854958984613\n",
            "Episode 18 finished after 200 timesteps - cumulative reward = -808.1912287136729\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:612.4859523773193 \t\tpolicy loss:1.4961511809378862 \t\tavg entropy:1.4391035905088003 \t\tstd entropy:0.3397090175644272\n",
            "Epoch: 2 \t\tvalue loss:641.7800941467285 \t\tpolicy loss:1.5400045961141586 \t\tavg entropy:1.4512435124090473 \t\tstd entropy:0.35681169290517933\n",
            "Epoch: 3 \t\tvalue loss:628.5235595703125 \t\tpolicy loss:1.2662263941019773 \t\tavg entropy:1.432542990497474 \t\tstd entropy:0.36341220426069143\n",
            "Epoch: 4 \t\tvalue loss:608.1846446990967 \t\tpolicy loss:1.3769933870062232 \t\tavg entropy:1.4277077533019749 \t\tstd entropy:0.35552107628479834\n",
            "Epoch: 5 \t\tvalue loss:580.017397403717 \t\tpolicy loss:1.378145826049149 \t\tavg entropy:1.4320533749722761 \t\tstd entropy:0.33576681938169234\n",
            "Epoch: 6 \t\tvalue loss:591.3978352546692 \t\tpolicy loss:1.4430126389488578 \t\tavg entropy:1.4107887254977676 \t\tstd entropy:0.33762837235593046\n",
            "Epoch: 7 \t\tvalue loss:625.9084701538086 \t\tpolicy loss:1.6237297654151917 \t\tavg entropy:1.4302134737096797 \t\tstd entropy:0.35393358889862486\n",
            "Epoch: 8 \t\tvalue loss:623.0460171699524 \t\tpolicy loss:1.3028297862038016 \t\tavg entropy:1.4391485278579321 \t\tstd entropy:0.36509092871826176\n",
            "Epoch: 9 \t\tvalue loss:604.3525276184082 \t\tpolicy loss:1.2777642421424389 \t\tavg entropy:1.4142568191157239 \t\tstd entropy:0.35465405905167297\n",
            "Epoch: 10 \t\tvalue loss:575.9383978843689 \t\tpolicy loss:1.3187571903690696 \t\tavg entropy:1.4124235637742897 \t\tstd entropy:0.3400438279680149\n",
            "Episode 19 finished after 200 timesteps - cumulative reward = -865.0919542014956\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:570.8145499229431 \t\tpolicy loss:1.4245602237060666 \t\tavg entropy:1.3812982161315157 \t\tstd entropy:0.34249676256550576\n",
            "Epoch: 2 \t\tvalue loss:611.8145251274109 \t\tpolicy loss:1.598324628546834 \t\tavg entropy:1.4015203668456524 \t\tstd entropy:0.3553665598313319\n",
            "Epoch: 3 \t\tvalue loss:627.2606148719788 \t\tpolicy loss:1.4171256525442004 \t\tavg entropy:1.4311423504883423 \t\tstd entropy:0.36721742233400534\n",
            "Epoch: 4 \t\tvalue loss:610.2145023345947 \t\tpolicy loss:1.2507124822586775 \t\tavg entropy:1.4106201145229862 \t\tstd entropy:0.35999502746033984\n",
            "Epoch: 5 \t\tvalue loss:584.720133304596 \t\tpolicy loss:1.289238260127604 \t\tavg entropy:1.4040293045523577 \t\tstd entropy:0.3453301575734776\n",
            "Epoch: 6 \t\tvalue loss:562.5974087715149 \t\tpolicy loss:1.3169322367757559 \t\tavg entropy:1.3695048463945276 \t\tstd entropy:0.34622081003943117\n",
            "Epoch: 7 \t\tvalue loss:591.7334656715393 \t\tpolicy loss:1.497492483817041 \t\tavg entropy:1.3657082488127052 \t\tstd entropy:0.35706443266565197\n",
            "Epoch: 8 \t\tvalue loss:621.7107620239258 \t\tpolicy loss:1.5013464242219925 \t\tavg entropy:1.402228492224589 \t\tstd entropy:0.3704675576341266\n",
            "Epoch: 9 \t\tvalue loss:610.129668712616 \t\tpolicy loss:1.2212342852726579 \t\tavg entropy:1.392985473793 \t\tstd entropy:0.37151343516323304\n",
            "Epoch: 10 \t\tvalue loss:590.23180103302 \t\tpolicy loss:1.2820168128237128 \t\tavg entropy:1.3781030240370893 \t\tstd entropy:0.3584945847716635\n",
            "Episode 20 finished after 200 timesteps - cumulative reward = -984.027335233985\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:560.2020902193509 \t\tpolicy loss:1.2865702610749465 \t\tavg entropy:1.3614236600126062 \t\tstd entropy:0.3510267368034729\n",
            "Epoch: 2 \t\tvalue loss:577.0703927847055 \t\tpolicy loss:1.4599303933290335 \t\tavg entropy:1.34606120342499 \t\tstd entropy:0.35622544220435326\n",
            "Epoch: 3 \t\tvalue loss:609.0387000450721 \t\tpolicy loss:1.6595054809863752 \t\tavg entropy:1.3930372330495315 \t\tstd entropy:0.374020156953606\n",
            "Epoch: 4 \t\tvalue loss:610.4185546875 \t\tpolicy loss:1.2766833662986756 \t\tavg entropy:1.422393650852167 \t\tstd entropy:0.37494624309093816\n",
            "Epoch: 5 \t\tvalue loss:594.2652052659255 \t\tpolicy loss:1.2904621637784517 \t\tavg entropy:1.4004942707777766 \t\tstd entropy:0.36609274304885253\n",
            "Epoch: 6 \t\tvalue loss:565.6517108623798 \t\tpolicy loss:1.3537171244621278 \t\tavg entropy:1.3949985376306784 \t\tstd entropy:0.3554445775047612\n",
            "Epoch: 7 \t\tvalue loss:566.5751450758714 \t\tpolicy loss:1.4473790590579694 \t\tavg entropy:1.364428075559414 \t\tstd entropy:0.36029307936993754\n",
            "Epoch: 8 \t\tvalue loss:601.0734332744892 \t\tpolicy loss:1.6268752098083497 \t\tavg entropy:1.3950040026231905 \t\tstd entropy:0.37954488903145184\n",
            "Epoch: 9 \t\tvalue loss:609.4052255483774 \t\tpolicy loss:1.337948751449585 \t\tavg entropy:1.4251374054748878 \t\tstd entropy:0.38660823328182786\n",
            "Epoch: 10 \t\tvalue loss:595.6420893742488 \t\tpolicy loss:1.2571098309296829 \t\tavg entropy:1.4081513887973403 \t\tstd entropy:0.37882242336433297\n",
            "Episode 21 finished after 200 timesteps - cumulative reward = -949.6730796165215\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:567.4270446777343 \t\tpolicy loss:1.3017630549577566 \t\tavg entropy:1.4015604593706612 \t\tstd entropy:0.3654679285055051\n",
            "Epoch: 2 \t\tvalue loss:554.3354604867789 \t\tpolicy loss:1.3847080615850595 \t\tavg entropy:1.3557264084248624 \t\tstd entropy:0.3745656394426554\n",
            "Epoch: 3 \t\tvalue loss:585.0681603064904 \t\tpolicy loss:1.5619019838479848 \t\tavg entropy:1.3704171293520409 \t\tstd entropy:0.3953253292834733\n",
            "Epoch: 4 \t\tvalue loss:597.141836313101 \t\tpolicy loss:1.398274631683643 \t\tavg entropy:1.4049926229367344 \t\tstd entropy:0.4042329048713053\n",
            "Epoch: 5 \t\tvalue loss:585.1009793795072 \t\tpolicy loss:1.229187051149515 \t\tavg entropy:1.3951206152687161 \t\tstd entropy:0.3975204567581149\n",
            "Epoch: 6 \t\tvalue loss:563.1272963303786 \t\tpolicy loss:1.2387836263729977 \t\tavg entropy:1.3830020070825675 \t\tstd entropy:0.3831904028294491\n",
            "Epoch: 7 \t\tvalue loss:548.280915245643 \t\tpolicy loss:1.3271296354440543 \t\tavg entropy:1.338161123455126 \t\tstd entropy:0.38701056054680977\n",
            "Epoch: 8 \t\tvalue loss:574.1055368276743 \t\tpolicy loss:1.5427802067536573 \t\tavg entropy:1.3433668658507536 \t\tstd entropy:0.4037419624421411\n",
            "Epoch: 9 \t\tvalue loss:588.5258596567007 \t\tpolicy loss:1.413593355508951 \t\tavg entropy:1.3867504123371581 \t\tstd entropy:0.4133323236866324\n",
            "Epoch: 10 \t\tvalue loss:577.0863478440505 \t\tpolicy loss:1.200421850497906 \t\tavg entropy:1.3751109352018338 \t\tstd entropy:0.41091016298631483\n",
            "Episode 22 finished after 200 timesteps - cumulative reward = -848.639622353817\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:557.6808884840746 \t\tpolicy loss:1.2153888060496403 \t\tavg entropy:1.3596737584845384 \t\tstd entropy:0.39780558540165223\n",
            "Epoch: 2 \t\tvalue loss:540.5256272536058 \t\tpolicy loss:1.3087683081626893 \t\tavg entropy:1.325182974093562 \t\tstd entropy:0.3957243120092185\n",
            "Epoch: 3 \t\tvalue loss:565.5191603440505 \t\tpolicy loss:1.5451042266992423 \t\tavg entropy:1.3316238561480223 \t\tstd entropy:0.4010557772531494\n",
            "Epoch: 4 \t\tvalue loss:585.3686490572416 \t\tpolicy loss:1.4780649854586674 \t\tavg entropy:1.3854055779424983 \t\tstd entropy:0.40797944446753\n",
            "Epoch: 5 \t\tvalue loss:574.3557556152343 \t\tpolicy loss:1.2046109236203708 \t\tavg entropy:1.3813306007961001 \t\tstd entropy:0.4039727344608494\n",
            "Epoch: 6 \t\tvalue loss:553.6834707406852 \t\tpolicy loss:1.2367458095917334 \t\tavg entropy:1.3620094197790136 \t\tstd entropy:0.3954223233048671\n",
            "Epoch: 7 \t\tvalue loss:533.956594144381 \t\tpolicy loss:1.2922258477944595 \t\tavg entropy:1.3323738347713607 \t\tstd entropy:0.39617699250033456\n",
            "Epoch: 8 \t\tvalue loss:557.694970703125 \t\tpolicy loss:1.5194399723639855 \t\tavg entropy:1.3248302079861747 \t\tstd entropy:0.40067650159997253\n",
            "Epoch: 9 \t\tvalue loss:583.648930476262 \t\tpolicy loss:1.563598445745615 \t\tavg entropy:1.3825255831739298 \t\tstd entropy:0.40883492260023924\n",
            "Epoch: 10 \t\tvalue loss:576.766587477464 \t\tpolicy loss:1.2240919489126938 \t\tavg entropy:1.3961204591430378 \t\tstd entropy:0.40144585292220497\n",
            "Episode 23 finished after 200 timesteps - cumulative reward = -848.4485522505964\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:554.6513329139123 \t\tpolicy loss:1.2707201205767118 \t\tavg entropy:1.3785424001409132 \t\tstd entropy:0.39460430731402985\n",
            "Epoch: 2 \t\tvalue loss:530.6084167480469 \t\tpolicy loss:1.287247647688939 \t\tavg entropy:1.3519586169995643 \t\tstd entropy:0.3976010867267386\n",
            "Epoch: 3 \t\tvalue loss:549.6985299917368 \t\tpolicy loss:1.4581549094273494 \t\tavg entropy:1.325354940575306 \t\tstd entropy:0.4053726410906262\n",
            "Epoch: 4 \t\tvalue loss:579.0659719613882 \t\tpolicy loss:1.6090335020652184 \t\tavg entropy:1.375937688731485 \t\tstd entropy:0.4165694787704412\n",
            "Epoch: 5 \t\tvalue loss:576.7733830378605 \t\tpolicy loss:1.2379044743684622 \t\tavg entropy:1.403246078806711 \t\tstd entropy:0.40887609883975695\n",
            "Epoch: 6 \t\tvalue loss:555.722463285006 \t\tpolicy loss:1.2713074922561645 \t\tavg entropy:1.3847025958568226 \t\tstd entropy:0.4052488632452943\n",
            "Epoch: 7 \t\tvalue loss:527.5468491774338 \t\tpolicy loss:1.2838563387210553 \t\tavg entropy:1.3609597002954026 \t\tstd entropy:0.407393762129185\n",
            "Epoch: 8 \t\tvalue loss:540.4689622145432 \t\tpolicy loss:1.4303208855482248 \t\tavg entropy:1.3241199530288577 \t\tstd entropy:0.4147211373880634\n",
            "Epoch: 9 \t\tvalue loss:572.5608764648438 \t\tpolicy loss:1.6243527339054988 \t\tavg entropy:1.3706162110092923 \t\tstd entropy:0.4252110926603905\n",
            "Epoch: 10 \t\tvalue loss:573.4993971604567 \t\tpolicy loss:1.258550538466527 \t\tavg entropy:1.405329025802605 \t\tstd entropy:0.41877085023212696\n",
            "Episode 24 finished after 200 timesteps - cumulative reward = -1062.51849917408\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:554.5239741398738 \t\tpolicy loss:1.2514967918395996 \t\tavg entropy:1.385891707615669 \t\tstd entropy:0.41658137660608086\n",
            "Epoch: 2 \t\tvalue loss:524.937660569411 \t\tpolicy loss:1.2682818779578575 \t\tavg entropy:1.3657363842331447 \t\tstd entropy:0.4159920317094135\n",
            "Epoch: 3 \t\tvalue loss:531.6939589280348 \t\tpolicy loss:1.4015574235182542 \t\tavg entropy:1.3204299281881406 \t\tstd entropy:0.42406017454983713\n",
            "Epoch: 4 \t\tvalue loss:566.4846482496995 \t\tpolicy loss:1.633826941710252 \t\tavg entropy:1.3599331464276865 \t\tstd entropy:0.43578349107311565\n",
            "Epoch: 5 \t\tvalue loss:570.8588209885818 \t\tpolicy loss:1.309153609092419 \t\tavg entropy:1.4030399952828885 \t\tstd entropy:0.4300897620263627\n",
            "Epoch: 6 \t\tvalue loss:553.8109656700722 \t\tpolicy loss:1.237770363000723 \t\tavg entropy:1.3878664452176828 \t\tstd entropy:0.42723930726248177\n",
            "Epoch: 7 \t\tvalue loss:524.8905649038461 \t\tpolicy loss:1.2457143783569335 \t\tavg entropy:1.3734081143021584 \t\tstd entropy:0.4220341312288203\n",
            "Epoch: 8 \t\tvalue loss:523.4195096529447 \t\tpolicy loss:1.3625549857433026 \t\tavg entropy:1.3167484150111675 \t\tstd entropy:0.4319319462675451\n",
            "Epoch: 9 \t\tvalue loss:559.1366473858174 \t\tpolicy loss:1.623368032162006 \t\tavg entropy:1.346604713008037 \t\tstd entropy:0.441783590207183\n",
            "Epoch: 10 \t\tvalue loss:568.0212270883413 \t\tpolicy loss:1.3478661619699919 \t\tavg entropy:1.3965099641760954 \t\tstd entropy:0.4355226682307138\n",
            "Episode 25 finished after 200 timesteps - cumulative reward = -870.0813013552577\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:553.2140313350793 \t\tpolicy loss:1.2393108814051657 \t\tavg entropy:1.3835815917334857 \t\tstd entropy:0.43473929368995184\n",
            "Epoch: 2 \t\tvalue loss:529.6916947798295 \t\tpolicy loss:1.312066185655016 \t\tavg entropy:1.3852247255736607 \t\tstd entropy:0.4210935494859659\n",
            "Epoch: 3 \t\tvalue loss:525.3819746537642 \t\tpolicy loss:1.4306481092265158 \t\tavg entropy:1.3489634930433854 \t\tstd entropy:0.4192549700365038\n",
            "Epoch: 4 \t\tvalue loss:565.8823639840791 \t\tpolicy loss:1.6320125063260396 \t\tavg entropy:1.3694150080303291 \t\tstd entropy:0.4209698894888797\n",
            "Epoch: 5 \t\tvalue loss:580.1373961477568 \t\tpolicy loss:1.4258986946308252 \t\tavg entropy:1.4256533700885947 \t\tstd entropy:0.4217328705135234\n",
            "Epoch: 6 \t\tvalue loss:564.2906568122633 \t\tpolicy loss:1.2817540322289322 \t\tavg entropy:1.425798042215224 \t\tstd entropy:0.42462594966157047\n",
            "Epoch: 7 \t\tvalue loss:533.0746335116299 \t\tpolicy loss:1.2841140663985051 \t\tavg entropy:1.4114879223480363 \t\tstd entropy:0.41842831826191834\n",
            "Epoch: 8 \t\tvalue loss:519.8942977442886 \t\tpolicy loss:1.383387722752311 \t\tavg entropy:1.3621277935304708 \t\tstd entropy:0.4246952613533498\n",
            "Epoch: 9 \t\tvalue loss:556.8932236180161 \t\tpolicy loss:1.5968415321725788 \t\tavg entropy:1.3666005404426094 \t\tstd entropy:0.4302155181608276\n",
            "Epoch: 10 \t\tvalue loss:572.6448276404178 \t\tpolicy loss:1.4295244551066197 \t\tavg entropy:1.419121045386133 \t\tstd entropy:0.43373081216842435\n",
            "Episode 26 finished after 200 timesteps - cumulative reward = -959.9315704290016\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:557.9126300233784 \t\tpolicy loss:1.236153789541938 \t\tavg entropy:1.4141954799768848 \t\tstd entropy:0.4419034228714919\n",
            "Epoch: 2 \t\tvalue loss:524.675351229581 \t\tpolicy loss:1.2023976689035243 \t\tavg entropy:1.3879215527726878 \t\tstd entropy:0.43735041060409346\n",
            "Epoch: 3 \t\tvalue loss:501.2220546838009 \t\tpolicy loss:1.2321536992535447 \t\tavg entropy:1.3170526350989071 \t\tstd entropy:0.45464047267094165\n",
            "Epoch: 4 \t\tvalue loss:530.4139191598604 \t\tpolicy loss:1.5504774117108546 \t\tavg entropy:1.3106578721049091 \t\tstd entropy:0.45733979925389384\n",
            "Epoch: 5 \t\tvalue loss:546.9563815954959 \t\tpolicy loss:1.4166435409675946 \t\tavg entropy:1.3729179788624657 \t\tstd entropy:0.4563839488661172\n",
            "Epoch: 6 \t\tvalue loss:536.8693450002959 \t\tpolicy loss:1.135674075646834 \t\tavg entropy:1.3639435057512936 \t\tstd entropy:0.46672637941913053\n",
            "Epoch: 7 \t\tvalue loss:509.09733165394175 \t\tpolicy loss:1.157384817347382 \t\tavg entropy:1.328201070930752 \t\tstd entropy:0.4651429146641324\n",
            "Epoch: 8 \t\tvalue loss:483.91612567323625 \t\tpolicy loss:1.180483270775188 \t\tavg entropy:1.2678612564041618 \t\tstd entropy:0.4795891521656902\n",
            "Epoch: 9 \t\tvalue loss:510.8784313779889 \t\tpolicy loss:1.4828821384545527 \t\tavg entropy:1.259370651763468 \t\tstd entropy:0.47483707678297726\n",
            "Epoch: 10 \t\tvalue loss:535.4469650730942 \t\tpolicy loss:1.4618737688570311 \t\tavg entropy:1.332607579320578 \t\tstd entropy:0.4707581550495375\n",
            "Episode 27 finished after 200 timesteps - cumulative reward = -1297.139914187988\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:528.1386168508818 \t\tpolicy loss:1.1099544375231771 \t\tavg entropy:1.3407183612930793 \t\tstd entropy:0.47861181884773946\n",
            "Epoch: 2 \t\tvalue loss:503.65926707874644 \t\tpolicy loss:1.1323209928743767 \t\tavg entropy:1.2988494887407416 \t\tstd entropy:0.48004111630294816\n",
            "Epoch: 3 \t\tvalue loss:476.3039490670869 \t\tpolicy loss:1.2067134028131312 \t\tavg entropy:1.2572258501075118 \t\tstd entropy:0.4885900515529092\n",
            "Epoch: 4 \t\tvalue loss:499.6661404696378 \t\tpolicy loss:1.4845851695898808 \t\tavg entropy:1.2524028910346692 \t\tstd entropy:0.4782608162957542\n",
            "Epoch: 5 \t\tvalue loss:532.8227173776338 \t\tpolicy loss:1.514030161229047 \t\tavg entropy:1.3278818486549142 \t\tstd entropy:0.47311037676591217\n",
            "Epoch: 6 \t\tvalue loss:530.8477186723189 \t\tpolicy loss:1.1444579408024296 \t\tavg entropy:1.349042001966069 \t\tstd entropy:0.4757798826768515\n",
            "Epoch: 7 \t\tvalue loss:504.3508027972597 \t\tpolicy loss:1.1501507885528333 \t\tavg entropy:1.31182441233044 \t\tstd entropy:0.47838301986519755\n",
            "Epoch: 8 \t\tvalue loss:474.0804748535156 \t\tpolicy loss:1.2122893902388485 \t\tavg entropy:1.277067211434336 \t\tstd entropy:0.4845472897761698\n",
            "Epoch: 9 \t\tvalue loss:494.974809588808 \t\tpolicy loss:1.4396294815973802 \t\tavg entropy:1.259064616342416 \t\tstd entropy:0.478791489490531\n",
            "Epoch: 10 \t\tvalue loss:534.4216119014856 \t\tpolicy loss:1.5772754369360027 \t\tavg entropy:1.3280224550451811 \t\tstd entropy:0.4763647551408722\n",
            "Episode 28 finished after 200 timesteps - cumulative reward = -1061.4688343620574\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:535.0038109981652 \t\tpolicy loss:1.1940949179909446 \t\tavg entropy:1.36452507246297 \t\tstd entropy:0.47375805592828785\n",
            "Epoch: 2 \t\tvalue loss:507.839287035393 \t\tpolicy loss:1.1640926743998672 \t\tavg entropy:1.333968045231872 \t\tstd entropy:0.47786191067766726\n",
            "Epoch: 3 \t\tvalue loss:475.4601218483665 \t\tpolicy loss:1.2294672321189533 \t\tavg entropy:1.3006964339112077 \t\tstd entropy:0.48226638520939996\n",
            "Epoch: 4 \t\tvalue loss:494.21813225023675 \t\tpolicy loss:1.4068352209799218 \t\tavg entropy:1.2724156527597128 \t\tstd entropy:0.4771803992527046\n",
            "Epoch: 5 \t\tvalue loss:535.5005201859908 \t\tpolicy loss:1.644591136412187 \t\tavg entropy:1.3363139632083816 \t\tstd entropy:0.47758401168302433\n",
            "Epoch: 6 \t\tvalue loss:538.0100546172171 \t\tpolicy loss:1.2330593900247053 \t\tavg entropy:1.3814978101378517 \t\tstd entropy:0.47388674019125726\n",
            "Epoch: 7 \t\tvalue loss:511.7205440636837 \t\tpolicy loss:1.1828164178313632 \t\tavg entropy:1.3567492987337808 \t\tstd entropy:0.4780188067070634\n",
            "Epoch: 8 \t\tvalue loss:477.8663459546638 \t\tpolicy loss:1.2503823683117374 \t\tavg entropy:1.3254423866189993 \t\tstd entropy:0.48012594115928153\n",
            "Epoch: 9 \t\tvalue loss:494.09496654163706 \t\tpolicy loss:1.403626479885795 \t\tavg entropy:1.292330720363281 \t\tstd entropy:0.47491370590680737\n",
            "Epoch: 10 \t\tvalue loss:537.1158192952474 \t\tpolicy loss:1.6665127692800579 \t\tavg entropy:1.350134455766812 \t\tstd entropy:0.4758156758416617\n",
            "Episode 29 finished after 200 timesteps - cumulative reward = -968.5747005374046\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:540.388506340258 \t\tpolicy loss:1.2651405478968765 \t\tavg entropy:1.395933061573993 \t\tstd entropy:0.4721769574106128\n",
            "Epoch: 2 \t\tvalue loss:515.2687733968099 \t\tpolicy loss:1.182954301436742 \t\tavg entropy:1.372032357312513 \t\tstd entropy:0.47832407981015646\n",
            "Epoch: 3 \t\tvalue loss:480.4896956935073 \t\tpolicy loss:1.2644914930517024 \t\tavg entropy:1.3431995969489217 \t\tstd entropy:0.4793169112516892\n",
            "Epoch: 4 \t\tvalue loss:492.28263253876656 \t\tpolicy loss:1.3845189385341876 \t\tavg entropy:1.3042468311874704 \t\tstd entropy:0.4757207102090202\n",
            "Epoch: 5 \t\tvalue loss:537.3753269079959 \t\tpolicy loss:1.6881695814204938 \t\tavg entropy:1.3551946118702038 \t\tstd entropy:0.47723562076520365\n",
            "Epoch: 6 \t\tvalue loss:542.1673523873994 \t\tpolicy loss:1.3063121510274482 \t\tavg entropy:1.4045560592394886 \t\tstd entropy:0.4722358579866657\n",
            "Epoch: 7 \t\tvalue loss:519.2823883981416 \t\tpolicy loss:1.1860915603059712 \t\tavg entropy:1.3814491391012615 \t\tstd entropy:0.4781351923057529\n",
            "Epoch: 8 \t\tvalue loss:483.77829164447206 \t\tpolicy loss:1.257994689724662 \t\tavg entropy:1.3583009049650847 \t\tstd entropy:0.47747624145654205\n",
            "Epoch: 9 \t\tvalue loss:486.52454029430044 \t\tpolicy loss:1.3584415578480922 \t\tavg entropy:1.3099043530819543 \t\tstd entropy:0.4780036082566984\n",
            "Epoch: 10 \t\tvalue loss:534.4706124970407 \t\tpolicy loss:1.6670392534949563 \t\tavg entropy:1.348661017432583 \t\tstd entropy:0.4759032685447227\n",
            "Episode 30 finished after 200 timesteps - cumulative reward = -951.086673576029\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:541.8166066639459 \t\tpolicy loss:1.3682421223441166 \t\tavg entropy:1.405162028839581 \t\tstd entropy:0.47114772847280717\n",
            "Epoch: 2 \t\tvalue loss:516.9714801845266 \t\tpolicy loss:1.1667600265189784 \t\tavg entropy:1.3815076002750961 \t\tstd entropy:0.4770809795115982\n",
            "Epoch: 3 \t\tvalue loss:473.36639632039993 \t\tpolicy loss:1.1763274420553178 \t\tavg entropy:1.3452098319258574 \t\tstd entropy:0.475658694742566\n",
            "Epoch: 4 \t\tvalue loss:459.51558992755946 \t\tpolicy loss:1.1775644430473668 \t\tavg entropy:1.2539924990779683 \t\tstd entropy:0.4963200669396842\n",
            "Epoch: 5 \t\tvalue loss:508.5871267745744 \t\tpolicy loss:1.6365747949970302 \t\tavg entropy:1.2769018023843761 \t\tstd entropy:0.48318438864399627\n",
            "Epoch: 6 \t\tvalue loss:525.1974970120102 \t\tpolicy loss:1.3994990444895048 \t\tavg entropy:1.3666995498632557 \t\tstd entropy:0.47070846761028146\n",
            "Epoch: 7 \t\tvalue loss:507.50023002055156 \t\tpolicy loss:1.072577786089769 \t\tavg entropy:1.3394488763264478 \t\tstd entropy:0.4854124032580963\n",
            "Epoch: 8 \t\tvalue loss:466.9518992865263 \t\tpolicy loss:1.1175215093057547 \t\tavg entropy:1.2958654951896191 \t\tstd entropy:0.48710348708410184\n",
            "Epoch: 9 \t\tvalue loss:437.46773699860074 \t\tpolicy loss:1.0681789726463717 \t\tavg entropy:1.2086429183736578 \t\tstd entropy:0.5069308952367063\n",
            "Epoch: 10 \t\tvalue loss:476.74143640318914 \t\tpolicy loss:1.4312920979599455 \t\tavg entropy:1.2012669436633587 \t\tstd entropy:0.4935739715545098\n",
            "Episode 31 finished after 200 timesteps - cumulative reward = -965.7022467234884\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:514.1031102422457 \t\tpolicy loss:1.5490135061207102 \t\tavg entropy:1.3017576069710215 \t\tstd entropy:0.47831010525475\n",
            "Epoch: 2 \t\tvalue loss:505.85035728340716 \t\tpolicy loss:1.1018980287793856 \t\tavg entropy:1.3254259865062243 \t\tstd entropy:0.487629330920683\n",
            "Epoch: 3 \t\tvalue loss:476.1419873593459 \t\tpolicy loss:1.0842813093270829 \t\tavg entropy:1.2780738843070814 \t\tstd entropy:0.49383445353233973\n",
            "Epoch: 4 \t\tvalue loss:441.37465565240205 \t\tpolicy loss:1.0966257034842648 \t\tavg entropy:1.2259793242396988 \t\tstd entropy:0.5081420471051954\n",
            "Epoch: 5 \t\tvalue loss:460.84994825676307 \t\tpolicy loss:1.294336311852754 \t\tavg entropy:1.1894724225291018 \t\tstd entropy:0.5067754300495951\n",
            "Epoch: 6 \t\tvalue loss:512.189873083314 \t\tpolicy loss:1.6764380602694269 \t\tavg entropy:1.2682689784945225 \t\tstd entropy:0.49101054828993196\n",
            "Epoch: 7 \t\tvalue loss:513.7910985234957 \t\tpolicy loss:1.223101091029039 \t\tavg entropy:1.3395817364599691 \t\tstd entropy:0.48356106968302887\n",
            "Epoch: 8 \t\tvalue loss:490.39862288289993 \t\tpolicy loss:1.0672637366536837 \t\tavg entropy:1.2996115823018264 \t\tstd entropy:0.4899026859155716\n",
            "Epoch: 9 \t\tvalue loss:454.18041126763643 \t\tpolicy loss:1.0838271334989746 \t\tavg entropy:1.25892457106319 \t\tstd entropy:0.4948224459633678\n",
            "Epoch: 10 \t\tvalue loss:449.2756010596432 \t\tpolicy loss:1.1427130481200432 \t\tavg entropy:1.1808131286129355 \t\tstd entropy:0.5119628118723407\n",
            "Episode 32 finished after 200 timesteps - cumulative reward = -1045.216840491607\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:501.321618379052 \t\tpolicy loss:1.630803008577717 \t\tavg entropy:1.223689018755301 \t\tstd entropy:0.49538203880890685\n",
            "Epoch: 2 \t\tvalue loss:517.8804963524661 \t\tpolicy loss:1.3864762338239756 \t\tavg entropy:1.3203830071695306 \t\tstd entropy:0.4825545502147476\n",
            "Epoch: 3 \t\tvalue loss:500.25029196668027 \t\tpolicy loss:1.0600367998009297 \t\tavg entropy:1.306402015938527 \t\tstd entropy:0.4901700555117537\n",
            "Epoch: 4 \t\tvalue loss:465.66170672516324 \t\tpolicy loss:1.094065477598959 \t\tavg entropy:1.2693876525473309 \t\tstd entropy:0.48921047156559727\n",
            "Epoch: 5 \t\tvalue loss:445.8810433914412 \t\tpolicy loss:1.1606175250081874 \t\tavg entropy:1.210207819475068 \t\tstd entropy:0.5004775751760404\n",
            "Epoch: 6 \t\tvalue loss:489.81107523903916 \t\tpolicy loss:1.566271932267431 \t\tavg entropy:1.2271850938267177 \t\tstd entropy:0.4901258122128332\n",
            "Epoch: 7 \t\tvalue loss:520.6645516922225 \t\tpolicy loss:1.5542584871178242 \t\tavg entropy:1.3232072737029903 \t\tstd entropy:0.4822284922336915\n",
            "Epoch: 8 \t\tvalue loss:508.3896985409865 \t\tpolicy loss:1.0949400174083994 \t\tavg entropy:1.3416944858429578 \t\tstd entropy:0.48617827769161515\n",
            "Epoch: 9 \t\tvalue loss:474.3643115598764 \t\tpolicy loss:1.1036115439970102 \t\tavg entropy:1.294519930032191 \t\tstd entropy:0.48735363601815074\n",
            "Epoch: 10 \t\tvalue loss:445.3311389524545 \t\tpolicy loss:1.1715035571980832 \t\tavg entropy:1.2481390250666633 \t\tstd entropy:0.4958475212646545\n",
            "Episode 33 finished after 200 timesteps - cumulative reward = -857.2063953160643\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:477.9349315130888 \t\tpolicy loss:1.4427662883231889 \t\tavg entropy:1.2379338744581638 \t\tstd entropy:0.4888144948150887\n",
            "Epoch: 2 \t\tvalue loss:525.744352426102 \t\tpolicy loss:1.6605962621631907 \t\tavg entropy:1.3218951592478387 \t\tstd entropy:0.48179115687889124\n",
            "Epoch: 3 \t\tvalue loss:518.9873055984725 \t\tpolicy loss:1.203833161894955 \t\tavg entropy:1.3662427932032926 \t\tstd entropy:0.4782392601312601\n",
            "Epoch: 4 \t\tvalue loss:487.19849999271224 \t\tpolicy loss:1.1293067763100808 \t\tavg entropy:1.3274158756107002 \t\tstd entropy:0.48072796982551486\n",
            "Epoch: 5 \t\tvalue loss:452.875974740555 \t\tpolicy loss:1.2230955264461574 \t\tavg entropy:1.3016886251852542 \t\tstd entropy:0.48353476327976674\n",
            "Epoch: 6 \t\tvalue loss:469.7186069773204 \t\tpolicy loss:1.3897826822836008 \t\tavg entropy:1.2698756366176251 \t\tstd entropy:0.4811052697610112\n",
            "Epoch: 7 \t\tvalue loss:527.6725709829758 \t\tpolicy loss:1.6952325532685464 \t\tavg entropy:1.331482833391892 \t\tstd entropy:0.4717010177435477\n",
            "Epoch: 8 \t\tvalue loss:528.0366183608326 \t\tpolicy loss:1.3413740796829337 \t\tavg entropy:1.3885042692883731 \t\tstd entropy:0.4684565808152687\n",
            "Epoch: 9 \t\tvalue loss:501.0961868513876 \t\tpolicy loss:1.1495604016887608 \t\tavg entropy:1.3603934870811993 \t\tstd entropy:0.47393703944067916\n",
            "Epoch: 10 \t\tvalue loss:463.8771075348356 \t\tpolicy loss:1.1945640160076654 \t\tavg entropy:1.3398615894712016 \t\tstd entropy:0.4747061333107717\n",
            "Episode 34 finished after 200 timesteps - cumulative reward = -966.8750819292582\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:457.9218112319263 \t\tpolicy loss:1.296716087789678 \t\tavg entropy:1.2803319163682745 \t\tstd entropy:0.4850164541988707\n",
            "Epoch: 2 \t\tvalue loss:515.0109403240147 \t\tpolicy loss:1.6339234344994844 \t\tavg entropy:1.309966183318131 \t\tstd entropy:0.47123447217721787\n",
            "Epoch: 3 \t\tvalue loss:532.5472931363689 \t\tpolicy loss:1.5175164421992515 \t\tavg entropy:1.3811210910106329 \t\tstd entropy:0.46623150996516693\n",
            "Epoch: 4 \t\tvalue loss:511.57777928594334 \t\tpolicy loss:1.1426701234347785 \t\tavg entropy:1.375841387164904 \t\tstd entropy:0.47381832105877436\n",
            "Epoch: 5 \t\tvalue loss:474.66069782314014 \t\tpolicy loss:1.1504463731352963 \t\tavg entropy:1.343745028652362 \t\tstd entropy:0.4749012027856256\n",
            "Epoch: 6 \t\tvalue loss:448.74963971038363 \t\tpolicy loss:1.2078591540678223 \t\tavg entropy:1.288341069019775 \t\tstd entropy:0.4848034207419638\n",
            "Epoch: 7 \t\tvalue loss:489.3213587518948 \t\tpolicy loss:1.4824502841750187 \t\tavg entropy:1.2869039553232602 \t\tstd entropy:0.47260198063846715\n",
            "Epoch: 8 \t\tvalue loss:535.0586666277985 \t\tpolicy loss:1.6293676593410436 \t\tavg entropy:1.3530321139986836 \t\tstd entropy:0.466057940324896\n",
            "Epoch: 9 \t\tvalue loss:521.6347027678988 \t\tpolicy loss:1.222239321737147 \t\tavg entropy:1.3791074535822245 \t\tstd entropy:0.4686278995451634\n",
            "Epoch: 10 \t\tvalue loss:486.81179171889573 \t\tpolicy loss:1.127280778849303 \t\tavg entropy:1.339990444871472 \t\tstd entropy:0.4763571513681543\n",
            "Episode 35 finished after 200 timesteps - cumulative reward = -752.5028444998555\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:448.62796514174516 \t\tpolicy loss:1.2138700458933325 \t\tavg entropy:1.3141113003179254 \t\tstd entropy:0.47973185171670085\n",
            "Epoch: 2 \t\tvalue loss:459.7960240981158 \t\tpolicy loss:1.2945653305334204 \t\tavg entropy:1.2672414964464094 \t\tstd entropy:0.4881647730557489\n",
            "Epoch: 3 \t\tvalue loss:524.6538660386029 \t\tpolicy loss:1.7017448640921538 \t\tavg entropy:1.308787790081863 \t\tstd entropy:0.4787320388675989\n",
            "Epoch: 4 \t\tvalue loss:525.8269384047564 \t\tpolicy loss:1.577118118019665 \t\tavg entropy:1.389573906805322 \t\tstd entropy:0.47262066591505764\n",
            "Epoch: 5 \t\tvalue loss:504.3048647712259 \t\tpolicy loss:1.1871677356607773 \t\tavg entropy:1.3967795144564783 \t\tstd entropy:0.47124864514351156\n",
            "Epoch: 6 \t\tvalue loss:470.45642583510454 \t\tpolicy loss:1.1552969401373583 \t\tavg entropy:1.36725394068308 \t\tstd entropy:0.4695314395781976\n",
            "Epoch: 7 \t\tvalue loss:450.5187961353975 \t\tpolicy loss:1.1830247772090576 \t\tavg entropy:1.2976490455453418 \t\tstd entropy:0.4863010145112201\n",
            "Epoch: 8 \t\tvalue loss:494.2410426420324 \t\tpolicy loss:1.4841384274118088 \t\tavg entropy:1.2871125314111955 \t\tstd entropy:0.481610000572105\n",
            "Epoch: 9 \t\tvalue loss:529.2790352316464 \t\tpolicy loss:1.696966795360341 \t\tavg entropy:1.3535400791326537 \t\tstd entropy:0.48229255305147284\n",
            "Epoch: 10 \t\tvalue loss:513.6076736450195 \t\tpolicy loss:1.2412223535425522 \t\tavg entropy:1.3933849604376813 \t\tstd entropy:0.47746451236561605\n",
            "Episode 36 finished after 200 timesteps - cumulative reward = -605.3609438460096\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:481.61073168586285 \t\tpolicy loss:1.126278835184434 \t\tavg entropy:1.3509214884100222 \t\tstd entropy:0.47964718085602415\n",
            "Epoch: 2 \t\tvalue loss:444.046547833611 \t\tpolicy loss:1.1078810507760328 \t\tavg entropy:1.3065769492830386 \t\tstd entropy:0.47844682924436716\n",
            "Epoch: 3 \t\tvalue loss:446.5370986040901 \t\tpolicy loss:1.1904117079342114 \t\tavg entropy:1.2309099732865565 \t\tstd entropy:0.4939700537331641\n",
            "Epoch: 4 \t\tvalue loss:499.66098201976104 \t\tpolicy loss:1.6095865079585243 \t\tavg entropy:1.2582514512519012 \t\tstd entropy:0.48341121406191856\n",
            "Epoch: 5 \t\tvalue loss:497.7119373994715 \t\tpolicy loss:1.2300318383118685 \t\tavg entropy:1.3149800944116066 \t\tstd entropy:0.4793610679776096\n",
            "Epoch: 6 \t\tvalue loss:468.2001697315889 \t\tpolicy loss:0.9666287828894222 \t\tavg entropy:1.2587606698351992 \t\tstd entropy:0.49331817057001\n",
            "Epoch: 7 \t\tvalue loss:427.8254722146427 \t\tpolicy loss:0.9736159400028341 \t\tavg entropy:1.2095414909054953 \t\tstd entropy:0.4917156800450347\n",
            "Epoch: 8 \t\tvalue loss:400.61197976505053 \t\tpolicy loss:0.9855914567323292 \t\tavg entropy:1.1200896839901406 \t\tstd entropy:0.5119227327735297\n",
            "Epoch: 9 \t\tvalue loss:439.44399081959443 \t\tpolicy loss:1.3088222667574883 \t\tavg entropy:1.1197580593048377 \t\tstd entropy:0.5025488853448896\n",
            "Epoch: 10 \t\tvalue loss:488.26936430089614 \t\tpolicy loss:1.5070898173486484 \t\tavg entropy:1.2074123268417505 \t\tstd entropy:0.4828564729795716\n",
            "Episode 37 finished after 200 timesteps - cumulative reward = -954.2312518788153\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:473.12019572538486 \t\tpolicy loss:1.0062473915955599 \t\tavg entropy:1.2378486144344305 \t\tstd entropy:0.4811914293006102\n",
            "Epoch: 2 \t\tvalue loss:439.6181645112879 \t\tpolicy loss:0.9360660257584908 \t\tavg entropy:1.1666020886466657 \t\tstd entropy:0.4886114650733105\n",
            "Epoch: 3 \t\tvalue loss:403.10712275785556 \t\tpolicy loss:0.9271543262635961 \t\tavg entropy:1.1281674252659877 \t\tstd entropy:0.4986901550428188\n",
            "Epoch: 4 \t\tvalue loss:399.4287028593176 \t\tpolicy loss:1.0124505057054407 \t\tavg entropy:1.0445792245448253 \t\tstd entropy:0.5220357044133466\n",
            "Epoch: 5 \t\tvalue loss:458.5981418385225 \t\tpolicy loss:1.5190490054733612 \t\tavg entropy:1.1016806144381945 \t\tstd entropy:0.5039995675170662\n",
            "Epoch: 6 \t\tvalue loss:482.4251183902516 \t\tpolicy loss:1.340312725480865 \t\tavg entropy:1.213768721778034 \t\tstd entropy:0.48487782179486366\n",
            "Epoch: 7 \t\tvalue loss:459.11326060575595 \t\tpolicy loss:0.9217095699380425 \t\tavg entropy:1.2002168040486747 \t\tstd entropy:0.49166806770537425\n",
            "Epoch: 8 \t\tvalue loss:421.606227425968 \t\tpolicy loss:0.9229594775859047 \t\tavg entropy:1.1406040105708606 \t\tstd entropy:0.4899672654391991\n",
            "Epoch: 9 \t\tvalue loss:392.7786367079791 \t\tpolicy loss:0.9560675730600077 \t\tavg entropy:1.0851955062477194 \t\tstd entropy:0.5032887987178839\n",
            "Epoch: 10 \t\tvalue loss:421.4638299381032 \t\tpolicy loss:1.211550194550963 \t\tavg entropy:1.0566113630986425 \t\tstd entropy:0.5113351420244746\n",
            "Episode 38 finished after 200 timesteps - cumulative reward = -860.1406648292301\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:479.03089770148785 \t\tpolicy loss:1.5617677367785399 \t\tavg entropy:1.1498186579515888 \t\tstd entropy:0.4939455731387713\n",
            "Epoch: 2 \t\tvalue loss:473.9277222577263 \t\tpolicy loss:1.1702657710103428 \t\tavg entropy:1.223925330842293 \t\tstd entropy:0.4849654648085813\n",
            "Epoch: 3 \t\tvalue loss:444.4613068524529 \t\tpolicy loss:0.9742577636943144 \t\tavg entropy:1.1858859355182862 \t\tstd entropy:0.48735750308269454\n",
            "Epoch: 4 \t\tvalue loss:410.62955474853516 \t\tpolicy loss:0.9861640268388916 \t\tavg entropy:1.160677621144153 \t\tstd entropy:0.4945688821442015\n",
            "Epoch: 5 \t\tvalue loss:406.6331652473001 \t\tpolicy loss:1.0781344745089025 \t\tavg entropy:1.0933540624764728 \t\tstd entropy:0.5153276939858958\n",
            "Epoch: 6 \t\tvalue loss:465.2187495512121 \t\tpolicy loss:1.5539508961579378 \t\tavg entropy:1.1351831402708734 \t\tstd entropy:0.5032447164366318\n",
            "Epoch: 7 \t\tvalue loss:485.2380604463465 \t\tpolicy loss:1.3763085779021769 \t\tavg entropy:1.2405383424808143 \t\tstd entropy:0.48583811158956935\n",
            "Epoch: 8 \t\tvalue loss:462.0420967550839 \t\tpolicy loss:0.9836723331142875 \t\tavg entropy:1.2302971931835938 \t\tstd entropy:0.4874634164578813\n",
            "Epoch: 9 \t\tvalue loss:425.4105224609375 \t\tpolicy loss:0.9742473460295621 \t\tavg entropy:1.1883943608535075 \t\tstd entropy:0.4904423484033382\n",
            "Epoch: 10 \t\tvalue loss:400.8661871517406 \t\tpolicy loss:1.0450857172117514 \t\tavg entropy:1.1258002052333063 \t\tstd entropy:0.5077675602229639\n",
            "Episode 39 finished after 200 timesteps - cumulative reward = -1074.7986630821345\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:440.0380388147691 \t\tpolicy loss:1.3634678397108526 \t\tavg entropy:1.125925376658273 \t\tstd entropy:0.5061103558663492\n",
            "Epoch: 2 \t\tvalue loss:488.08489047779756 \t\tpolicy loss:1.4730772060506485 \t\tavg entropy:1.2104441175823484 \t\tstd entropy:0.4890965262010659\n",
            "Epoch: 3 \t\tvalue loss:469.52381941851445 \t\tpolicy loss:1.063474003882969 \t\tavg entropy:1.2359949198693914 \t\tstd entropy:0.4815095365397239\n",
            "Epoch: 4 \t\tvalue loss:434.2392344755285 \t\tpolicy loss:0.9801670922076001 \t\tavg entropy:1.184773836880265 \t\tstd entropy:0.4833031908877397\n",
            "Epoch: 5 \t\tvalue loss:401.1029577816234 \t\tpolicy loss:1.0177964116720593 \t\tavg entropy:1.1510240355219472 \t\tstd entropy:0.4979324818038417\n",
            "Epoch: 6 \t\tvalue loss:413.96047120935776 \t\tpolicy loss:1.1848274982150864 \t\tavg entropy:1.1011025410629809 \t\tstd entropy:0.5141782711528883\n",
            "Epoch: 7 \t\tvalue loss:478.033727758071 \t\tpolicy loss:1.5582732049857868 \t\tavg entropy:1.165412634310696 \t\tstd entropy:0.49148815954322367\n",
            "Epoch: 8 \t\tvalue loss:476.1894903743968 \t\tpolicy loss:1.2417494093670565 \t\tavg entropy:1.242748544974362 \t\tstd entropy:0.47789040816014916\n",
            "Epoch: 9 \t\tvalue loss:446.9429828419405 \t\tpolicy loss:0.9715549774906215 \t\tavg entropy:1.204148577446885 \t\tstd entropy:0.4826626405054514\n",
            "Epoch: 10 \t\tvalue loss:410.92216738532574 \t\tpolicy loss:0.9764355688410646 \t\tavg entropy:1.1739016737550059 \t\tstd entropy:0.49430547426951016\n",
            "Episode 40 finished after 200 timesteps - cumulative reward = -744.7729918916507\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:394.96768763445425 \t\tpolicy loss:1.0654682206070942 \t\tavg entropy:1.1012663846196404 \t\tstd entropy:0.5151527965612384\n",
            "Epoch: 2 \t\tvalue loss:453.6208226300668 \t\tpolicy loss:1.4586273172627324 \t\tavg entropy:1.1385935155260773 \t\tstd entropy:0.49886725749629385\n",
            "Epoch: 3 \t\tvalue loss:481.98733785878056 \t\tpolicy loss:1.387028233728547 \t\tavg entropy:1.2323067429729484 \t\tstd entropy:0.4749101897646429\n",
            "Epoch: 4 \t\tvalue loss:456.82083483709806 \t\tpolicy loss:0.9796159876429517 \t\tavg entropy:1.2187852270315644 \t\tstd entropy:0.47657036895469107\n",
            "Epoch: 5 \t\tvalue loss:419.8829929517663 \t\tpolicy loss:0.9845948931963547 \t\tavg entropy:1.1518488130751243 \t\tstd entropy:0.5007608237201492\n",
            "Epoch: 6 \t\tvalue loss:388.1632935897164 \t\tpolicy loss:1.1029428969258848 \t\tavg entropy:1.1103112086148992 \t\tstd entropy:0.5280545399939918\n",
            "Epoch: 7 \t\tvalue loss:422.58056905995244 \t\tpolicy loss:1.2869573861792467 \t\tavg entropy:1.1146131678449969 \t\tstd entropy:0.5194483740552412\n",
            "Epoch: 8 \t\tvalue loss:485.012584299281 \t\tpolicy loss:1.5548890958661619 \t\tavg entropy:1.2030639816905833 \t\tstd entropy:0.4831851969297394\n",
            "Epoch: 9 \t\tvalue loss:469.9467180777287 \t\tpolicy loss:1.1762834312259287 \t\tavg entropy:1.254717752029819 \t\tstd entropy:0.47314349317944493\n",
            "Epoch: 10 \t\tvalue loss:438.92651499872625 \t\tpolicy loss:1.0010233810846356 \t\tavg entropy:1.1990832248760546 \t\tstd entropy:0.4806659574517361\n",
            "Episode 41 finished after 200 timesteps - cumulative reward = -756.9995291378331\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:405.4495064555735 \t\tpolicy loss:1.0298894432143888 \t\tavg entropy:1.1748792094580438 \t\tstd entropy:0.49915739357718997\n",
            "Epoch: 2 \t\tvalue loss:400.2745321522588 \t\tpolicy loss:1.1232481940069061 \t\tavg entropy:1.1129694888252786 \t\tstd entropy:0.5232833703661911\n",
            "Epoch: 3 \t\tvalue loss:465.4719645182292 \t\tpolicy loss:1.5260562736918961 \t\tavg entropy:1.152161855744906 \t\tstd entropy:0.5079140371450708\n",
            "Epoch: 4 \t\tvalue loss:481.43464726987094 \t\tpolicy loss:1.4391804890356201 \t\tavg entropy:1.2620701710727305 \t\tstd entropy:0.48300909100599415\n",
            "Epoch: 5 \t\tvalue loss:456.2479557645494 \t\tpolicy loss:1.0119879168012869 \t\tavg entropy:1.2494484647343818 \t\tstd entropy:0.48214877582005256\n",
            "Epoch: 6 \t\tvalue loss:419.87098030422044 \t\tpolicy loss:0.9718409690304078 \t\tavg entropy:1.2018116030709176 \t\tstd entropy:0.49282172430935567\n",
            "Epoch: 7 \t\tvalue loss:393.42092674365944 \t\tpolicy loss:1.0755090959694074 \t\tavg entropy:1.1361719246818658 \t\tstd entropy:0.5132647628988655\n",
            "Epoch: 8 \t\tvalue loss:428.03605099000794 \t\tpolicy loss:1.2745851809563844 \t\tavg entropy:1.1239483818045835 \t\tstd entropy:0.5196935878314844\n",
            "Epoch: 9 \t\tvalue loss:483.8700707477072 \t\tpolicy loss:1.5411013451175413 \t\tavg entropy:1.1995983255693787 \t\tstd entropy:0.4962016842152503\n",
            "Epoch: 10 \t\tvalue loss:465.6910709989244 \t\tpolicy loss:1.1843474999718044 \t\tavg entropy:1.2508347613619346 \t\tstd entropy:0.48302112356367416\n",
            "Episode 42 finished after 200 timesteps - cumulative reward = -850.430264063005\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:432.50262849227244 \t\tpolicy loss:0.9756877910399783 \t\tavg entropy:1.1999468196500582 \t\tstd entropy:0.4877498798657389\n",
            "Epoch: 2 \t\tvalue loss:399.7345715951229 \t\tpolicy loss:0.9814547200997671 \t\tavg entropy:1.1643906808131899 \t\tstd entropy:0.5026374626267186\n",
            "Epoch: 3 \t\tvalue loss:397.14674399555594 \t\tpolicy loss:1.0895598219788594 \t\tavg entropy:1.096243160288162 \t\tstd entropy:0.525998063211745\n",
            "Epoch: 4 \t\tvalue loss:457.2770956288213 \t\tpolicy loss:1.48638019743173 \t\tavg entropy:1.1333489761881683 \t\tstd entropy:0.51716499523586\n",
            "Epoch: 5 \t\tvalue loss:476.58757041157156 \t\tpolicy loss:1.380272721034893 \t\tavg entropy:1.2372413877499868 \t\tstd entropy:0.4915116720243055\n",
            "Epoch: 6 \t\tvalue loss:448.5666928498641 \t\tpolicy loss:0.9757206526355467 \t\tavg entropy:1.2172529122450608 \t\tstd entropy:0.4906890612114234\n",
            "Epoch: 7 \t\tvalue loss:408.4495991140172 \t\tpolicy loss:0.9361367411371591 \t\tavg entropy:1.15305377719093 \t\tstd entropy:0.5022454149173528\n",
            "Epoch: 8 \t\tvalue loss:379.1697871996009 \t\tpolicy loss:1.002230024424152 \t\tavg entropy:1.0802039264889745 \t\tstd entropy:0.5254987263886622\n",
            "Epoch: 9 \t\tvalue loss:409.7724914550781 \t\tpolicy loss:1.2401613308035808 \t\tavg entropy:1.0659599185811013 \t\tstd entropy:0.5320286047853073\n",
            "Epoch: 10 \t\tvalue loss:473.552037778108 \t\tpolicy loss:1.5478141290554102 \t\tavg entropy:1.1727679818554757 \t\tstd entropy:0.5021439639002654\n",
            "Episode 43 finished after 200 timesteps - cumulative reward = -820.2333347300522\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:462.9709406313689 \t\tpolicy loss:1.2110104595405469 \t\tavg entropy:1.2463549147076298 \t\tstd entropy:0.4865994775661713\n",
            "Epoch: 2 \t\tvalue loss:430.30393296393794 \t\tpolicy loss:0.9436241444470226 \t\tavg entropy:1.1929300576431114 \t\tstd entropy:0.4923102294893737\n",
            "Epoch: 3 \t\tvalue loss:391.90901759050894 \t\tpolicy loss:0.9330301682154337 \t\tavg entropy:1.1444215668718396 \t\tstd entropy:0.5070390608822377\n",
            "Epoch: 4 \t\tvalue loss:381.0937610570935 \t\tpolicy loss:1.0119448226431143 \t\tavg entropy:1.0518826322939767 \t\tstd entropy:0.5296443004777719\n",
            "Epoch: 5 \t\tvalue loss:437.31860484247625 \t\tpolicy loss:1.3943027616411015 \t\tavg entropy:1.0920054714739063 \t\tstd entropy:0.5240029011646885\n",
            "Epoch: 6 \t\tvalue loss:469.4328891920007 \t\tpolicy loss:1.3753406180851702 \t\tavg entropy:1.2001782010566053 \t\tstd entropy:0.49506188733365564\n",
            "Epoch: 7 \t\tvalue loss:443.0422668457031 \t\tpolicy loss:0.9330434272254723 \t\tavg entropy:1.1961908437467592 \t\tstd entropy:0.490546435840438\n",
            "Epoch: 8 \t\tvalue loss:404.2402500760728 \t\tpolicy loss:0.9037215346875398 \t\tavg entropy:1.1175978452608335 \t\tstd entropy:0.5003562735626892\n",
            "Epoch: 9 \t\tvalue loss:372.7070323557094 \t\tpolicy loss:0.9278381173161493 \t\tavg entropy:1.0630034500879213 \t\tstd entropy:0.5203284354009466\n",
            "Epoch: 10 \t\tvalue loss:391.10110650546306 \t\tpolicy loss:1.1033888299396073 \t\tavg entropy:1.0137786015751231 \t\tstd entropy:0.537407096599934\n",
            "Episode 44 finished after 200 timesteps - cumulative reward = -908.0485665163959\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:455.00194958673006 \t\tpolicy loss:1.522574271844781 \t\tavg entropy:1.1066343765716622 \t\tstd entropy:0.5149743718345732\n",
            "Epoch: 2 \t\tvalue loss:452.47518345929575 \t\tpolicy loss:1.1775110528089 \t\tavg entropy:1.203289560476939 \t\tstd entropy:0.49315683484008543\n",
            "Epoch: 3 \t\tvalue loss:423.0424645465353 \t\tpolicy loss:0.867310121871423 \t\tavg entropy:1.145623444718619 \t\tstd entropy:0.5021276983882894\n",
            "Epoch: 4 \t\tvalue loss:385.7730794713117 \t\tpolicy loss:0.8749799572903177 \t\tavg entropy:1.088810779362701 \t\tstd entropy:0.5146425296330168\n",
            "Epoch: 5 \t\tvalue loss:368.7011756344118 \t\tpolicy loss:0.9671895607658054 \t\tavg entropy:1.0050573435878192 \t\tstd entropy:0.5310091994395569\n",
            "Epoch: 6 \t\tvalue loss:416.3814754762511 \t\tpolicy loss:1.3320702279823413 \t\tavg entropy:1.043082387794727 \t\tstd entropy:0.5307633658408443\n",
            "Epoch: 7 \t\tvalue loss:458.639844811481 \t\tpolicy loss:1.3898902103520823 \t\tavg entropy:1.1544677421500285 \t\tstd entropy:0.5009133334573801\n",
            "Epoch: 8 \t\tvalue loss:436.6664357945539 \t\tpolicy loss:0.9746218237324037 \t\tavg entropy:1.1728543107561442 \t\tstd entropy:0.49152745766966727\n",
            "Epoch: 9 \t\tvalue loss:401.41181835229844 \t\tpolicy loss:0.8926520554915719 \t\tavg entropy:1.1063820330192862 \t\tstd entropy:0.5002562440620668\n",
            "Epoch: 10 \t\tvalue loss:370.32162696727806 \t\tpolicy loss:0.904060859611069 \t\tavg entropy:1.055598609700691 \t\tstd entropy:0.5197849832874678\n",
            "Episode 45 finished after 200 timesteps - cumulative reward = -757.0102087296465\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:383.27958984375 \t\tpolicy loss:1.080844732267516 \t\tavg entropy:1.0008408207398962 \t\tstd entropy:0.5376597866758255\n",
            "Epoch: 2 \t\tvalue loss:445.76305716378346 \t\tpolicy loss:1.5388233546699797 \t\tavg entropy:1.0879748878932713 \t\tstd entropy:0.5217484678544583\n",
            "Epoch: 3 \t\tvalue loss:444.2786847795759 \t\tpolicy loss:1.3653357548373086 \t\tavg entropy:1.2161318920100528 \t\tstd entropy:0.49626857368514155\n",
            "Epoch: 4 \t\tvalue loss:421.71342032296315 \t\tpolicy loss:0.9404012552329473 \t\tavg entropy:1.2011081039194944 \t\tstd entropy:0.4962340460511953\n",
            "Epoch: 5 \t\tvalue loss:386.6475522722517 \t\tpolicy loss:0.9132673736129489 \t\tavg entropy:1.135253145742537 \t\tstd entropy:0.5168779687412333\n",
            "Epoch: 6 \t\tvalue loss:374.3729984828404 \t\tpolicy loss:1.013430597952434 \t\tavg entropy:1.0430166050950012 \t\tstd entropy:0.5379980496806078\n",
            "Epoch: 7 \t\tvalue loss:426.7110791887556 \t\tpolicy loss:1.357350269811494 \t\tavg entropy:1.0665451738627014 \t\tstd entropy:0.5384326799803864\n",
            "Epoch: 8 \t\tvalue loss:452.65453055245536 \t\tpolicy loss:1.4104336338383812 \t\tavg entropy:1.18156084504463 \t\tstd entropy:0.5143914822446137\n",
            "Epoch: 9 \t\tvalue loss:429.97493940080915 \t\tpolicy loss:0.9746444225311279 \t\tavg entropy:1.1990178865345056 \t\tstd entropy:0.5069293907894202\n",
            "Epoch: 10 \t\tvalue loss:394.79978136335103 \t\tpolicy loss:0.9223418137856892 \t\tavg entropy:1.1366081144315099 \t\tstd entropy:0.5147129216280646\n",
            "Episode 46 finished after 200 timesteps - cumulative reward = -872.9424708613126\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:367.6986321585519 \t\tpolicy loss:0.9402209771530968 \t\tavg entropy:1.0671220391669904 \t\tstd entropy:0.5353858115336477\n",
            "Epoch: 2 \t\tvalue loss:397.4088950020926 \t\tpolicy loss:1.2070044887917382 \t\tavg entropy:1.0256573659950272 \t\tstd entropy:0.5511118966936747\n",
            "Epoch: 3 \t\tvalue loss:451.73323625837054 \t\tpolicy loss:1.464034784265927 \t\tavg entropy:1.1177688888340158 \t\tstd entropy:0.527776936357513\n",
            "Epoch: 4 \t\tvalue loss:437.76770368303573 \t\tpolicy loss:1.0751404617513929 \t\tavg entropy:1.1852585767426391 \t\tstd entropy:0.5088205909466348\n",
            "Epoch: 5 \t\tvalue loss:406.2283676147461 \t\tpolicy loss:0.9299674306597029 \t\tavg entropy:1.1358309566871576 \t\tstd entropy:0.5150672138250373\n",
            "Epoch: 6 \t\tvalue loss:372.84143306187224 \t\tpolicy loss:0.9348224580287934 \t\tavg entropy:1.0909127068044 \t\tstd entropy:0.5323628534538942\n",
            "Epoch: 7 \t\tvalue loss:375.3036145891462 \t\tpolicy loss:1.0537735704864775 \t\tavg entropy:1.0184105252280231 \t\tstd entropy:0.5506650112238974\n",
            "Epoch: 8 \t\tvalue loss:436.1956734793527 \t\tpolicy loss:1.4581085873501642 \t\tavg entropy:1.0726268801650285 \t\tstd entropy:0.5421910122619482\n",
            "Epoch: 9 \t\tvalue loss:444.7674822126116 \t\tpolicy loss:1.1681186888899122 \t\tavg entropy:1.1801364542241142 \t\tstd entropy:0.5126624543554557\n",
            "Epoch: 10 \t\tvalue loss:414.08016793387276 \t\tpolicy loss:0.861096420458385 \t\tavg entropy:1.132088358536556 \t\tstd entropy:0.5188887683745681\n",
            "Episode 47 finished after 200 timesteps - cumulative reward = -948.9108295027822\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:375.48045632498605 \t\tpolicy loss:0.8841078392096928 \t\tavg entropy:1.0781935921993278 \t\tstd entropy:0.5295035124220883\n",
            "Epoch: 2 \t\tvalue loss:359.59028887067524 \t\tpolicy loss:0.984249490073749 \t\tavg entropy:1.0031143000185618 \t\tstd entropy:0.5441019177937109\n",
            "Epoch: 3 \t\tvalue loss:408.1499494280134 \t\tpolicy loss:1.314187494771821 \t\tavg entropy:1.0309203890214365 \t\tstd entropy:0.54685599082867\n",
            "Epoch: 4 \t\tvalue loss:446.0062752859933 \t\tpolicy loss:1.3091519168445043 \t\tavg entropy:1.135510061124618 \t\tstd entropy:0.5173660071312417\n",
            "Epoch: 5 \t\tvalue loss:417.934816632952 \t\tpolicy loss:0.9012130920376097 \t\tavg entropy:1.1232822682601453 \t\tstd entropy:0.5148137304647911\n",
            "Epoch: 6 \t\tvalue loss:381.97450278145925 \t\tpolicy loss:0.8909597094569888 \t\tavg entropy:1.0721440144311423 \t\tstd entropy:0.5205298409813\n",
            "Epoch: 7 \t\tvalue loss:355.91473781040736 \t\tpolicy loss:0.9271075427532196 \t\tavg entropy:1.0271248606076442 \t\tstd entropy:0.5386190114755443\n",
            "Epoch: 8 \t\tvalue loss:384.1115910121373 \t\tpolicy loss:1.152650827595166 \t\tavg entropy:0.9997076594817784 \t\tstd entropy:0.5484479905948344\n",
            "Epoch: 9 \t\tvalue loss:442.7222564697266 \t\tpolicy loss:1.4657724039895195 \t\tavg entropy:1.0923628294954997 \t\tstd entropy:0.5299549011336645\n",
            "Epoch: 10 \t\tvalue loss:426.91275460379467 \t\tpolicy loss:1.0742931885378701 \t\tavg entropy:1.1640484953635297 \t\tstd entropy:0.5123921680546145\n",
            "Episode 48 finished after 200 timesteps - cumulative reward = -1012.825106333061\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:393.941667175293 \t\tpolicy loss:0.9020487036023821 \t\tavg entropy:1.1150969195086242 \t\tstd entropy:0.521081884584657\n",
            "Epoch: 2 \t\tvalue loss:361.7108234950474 \t\tpolicy loss:0.8883904989276613 \t\tavg entropy:1.0669839013267555 \t\tstd entropy:0.5395245140890113\n",
            "Epoch: 3 \t\tvalue loss:367.61236703055243 \t\tpolicy loss:1.0420177893979208 \t\tavg entropy:0.9956287966148774 \t\tstd entropy:0.5513274160064773\n",
            "Epoch: 4 \t\tvalue loss:432.5734815325056 \t\tpolicy loss:1.4734959755625043 \t\tavg entropy:1.066511351851517 \t\tstd entropy:0.5416259763969876\n",
            "Epoch: 5 \t\tvalue loss:436.2322553362165 \t\tpolicy loss:1.144168724332537 \t\tavg entropy:1.1710067219274347 \t\tstd entropy:0.5149710028322099\n",
            "Epoch: 6 \t\tvalue loss:402.45206952776226 \t\tpolicy loss:0.8492635037217822 \t\tavg entropy:1.1160362305387557 \t\tstd entropy:0.5228790965297896\n",
            "Epoch: 7 \t\tvalue loss:364.3408192225865 \t\tpolicy loss:0.8585602300507682 \t\tavg entropy:1.062028045325959 \t\tstd entropy:0.5352880704853211\n",
            "Epoch: 8 \t\tvalue loss:351.8263168334961 \t\tpolicy loss:0.9926181197166443 \t\tavg entropy:0.9862403228268412 \t\tstd entropy:0.5452178649468218\n",
            "Epoch: 9 \t\tvalue loss:409.87295009068083 \t\tpolicy loss:1.361672778214727 \t\tavg entropy:1.0322460934169655 \t\tstd entropy:0.5451485697103271\n",
            "Epoch: 10 \t\tvalue loss:443.0155116489955 \t\tpolicy loss:1.2492884985038213 \t\tavg entropy:1.1386618727427456 \t\tstd entropy:0.5140463628710409\n",
            "Episode 49 finished after 200 timesteps - cumulative reward = -757.7404629614265\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:409.4209250313895 \t\tpolicy loss:0.8628986873797008 \t\tavg entropy:1.1107246264967536 \t\tstd entropy:0.5144721656852017\n",
            "Epoch: 2 \t\tvalue loss:371.08314034598214 \t\tpolicy loss:0.8697429601635251 \t\tavg entropy:1.060120971534401 \t\tstd entropy:0.5217402725912541\n",
            "Epoch: 3 \t\tvalue loss:347.0951967511858 \t\tpolicy loss:0.9503400921821594 \t\tavg entropy:1.0097392654246518 \t\tstd entropy:0.5370550055359685\n",
            "Epoch: 4 \t\tvalue loss:384.9256927490234 \t\tpolicy loss:1.2127010353973933 \t\tavg entropy:1.0050348344675133 \t\tstd entropy:0.5432594799322833\n",
            "Epoch: 5 \t\tvalue loss:441.84769243512835 \t\tpolicy loss:1.412696692773274 \t\tavg entropy:1.1049195302558796 \t\tstd entropy:0.5189109970448433\n",
            "Epoch: 6 \t\tvalue loss:419.83184422084264 \t\tpolicy loss:1.0076946645975113 \t\tavg entropy:1.1464546939598663 \t\tstd entropy:0.5073664106689979\n",
            "Epoch: 7 \t\tvalue loss:383.86437530517577 \t\tpolicy loss:0.8985883955444608 \t\tavg entropy:1.0992108074463904 \t\tstd entropy:0.5161667947643116\n",
            "Epoch: 8 \t\tvalue loss:352.4809472220285 \t\tpolicy loss:0.8876722795622689 \t\tavg entropy:1.0553145397509847 \t\tstd entropy:0.5362755767442591\n",
            "Epoch: 9 \t\tvalue loss:366.43400224958145 \t\tpolicy loss:1.0536105858428138 \t\tavg entropy:0.9858861433556038 \t\tstd entropy:0.5459631091568861\n",
            "Epoch: 10 \t\tvalue loss:433.90897957938057 \t\tpolicy loss:1.4804608919790814 \t\tavg entropy:1.062610269367482 \t\tstd entropy:0.5359452932339842\n",
            "Episode 50 finished after 200 timesteps - cumulative reward = -845.8068819886017\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:431.3290370887434 \t\tpolicy loss:1.1200131429752833 \t\tavg entropy:1.1625834296944093 \t\tstd entropy:0.5127926809681411\n",
            "Epoch: 2 \t\tvalue loss:395.300283082774 \t\tpolicy loss:0.9129751400208809 \t\tavg entropy:1.118833834338944 \t\tstd entropy:0.5200583920120956\n",
            "Epoch: 3 \t\tvalue loss:359.15711480798853 \t\tpolicy loss:0.8720450623774193 \t\tavg entropy:1.0772293945009213 \t\tstd entropy:0.537163289094756\n",
            "Epoch: 4 \t\tvalue loss:354.58641353123625 \t\tpolicy loss:1.056773906862232 \t\tavg entropy:1.003448504271076 \t\tstd entropy:0.5462590035872775\n",
            "Epoch: 5 \t\tvalue loss:418.91279666524537 \t\tpolicy loss:1.3756203483527816 \t\tavg entropy:1.0441397194619533 \t\tstd entropy:0.5484346218628424\n",
            "Epoch: 6 \t\tvalue loss:437.288759903169 \t\tpolicy loss:1.2725740813873183 \t\tavg entropy:1.149722099512115 \t\tstd entropy:0.5284401224054579\n",
            "Epoch: 7 \t\tvalue loss:407.50457892619386 \t\tpolicy loss:0.8951774189169978 \t\tavg entropy:1.13796741009926 \t\tstd entropy:0.5317347773461131\n",
            "Epoch: 8 \t\tvalue loss:369.6919873734595 \t\tpolicy loss:0.890829968620354 \t\tavg entropy:1.0908964849754827 \t\tstd entropy:0.5394998129682126\n",
            "Epoch: 9 \t\tvalue loss:348.350654279682 \t\tpolicy loss:1.0326101389569295 \t\tavg entropy:1.0255813915541454 \t\tstd entropy:0.5499846787866133\n",
            "Epoch: 10 \t\tvalue loss:397.6546226823834 \t\tpolicy loss:1.2583370271702887 \t\tavg entropy:1.0266441080572768 \t\tstd entropy:0.5564876853817929\n",
            "Episode 51 finished after 200 timesteps - cumulative reward = -743.2687879304648\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:442.5682171029104 \t\tpolicy loss:1.3800999899985085 \t\tavg entropy:1.12036755305662 \t\tstd entropy:0.5379865581383377\n",
            "Epoch: 2 \t\tvalue loss:420.37386182328345 \t\tpolicy loss:0.9265047164869981 \t\tavg entropy:1.1367237484604331 \t\tstd entropy:0.5290882672306987\n",
            "Epoch: 3 \t\tvalue loss:383.82313516106404 \t\tpolicy loss:0.9181078840309466 \t\tavg entropy:1.094926718057726 \t\tstd entropy:0.5348624528229992\n",
            "Epoch: 4 \t\tvalue loss:354.6625374807438 \t\tpolicy loss:1.0128853295890379 \t\tavg entropy:1.0580761474194216 \t\tstd entropy:0.5479425189523953\n",
            "Epoch: 5 \t\tvalue loss:386.2969738597601 \t\tpolicy loss:1.19606979296241 \t\tavg entropy:1.0319314475967125 \t\tstd entropy:0.5523163064554861\n",
            "Epoch: 6 \t\tvalue loss:443.7762266347106 \t\tpolicy loss:1.469378176289545 \t\tavg entropy:1.1083111202188725 \t\tstd entropy:0.5406718829639394\n",
            "Epoch: 7 \t\tvalue loss:428.621083864024 \t\tpolicy loss:1.0593721904385258 \t\tavg entropy:1.1680035865281455 \t\tstd entropy:0.5284771789267364\n",
            "Epoch: 8 \t\tvalue loss:396.510444533657 \t\tpolicy loss:0.926798487633047 \t\tavg entropy:1.1354554773491425 \t\tstd entropy:0.5327257394468442\n",
            "Epoch: 9 \t\tvalue loss:361.4771208427322 \t\tpolicy loss:0.9441756740422316 \t\tavg entropy:1.0849576159432235 \t\tstd entropy:0.5500742176108596\n",
            "Epoch: 10 \t\tvalue loss:376.49057565608496 \t\tpolicy loss:1.141125093463441 \t\tavg entropy:1.0286055694683456 \t\tstd entropy:0.5570389378989952\n",
            "Episode 52 finished after 200 timesteps - cumulative reward = -848.0048155088385\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:439.1561799385178 \t\tpolicy loss:1.5265529466346957 \t\tavg entropy:1.0936943904772696 \t\tstd entropy:0.553112492239649\n",
            "Epoch: 2 \t\tvalue loss:435.56952291139413 \t\tpolicy loss:1.1220841793946816 \t\tavg entropy:1.1886688817502462 \t\tstd entropy:0.5347524285055772\n",
            "Epoch: 3 \t\tvalue loss:405.3013499085332 \t\tpolicy loss:0.9247217069209461 \t\tavg entropy:1.1571298246153185 \t\tstd entropy:0.5365745348466814\n",
            "Epoch: 4 \t\tvalue loss:366.2647619113116 \t\tpolicy loss:0.9152585603821446 \t\tavg entropy:1.0989589502476684 \t\tstd entropy:0.5488881087436306\n",
            "Epoch: 5 \t\tvalue loss:368.652285293794 \t\tpolicy loss:1.1330311159852524 \t\tavg entropy:1.0339149466814426 \t\tstd entropy:0.5550752200687807\n",
            "Epoch: 6 \t\tvalue loss:435.42443890638754 \t\tpolicy loss:1.5509335453241644 \t\tavg entropy:1.093059153530124 \t\tstd entropy:0.5535190698171482\n",
            "Epoch: 7 \t\tvalue loss:444.3391590387049 \t\tpolicy loss:1.1907404484883162 \t\tavg entropy:1.2007560438889662 \t\tstd entropy:0.53391545989535\n",
            "Epoch: 8 \t\tvalue loss:412.8926228268046 \t\tpolicy loss:0.914738011192268 \t\tavg entropy:1.171390222823168 \t\tstd entropy:0.5390163370576284\n",
            "Epoch: 9 \t\tvalue loss:371.5816263548085 \t\tpolicy loss:0.9039081217537464 \t\tavg entropy:1.1106429302407477 \t\tstd entropy:0.5455725943392142\n",
            "Epoch: 10 \t\tvalue loss:363.3123607366857 \t\tpolicy loss:1.1232185107721409 \t\tavg entropy:1.040650737397795 \t\tstd entropy:0.5517106248572164\n",
            "Episode 53 finished after 200 timesteps - cumulative reward = -765.0549454479354\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:427.5236941055513 \t\tpolicy loss:1.4986928252267167 \t\tavg entropy:1.0924726376110045 \t\tstd entropy:0.5525362961116375\n",
            "Epoch: 2 \t\tvalue loss:448.2321562431228 \t\tpolicy loss:1.2613978998761781 \t\tavg entropy:1.1981414667748462 \t\tstd entropy:0.5331357722544414\n",
            "Epoch: 3 \t\tvalue loss:416.93731990330656 \t\tpolicy loss:0.9049443090465706 \t\tavg entropy:1.1751473401311132 \t\tstd entropy:0.5386357288357636\n",
            "Epoch: 4 \t\tvalue loss:375.25987243652344 \t\tpolicy loss:0.8900289720212909 \t\tavg entropy:1.1164301780803856 \t\tstd entropy:0.5446068512130602\n",
            "Epoch: 5 \t\tvalue loss:357.39950797927213 \t\tpolicy loss:1.0923204929895804 \t\tavg entropy:1.0423305135610708 \t\tstd entropy:0.5507197668419972\n",
            "Epoch: 6 \t\tvalue loss:415.468711315746 \t\tpolicy loss:1.4210501737158063 \t\tavg entropy:1.0814115142562337 \t\tstd entropy:0.5541882062947104\n",
            "Epoch: 7 \t\tvalue loss:449.98645664268815 \t\tpolicy loss:1.3506242977061742 \t\tavg entropy:1.1833625695371401 \t\tstd entropy:0.5312458389608121\n",
            "Epoch: 8 \t\tvalue loss:420.2796317087093 \t\tpolicy loss:0.9040471479086809 \t\tavg entropy:1.1761236986085393 \t\tstd entropy:0.5291663584600925\n",
            "Epoch: 9 \t\tvalue loss:380.89794384593694 \t\tpolicy loss:0.9131618364596031 \t\tavg entropy:1.1214237739056307 \t\tstd entropy:0.5383457945338\n",
            "Epoch: 10 \t\tvalue loss:355.74444021305567 \t\tpolicy loss:1.064079934862298 \t\tavg entropy:1.0635626326588714 \t\tstd entropy:0.5473760546589458\n",
            "Episode 54 finished after 200 timesteps - cumulative reward = -847.9821550001687\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:399.603739993673 \t\tpolicy loss:1.308362408423088 \t\tavg entropy:1.0698356517559626 \t\tstd entropy:0.5520031896723058\n",
            "Epoch: 2 \t\tvalue loss:446.7068859691351 \t\tpolicy loss:1.457401907779801 \t\tavg entropy:1.1629228912770329 \t\tstd entropy:0.5314169594268263\n",
            "Epoch: 3 \t\tvalue loss:424.2307760749065 \t\tpolicy loss:0.9859557957716392 \t\tavg entropy:1.1876671886832362 \t\tstd entropy:0.5262666873031907\n",
            "Epoch: 4 \t\tvalue loss:389.4134074466329 \t\tpolicy loss:0.9383328330348915 \t\tavg entropy:1.1451029626570748 \t\tstd entropy:0.5325490537792579\n",
            "Epoch: 5 \t\tvalue loss:356.8158143809144 \t\tpolicy loss:0.9958389116005159 \t\tavg entropy:1.0949962945549 \t\tstd entropy:0.5464815951395839\n",
            "Epoch: 6 \t\tvalue loss:382.6496152206206 \t\tpolicy loss:1.1821121628015814 \t\tavg entropy:1.051882927005035 \t\tstd entropy:0.5534144564413785\n",
            "Epoch: 7 \t\tvalue loss:440.7626966019751 \t\tpolicy loss:1.5321819068680347 \t\tavg entropy:1.1271592290457075 \t\tstd entropy:0.5450883717388453\n",
            "Epoch: 8 \t\tvalue loss:428.83872104698503 \t\tpolicy loss:1.0669057306269525 \t\tavg entropy:1.199173613389403 \t\tstd entropy:0.5304793470499123\n",
            "Epoch: 9 \t\tvalue loss:398.14811083296655 \t\tpolicy loss:0.9400465866209755 \t\tavg entropy:1.161970173856532 \t\tstd entropy:0.5337232788039673\n",
            "Epoch: 10 \t\tvalue loss:359.9177931664695 \t\tpolicy loss:0.9209158789943641 \t\tavg entropy:1.1053892380863848 \t\tstd entropy:0.5501144699305857\n",
            "Episode 55 finished after 200 timesteps - cumulative reward = -626.6848305281227\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:366.1883850097656 \t\tpolicy loss:1.0923987937470276 \t\tavg entropy:1.0309860459641877 \t\tstd entropy:0.5565148568987395\n",
            "Epoch: 2 \t\tvalue loss:425.19265196058484 \t\tpolicy loss:1.6067870631814003 \t\tavg entropy:1.099727102283158 \t\tstd entropy:0.5561394720869205\n",
            "Epoch: 3 \t\tvalue loss:426.99349933200415 \t\tpolicy loss:1.2499474154578314 \t\tavg entropy:1.2210682841198874 \t\tstd entropy:0.5393040429366049\n",
            "Epoch: 4 \t\tvalue loss:406.11582226223413 \t\tpolicy loss:0.9813564804693063 \t\tavg entropy:1.2130841596953026 \t\tstd entropy:0.5347187432845932\n",
            "Epoch: 5 \t\tvalue loss:366.33831956651477 \t\tpolicy loss:0.92652328312397 \t\tavg entropy:1.1482435699339872 \t\tstd entropy:0.5432923174556447\n",
            "Epoch: 6 \t\tvalue loss:359.7185529073079 \t\tpolicy loss:1.0695437668926187 \t\tavg entropy:1.0582956369906098 \t\tstd entropy:0.5522746823586702\n",
            "Epoch: 7 \t\tvalue loss:417.4021275838216 \t\tpolicy loss:1.5005179117951128 \t\tavg entropy:1.0861444652306564 \t\tstd entropy:0.5580105260205958\n",
            "Epoch: 8 \t\tvalue loss:427.4593438042535 \t\tpolicy loss:1.2527917979492083 \t\tavg entropy:1.1959444772101562 \t\tstd entropy:0.5479249076570665\n",
            "Epoch: 9 \t\tvalue loss:404.7429551018609 \t\tpolicy loss:0.9168606479134824 \t\tavg entropy:1.184174889901946 \t\tstd entropy:0.5467512616556763\n",
            "Epoch: 10 \t\tvalue loss:366.0629319085015 \t\tpolicy loss:0.9156194453438123 \t\tavg entropy:1.130191602286349 \t\tstd entropy:0.5452951979806723\n",
            "Episode 56 finished after 200 timesteps - cumulative reward = -753.2704914895525\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:350.4023901621501 \t\tpolicy loss:1.0583117293814819 \t\tavg entropy:1.0469236490253557 \t\tstd entropy:0.5515822576715218\n",
            "Epoch: 2 \t\tvalue loss:407.4549403720432 \t\tpolicy loss:1.4365033060312271 \t\tavg entropy:1.0715335106860104 \t\tstd entropy:0.5568840241777595\n",
            "Epoch: 3 \t\tvalue loss:428.8233740064833 \t\tpolicy loss:1.2160442877146933 \t\tavg entropy:1.1628237484485244 \t\tstd entropy:0.5461131089434437\n",
            "Epoch: 4 \t\tvalue loss:401.2119322882758 \t\tpolicy loss:0.8615909015966786 \t\tavg entropy:1.1393685892082919 \t\tstd entropy:0.5479168832371999\n",
            "Epoch: 5 \t\tvalue loss:364.63161828782825 \t\tpolicy loss:0.9237753997246424 \t\tavg entropy:1.1043603064671306 \t\tstd entropy:0.5443655482319406\n",
            "Epoch: 6 \t\tvalue loss:347.0337348514133 \t\tpolicy loss:1.0754633136093616 \t\tavg entropy:1.042124206885299 \t\tstd entropy:0.5475187483188684\n",
            "Epoch: 7 \t\tvalue loss:400.1299298604329 \t\tpolicy loss:1.3934511173930433 \t\tavg entropy:1.0677871071501357 \t\tstd entropy:0.5505859285970421\n",
            "Epoch: 8 \t\tvalue loss:431.3532829284668 \t\tpolicy loss:1.2595658658279314 \t\tavg entropy:1.1571226366230545 \t\tstd entropy:0.5351553892016003\n",
            "Epoch: 9 \t\tvalue loss:402.5619167751736 \t\tpolicy loss:0.8749909876949258 \t\tavg entropy:1.1376288856444907 \t\tstd entropy:0.5333140374981856\n",
            "Epoch: 10 \t\tvalue loss:366.7727035946316 \t\tpolicy loss:0.9303529684742292 \t\tavg entropy:1.1040812674508083 \t\tstd entropy:0.5364366049743196\n",
            "Episode 57 finished after 200 timesteps - cumulative reward = -660.4883429092968\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:346.9649480183919 \t\tpolicy loss:1.067685879766941 \t\tavg entropy:1.0498411065135047 \t\tstd entropy:0.5432999343781327\n",
            "Epoch: 2 \t\tvalue loss:393.23062981499567 \t\tpolicy loss:1.2969327589703932 \t\tavg entropy:1.0548554754634933 \t\tstd entropy:0.5492955598303979\n",
            "Epoch: 3 \t\tvalue loss:426.68210262722437 \t\tpolicy loss:1.2585077583789825 \t\tavg entropy:1.1356895945894443 \t\tstd entropy:0.5335787348180393\n",
            "Epoch: 4 \t\tvalue loss:396.9319029914008 \t\tpolicy loss:0.8271896984014246 \t\tavg entropy:1.1191906684720316 \t\tstd entropy:0.5281576001781573\n",
            "Epoch: 5 \t\tvalue loss:359.27318530612524 \t\tpolicy loss:0.9066096535987325 \t\tavg entropy:1.0700630747383284 \t\tstd entropy:0.5379176367049567\n",
            "Epoch: 6 \t\tvalue loss:335.61007351345484 \t\tpolicy loss:0.985243811375565 \t\tavg entropy:1.0161382521680613 \t\tstd entropy:0.5462106340698073\n",
            "Epoch: 7 \t\tvalue loss:378.68849605984155 \t\tpolicy loss:1.2420738037261698 \t\tavg entropy:1.0131554493983406 \t\tstd entropy:0.5506446342685962\n",
            "Epoch: 8 \t\tvalue loss:421.81849416097003 \t\tpolicy loss:1.3091994929644797 \t\tavg entropy:1.1041643472881624 \t\tstd entropy:0.5331330016182906\n",
            "Epoch: 9 \t\tvalue loss:396.2844996982151 \t\tpolicy loss:0.8510472952491708 \t\tavg entropy:1.1128432406873219 \t\tstd entropy:0.5229917519976544\n",
            "Epoch: 10 \t\tvalue loss:360.9109174940321 \t\tpolicy loss:0.9079154051012464 \t\tavg entropy:1.06704385441858 \t\tstd entropy:0.5340012686501998\n",
            "Episode 58 finished after 200 timesteps - cumulative reward = -813.7393288236403\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:334.2582427130805 \t\tpolicy loss:0.9554627574980259 \t\tavg entropy:1.0170674205656502 \t\tstd entropy:0.54610202316829\n",
            "Epoch: 2 \t\tvalue loss:371.11397128634985 \t\tpolicy loss:1.1972436242633395 \t\tavg entropy:0.9954221061773827 \t\tstd entropy:0.5499037201619984\n",
            "Epoch: 3 \t\tvalue loss:419.3733639187283 \t\tpolicy loss:1.3703766953614023 \t\tavg entropy:1.0904678568485762 \t\tstd entropy:0.5367848943694018\n",
            "Epoch: 4 \t\tvalue loss:398.60170449150934 \t\tpolicy loss:0.9281707120438417 \t\tavg entropy:1.1235465536303921 \t\tstd entropy:0.5256256606684531\n",
            "Epoch: 5 \t\tvalue loss:366.52281824747723 \t\tpolicy loss:0.9296463160879083 \t\tavg entropy:1.0940653744828692 \t\tstd entropy:0.5320101284091221\n",
            "Epoch: 6 \t\tvalue loss:337.2564228905572 \t\tpolicy loss:0.9512946833339002 \t\tavg entropy:1.0448483886790765 \t\tstd entropy:0.5468634878176658\n",
            "Epoch: 7 \t\tvalue loss:368.61701668633356 \t\tpolicy loss:1.174846652481291 \t\tavg entropy:1.0014819627931772 \t\tstd entropy:0.5506838608483925\n",
            "Epoch: 8 \t\tvalue loss:416.850282880995 \t\tpolicy loss:1.3786260270410113 \t\tavg entropy:1.0872783128886088 \t\tstd entropy:0.5424101631585755\n",
            "Epoch: 9 \t\tvalue loss:398.22844823201496 \t\tpolicy loss:0.9397964771423075 \t\tavg entropy:1.126334047075828 \t\tstd entropy:0.5300707027842518\n",
            "Epoch: 10 \t\tvalue loss:367.3672154744466 \t\tpolicy loss:0.9446214843127463 \t\tavg entropy:1.1002478183087865 \t\tstd entropy:0.533649833457752\n",
            "Episode 59 finished after 200 timesteps - cumulative reward = -728.0510396277745\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:337.5093871222602 \t\tpolicy loss:0.952866675125228 \t\tavg entropy:1.0548918242759795 \t\tstd entropy:0.5491464236940301\n",
            "Epoch: 2 \t\tvalue loss:367.24960242377387 \t\tpolicy loss:1.1773941574825182 \t\tavg entropy:1.006239717569823 \t\tstd entropy:0.5512579760030991\n",
            "Epoch: 3 \t\tvalue loss:416.4585486518012 \t\tpolicy loss:1.3717875911129847 \t\tavg entropy:1.087368189399648 \t\tstd entropy:0.5426741157071793\n",
            "Epoch: 4 \t\tvalue loss:398.65638097127277 \t\tpolicy loss:0.9412727103465133 \t\tavg entropy:1.1247614132038628 \t\tstd entropy:0.5301145855958239\n",
            "Epoch: 5 \t\tvalue loss:368.27256117926703 \t\tpolicy loss:0.9519228513042132 \t\tavg entropy:1.1017863219615278 \t\tstd entropy:0.5337894945189589\n",
            "Epoch: 6 \t\tvalue loss:339.40452088250055 \t\tpolicy loss:0.9759068265557289 \t\tavg entropy:1.0591630750016 \t\tstd entropy:0.5491299861653689\n",
            "Epoch: 7 \t\tvalue loss:369.0217577616374 \t\tpolicy loss:1.1880719719661608 \t\tavg entropy:1.0145547443250609 \t\tstd entropy:0.5511437348210937\n",
            "Epoch: 8 \t\tvalue loss:416.27556228637695 \t\tpolicy loss:1.3447036056054964 \t\tavg entropy:1.0919836033554748 \t\tstd entropy:0.5407923556631594\n",
            "Epoch: 9 \t\tvalue loss:397.205628712972 \t\tpolicy loss:0.9252197626564238 \t\tavg entropy:1.119599880136446 \t\tstd entropy:0.5287636078826271\n",
            "Epoch: 10 \t\tvalue loss:366.51122856140137 \t\tpolicy loss:0.9506715320878558 \t\tavg entropy:1.0969220961794879 \t\tstd entropy:0.5337747416145251\n",
            "Episode 60 finished after 200 timesteps - cumulative reward = -496.2149477254227\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:337.11940002441406 \t\tpolicy loss:1.0009982585906982 \t\tavg entropy:1.0549806510058548 \t\tstd entropy:0.5491404483775693\n",
            "Epoch: 2 \t\tvalue loss:368.451835945861 \t\tpolicy loss:1.2656472166107124 \t\tavg entropy:1.0311131642447202 \t\tstd entropy:0.5484992528217826\n",
            "Epoch: 3 \t\tvalue loss:405.0244090459118 \t\tpolicy loss:1.3027782611650964 \t\tavg entropy:1.110744085871612 \t\tstd entropy:0.5319829338344912\n",
            "Epoch: 4 \t\tvalue loss:381.8639501284247 \t\tpolicy loss:0.8633425472533867 \t\tavg entropy:1.1188377281236062 \t\tstd entropy:0.521450456186266\n",
            "Epoch: 5 \t\tvalue loss:353.5038088445794 \t\tpolicy loss:0.9780459289681421 \t\tavg entropy:1.0826379459667075 \t\tstd entropy:0.5325652790068051\n",
            "Epoch: 6 \t\tvalue loss:331.5216959078018 \t\tpolicy loss:1.023702418150967 \t\tavg entropy:1.049531454602953 \t\tstd entropy:0.5428055882506089\n",
            "Epoch: 7 \t\tvalue loss:368.01081221071007 \t\tpolicy loss:1.2451037948262202 \t\tavg entropy:1.0295170710728068 \t\tstd entropy:0.5408163732026519\n",
            "Epoch: 8 \t\tvalue loss:403.63350583429207 \t\tpolicy loss:1.262227044530111 \t\tavg entropy:1.1094787566918993 \t\tstd entropy:0.5234807144683383\n",
            "Epoch: 9 \t\tvalue loss:380.2933696590058 \t\tpolicy loss:0.8604177730540706 \t\tavg entropy:1.112257617838949 \t\tstd entropy:0.5147067533202397\n",
            "Epoch: 10 \t\tvalue loss:350.62407987411706 \t\tpolicy loss:0.95881635440539 \t\tavg entropy:1.0745709115071764 \t\tstd entropy:0.5294855741266943\n",
            "Episode 61 finished after 200 timesteps - cumulative reward = -797.717331138003\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:331.9271704268782 \t\tpolicy loss:1.0506192649880501 \t\tavg entropy:1.0344469505653444 \t\tstd entropy:0.5385380169068928\n",
            "Epoch: 2 \t\tvalue loss:373.303710519451 \t\tpolicy loss:1.2238174350294349 \t\tavg entropy:1.027002739526064 \t\tstd entropy:0.5411402077052023\n",
            "Epoch: 3 \t\tvalue loss:397.8627336057898 \t\tpolicy loss:1.1316787494372016 \t\tavg entropy:1.093103552078571 \t\tstd entropy:0.52766917477849\n",
            "Epoch: 4 \t\tvalue loss:367.963745326212 \t\tpolicy loss:0.7712939744942808 \t\tavg entropy:1.0702636347510415 \t\tstd entropy:0.5247697790506022\n",
            "Epoch: 5 \t\tvalue loss:334.40524208382385 \t\tpolicy loss:0.9075080515587166 \t\tavg entropy:1.0213706300841923 \t\tstd entropy:0.537907661979925\n",
            "Epoch: 6 \t\tvalue loss:322.2209930419922 \t\tpolicy loss:1.0076228849691888 \t\tavg entropy:0.9676512954558004 \t\tstd entropy:0.5427843499271395\n",
            "Epoch: 7 \t\tvalue loss:375.55373685653893 \t\tpolicy loss:1.3049426544202518 \t\tavg entropy:0.9995421729017646 \t\tstd entropy:0.5448772459418171\n",
            "Epoch: 8 \t\tvalue loss:395.44210690014984 \t\tpolicy loss:1.0586250897956222 \t\tavg entropy:1.0854735548637848 \t\tstd entropy:0.5236126151717593\n",
            "Epoch: 9 \t\tvalue loss:362.58554056245987 \t\tpolicy loss:0.7772547798613979 \t\tavg entropy:1.0486787683082466 \t\tstd entropy:0.5250126966253372\n",
            "Epoch: 10 \t\tvalue loss:327.84396634036545 \t\tpolicy loss:0.885208691636177 \t\tavg entropy:1.0044884596332826 \t\tstd entropy:0.5375124819582139\n",
            "Episode 62 finished after 200 timesteps - cumulative reward = -749.1850596874937\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:323.90693789965485 \t\tpolicy loss:1.0154350257083162 \t\tavg entropy:0.9453627047228008 \t\tstd entropy:0.5391988025468097\n",
            "Epoch: 2 \t\tvalue loss:388.9632840091235 \t\tpolicy loss:1.4205484676034483 \t\tavg entropy:1.0027876078020832 \t\tstd entropy:0.5449081737086673\n",
            "Epoch: 3 \t\tvalue loss:403.5313724883615 \t\tpolicy loss:1.0905331189501775 \t\tavg entropy:1.1137457953363699 \t\tstd entropy:0.527747803699494\n",
            "Epoch: 4 \t\tvalue loss:370.5277833285397 \t\tpolicy loss:0.8713643261014599 \t\tavg entropy:1.0827894056868579 \t\tstd entropy:0.5293014063213196\n",
            "Epoch: 5 \t\tvalue loss:333.110962541136 \t\tpolicy loss:0.8664105273272893 \t\tavg entropy:1.032155615352402 \t\tstd entropy:0.5421711221613448\n",
            "Epoch: 6 \t\tvalue loss:337.833482089108 \t\tpolicy loss:1.0796283328369871 \t\tavg entropy:0.9596133739496107 \t\tstd entropy:0.54522762925914\n",
            "Epoch: 7 \t\tvalue loss:406.56552333047944 \t\tpolicy loss:1.526656739515801 \t\tavg entropy:1.039377302318996 \t\tstd entropy:0.5540066473206106\n",
            "Epoch: 8 \t\tvalue loss:405.48334241893195 \t\tpolicy loss:1.0669543396120202 \t\tavg entropy:1.152716194550217 \t\tstd entropy:0.5358650242252507\n",
            "Epoch: 9 \t\tvalue loss:372.1401636829115 \t\tpolicy loss:0.9351355302823733 \t\tavg entropy:1.1220796500838082 \t\tstd entropy:0.5328886168887143\n",
            "Epoch: 10 \t\tvalue loss:335.71896634036545 \t\tpolicy loss:0.9035153307326852 \t\tavg entropy:1.0655930035319128 \t\tstd entropy:0.5462680167592148\n",
            "Episode 63 finished after 200 timesteps - cumulative reward = -631.7273098128676\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:351.1138232505485 \t\tpolicy loss:1.132284753126641 \t\tavg entropy:0.9911139077143641 \t\tstd entropy:0.5525870178756118\n",
            "Epoch: 2 \t\tvalue loss:417.8188601977205 \t\tpolicy loss:1.5461632580789801 \t\tavg entropy:1.0785089848526233 \t\tstd entropy:0.5567416910794857\n",
            "Epoch: 3 \t\tvalue loss:405.7194606833262 \t\tpolicy loss:1.0448945793387008 \t\tavg entropy:1.1675233316551485 \t\tstd entropy:0.5342202062230956\n",
            "Epoch: 4 \t\tvalue loss:371.2794139287243 \t\tpolicy loss:0.9720481503499697 \t\tavg entropy:1.1464023275091058 \t\tstd entropy:0.5305036339004291\n",
            "Epoch: 5 \t\tvalue loss:338.54317652036065 \t\tpolicy loss:0.9749419485052971 \t\tavg entropy:1.0913670373611808 \t\tstd entropy:0.5444386635034959\n",
            "Epoch: 6 \t\tvalue loss:366.94878272487693 \t\tpolicy loss:1.2122136983152938 \t\tavg entropy:1.0367616739758587 \t\tstd entropy:0.5540194621077555\n",
            "Epoch: 7 \t\tvalue loss:421.4906277591235 \t\tpolicy loss:1.4488289086786035 \t\tavg entropy:1.1198012135876854 \t\tstd entropy:0.5499514757096715\n",
            "Epoch: 8 \t\tvalue loss:399.0320079228649 \t\tpolicy loss:0.9624208269053942 \t\tavg entropy:1.1645076115072224 \t\tstd entropy:0.5315581175273069\n",
            "Epoch: 9 \t\tvalue loss:364.5464030226616 \t\tpolicy loss:0.9718102935242326 \t\tavg entropy:1.132516025740776 \t\tstd entropy:0.5354280692197411\n",
            "Epoch: 10 \t\tvalue loss:338.5143553851402 \t\tpolicy loss:1.0291974520030087 \t\tavg entropy:1.0771318263273972 \t\tstd entropy:0.5478378523803166\n",
            "Episode 64 finished after 200 timesteps - cumulative reward = -728.8664354160767\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:376.7220622023491 \t\tpolicy loss:1.278481710447024 \t\tavg entropy:1.052118133190356 \t\tstd entropy:0.5566713223501889\n",
            "Epoch: 2 \t\tvalue loss:412.82983273022796 \t\tpolicy loss:1.2875165588235202 \t\tavg entropy:1.1325285017741462 \t\tstd entropy:0.5431680500440808\n",
            "Epoch: 3 \t\tvalue loss:384.3311439409648 \t\tpolicy loss:0.8392965026097755 \t\tavg entropy:1.133389054977016 \t\tstd entropy:0.5322048931372428\n",
            "Epoch: 4 \t\tvalue loss:348.7822228000589 \t\tpolicy loss:0.9318863382078197 \t\tavg entropy:1.0783089615044323 \t\tstd entropy:0.5460182703567482\n",
            "Epoch: 5 \t\tvalue loss:329.2029594525899 \t\tpolicy loss:1.0098513940425768 \t\tavg entropy:1.0200088349611383 \t\tstd entropy:0.5532696738392184\n",
            "Epoch: 6 \t\tvalue loss:373.14724773250214 \t\tpolicy loss:1.2881816058126214 \t\tavg entropy:1.0249332234285364 \t\tstd entropy:0.5590741298975539\n",
            "Epoch: 7 \t\tvalue loss:404.4948705385809 \t\tpolicy loss:1.1797610186550715 \t\tavg entropy:1.1130526079615093 \t\tstd entropy:0.5404587218067534\n",
            "Epoch: 8 \t\tvalue loss:375.35555152370506 \t\tpolicy loss:0.7999318115515252 \t\tavg entropy:1.0945112040544005 \t\tstd entropy:0.5328374287951105\n",
            "Epoch: 9 \t\tvalue loss:340.01306298660904 \t\tpolicy loss:0.8967809660793984 \t\tavg entropy:1.0404971145151616 \t\tstd entropy:0.5464353631747956\n",
            "Epoch: 10 \t\tvalue loss:324.8697936175621 \t\tpolicy loss:0.9948678428996099 \t\tavg entropy:0.9834667167821771 \t\tstd entropy:0.55031435950425\n",
            "Episode 65 finished after 200 timesteps - cumulative reward = -723.9349330786432\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:373.88634140427047 \t\tpolicy loss:1.3211612810154219 \t\tavg entropy:1.0052012317078545 \t\tstd entropy:0.557988897789416\n",
            "Epoch: 2 \t\tvalue loss:397.59014150258656 \t\tpolicy loss:1.2940974066386353 \t\tavg entropy:1.1227147139940676 \t\tstd entropy:0.5431800344861926\n",
            "Epoch: 3 \t\tvalue loss:374.72092293404245 \t\tpolicy loss:0.8893163747884132 \t\tavg entropy:1.1229283137854615 \t\tstd entropy:0.5383734188377292\n",
            "Epoch: 4 \t\tvalue loss:339.9998630832981 \t\tpolicy loss:0.8697143487028174 \t\tavg entropy:1.073447903956139 \t\tstd entropy:0.5517727023447794\n",
            "Epoch: 5 \t\tvalue loss:328.54754164412213 \t\tpolicy loss:0.9880763985015251 \t\tavg entropy:0.9888962966699377 \t\tstd entropy:0.5574242330570413\n",
            "Epoch: 6 \t\tvalue loss:381.74847700789167 \t\tpolicy loss:1.4771329011466052 \t\tavg entropy:1.0194643309633018 \t\tstd entropy:0.5659168367154759\n",
            "Epoch: 7 \t\tvalue loss:397.94836095861484 \t\tpolicy loss:1.19425463515359 \t\tavg entropy:1.145478067239085 \t\tstd entropy:0.5502953187777024\n",
            "Epoch: 8 \t\tvalue loss:372.38891931482266 \t\tpolicy loss:0.8799394357043344 \t\tavg entropy:1.1288160240773335 \t\tstd entropy:0.5452139853466333\n",
            "Epoch: 9 \t\tvalue loss:334.31436342806427 \t\tpolicy loss:0.8682361704272192 \t\tavg entropy:1.0728354646553555 \t\tstd entropy:0.5506926655667237\n",
            "Epoch: 10 \t\tvalue loss:329.5236156566723 \t\tpolicy loss:1.0076883145280786 \t\tavg entropy:0.9803199542166736 \t\tstd entropy:0.5560860752539645\n",
            "Episode 66 finished after 200 timesteps - cumulative reward = -949.4261297496215\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:386.20716156830656 \t\tpolicy loss:1.5453952575052106 \t\tavg entropy:1.0318408708216507 \t\tstd entropy:0.5687256739965456\n",
            "Epoch: 2 \t\tvalue loss:393.12997931403083 \t\tpolicy loss:1.0375082827097661 \t\tavg entropy:1.138941953099188 \t\tstd entropy:0.5531728569814585\n",
            "Epoch: 3 \t\tvalue loss:362.4483722996067 \t\tpolicy loss:0.890269972182609 \t\tavg entropy:1.1080944462496107 \t\tstd entropy:0.5465267946075508\n",
            "Epoch: 4 \t\tvalue loss:326.90145853403453 \t\tpolicy loss:0.9139767901317494 \t\tavg entropy:1.0650307058464807 \t\tstd entropy:0.5444290655328421\n",
            "Epoch: 5 \t\tvalue loss:335.1602537825301 \t\tpolicy loss:1.1008794658087395 \t\tavg entropy:0.9893396867939578 \t\tstd entropy:0.5490910495068299\n",
            "Epoch: 6 \t\tvalue loss:395.5010248132654 \t\tpolicy loss:1.4618735023446985 \t\tavg entropy:1.054169674394627 \t\tstd entropy:0.5577061789874151\n",
            "Epoch: 7 \t\tvalue loss:387.8819567706134 \t\tpolicy loss:1.0168802637506176 \t\tavg entropy:1.1338232270617048 \t\tstd entropy:0.5445505074319938\n",
            "Epoch: 8 \t\tvalue loss:357.1482838811101 \t\tpolicy loss:0.9244034902469532 \t\tavg entropy:1.1162836765628867 \t\tstd entropy:0.5391502614730119\n",
            "Epoch: 9 \t\tvalue loss:325.67393782332135 \t\tpolicy loss:0.9259251138648471 \t\tavg entropy:1.0651067248610044 \t\tstd entropy:0.5448242595228863\n",
            "Epoch: 10 \t\tvalue loss:350.29675952808276 \t\tpolicy loss:1.1864267374212678 \t\tavg entropy:1.007019442646112 \t\tstd entropy:0.5518880403986691\n",
            "Episode 67 finished after 200 timesteps - cumulative reward = -613.4106833619544\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:402.6482300629487 \t\tpolicy loss:1.4036777671929952 \t\tavg entropy:1.0887987093112188 \t\tstd entropy:0.5534933911581152\n",
            "Epoch: 2 \t\tvalue loss:378.9332246522646 \t\tpolicy loss:0.9585608090903308 \t\tavg entropy:1.1333559770927684 \t\tstd entropy:0.5379931742137595\n",
            "Epoch: 3 \t\tvalue loss:347.9674191861539 \t\tpolicy loss:0.9477604249039212 \t\tavg entropy:1.1150178017899515 \t\tstd entropy:0.541140839424451\n",
            "Epoch: 4 \t\tvalue loss:324.63343213055583 \t\tpolicy loss:0.9894326297012536 \t\tavg entropy:1.054591701826202 \t\tstd entropy:0.5491431189709635\n",
            "Epoch: 5 \t\tvalue loss:365.4048292830184 \t\tpolicy loss:1.2823495711829211 \t\tavg entropy:1.0282721729043103 \t\tstd entropy:0.5584339314258289\n",
            "Epoch: 6 \t\tvalue loss:399.4944179638012 \t\tpolicy loss:1.2508025338520874 \t\tavg entropy:1.1163940864589834 \t\tstd entropy:0.5493928651238149\n",
            "Epoch: 7 \t\tvalue loss:370.5748915801177 \t\tpolicy loss:0.8440933231566403 \t\tavg entropy:1.1192421324238302 \t\tstd entropy:0.5396304992154523\n",
            "Epoch: 8 \t\tvalue loss:336.2596984038482 \t\tpolicy loss:0.9402671391899521 \t\tavg entropy:1.0737869718555442 \t\tstd entropy:0.5493564539654034\n",
            "Epoch: 9 \t\tvalue loss:323.4903484035183 \t\tpolicy loss:1.0299917274230235 \t\tavg entropy:1.014600987651426 \t\tstd entropy:0.550430252266153\n",
            "Epoch: 10 \t\tvalue loss:375.5700271194046 \t\tpolicy loss:1.376441619283444 \t\tavg entropy:1.0274011556904399 \t\tstd entropy:0.5590405119584764\n",
            "Episode 68 finished after 200 timesteps - cumulative reward = -724.8006558492872\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:394.1285159136798 \t\tpolicy loss:1.1445039397961385 \t\tavg entropy:1.1266981899136612 \t\tstd entropy:0.5457183648348228\n",
            "Epoch: 2 \t\tvalue loss:366.5296321559597 \t\tpolicy loss:0.8518670346285846 \t\tavg entropy:1.110442816389467 \t\tstd entropy:0.5457854365042104\n",
            "Epoch: 3 \t\tvalue loss:332.091569436563 \t\tpolicy loss:0.9483621579569739 \t\tavg entropy:1.0700055030187172 \t\tstd entropy:0.5490015381566054\n",
            "Epoch: 4 \t\tvalue loss:329.42446631354255 \t\tpolicy loss:1.0877936679769207 \t\tavg entropy:1.0034065652705304 \t\tstd entropy:0.5503212268024363\n",
            "Epoch: 5 \t\tvalue loss:389.0625160835885 \t\tpolicy loss:1.4674729269904059 \t\tavg entropy:1.054768146813663 \t\tstd entropy:0.5574776999520341\n",
            "Epoch: 6 \t\tvalue loss:394.16417570371885 \t\tpolicy loss:1.1034724378102534 \t\tavg entropy:1.156006701921484 \t\tstd entropy:0.5413248565649578\n",
            "Epoch: 7 \t\tvalue loss:365.916124498522 \t\tpolicy loss:0.9115873273160007 \t\tavg entropy:1.139166902925486 \t\tstd entropy:0.5403511424181547\n",
            "Epoch: 8 \t\tvalue loss:331.5869571582691 \t\tpolicy loss:0.9450987730477307 \t\tavg entropy:1.0852662674866638 \t\tstd entropy:0.5475272927758379\n",
            "Epoch: 9 \t\tvalue loss:341.36630187163485 \t\tpolicy loss:1.1445081024556547 \t\tavg entropy:1.015159352069019 \t\tstd entropy:0.554216438527407\n",
            "Epoch: 10 \t\tvalue loss:399.03104050095016 \t\tpolicy loss:1.48283742046034 \t\tavg entropy:1.08413469328911 \t\tstd entropy:0.5567764615225322\n",
            "Episode 69 finished after 200 timesteps - cumulative reward = -628.3611009863075\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:389.2418971706081 \t\tpolicy loss:1.0522081412979074 \t\tavg entropy:1.1677132869984048 \t\tstd entropy:0.5377508102142118\n",
            "Epoch: 2 \t\tvalue loss:360.04962838662635 \t\tpolicy loss:0.9570433054421399 \t\tavg entropy:1.1538973102326933 \t\tstd entropy:0.5366610432564527\n",
            "Epoch: 3 \t\tvalue loss:329.65576254354943 \t\tpolicy loss:0.9780134695607263 \t\tavg entropy:1.0905521993830398 \t\tstd entropy:0.5483674293038764\n",
            "Epoch: 4 \t\tvalue loss:352.78169621648016 \t\tpolicy loss:1.1931241307709668 \t\tavg entropy:1.0346874522238567 \t\tstd entropy:0.5575306087331102\n",
            "Epoch: 5 \t\tvalue loss:398.7701721191406 \t\tpolicy loss:1.3859059496505841 \t\tavg entropy:1.105576870685997 \t\tstd entropy:0.5551499949940809\n",
            "Epoch: 6 \t\tvalue loss:376.8327620222762 \t\tpolicy loss:0.9297101115052765 \t\tavg entropy:1.1492598081957046 \t\tstd entropy:0.5377758284352263\n",
            "Epoch: 7 \t\tvalue loss:347.719861829603 \t\tpolicy loss:0.9664820658194052 \t\tavg entropy:1.1207073756545984 \t\tstd entropy:0.5456946730628425\n",
            "Epoch: 8 \t\tvalue loss:325.4029466783678 \t\tpolicy loss:1.0107122780503452 \t\tavg entropy:1.0595997229278893 \t\tstd entropy:0.5566035536115741\n",
            "Epoch: 9 \t\tvalue loss:362.376512269716 \t\tpolicy loss:1.2721502531219173 \t\tavg entropy:1.0349581115531257 \t\tstd entropy:0.5638426851740919\n",
            "Epoch: 10 \t\tvalue loss:393.6949056676916 \t\tpolicy loss:1.2284591093256667 \t\tavg entropy:1.1190811732443604 \t\tstd entropy:0.5485694487160964\n",
            "Episode 70 finished after 200 timesteps - cumulative reward = -625.3344718666472\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:366.67699076334634 \t\tpolicy loss:0.8397363301118215 \t\tavg entropy:1.1210880087087096 \t\tstd entropy:0.5414496259765337\n",
            "Epoch: 2 \t\tvalue loss:333.385498046875 \t\tpolicy loss:0.9190271564324697 \t\tavg entropy:1.0650267440139887 \t\tstd entropy:0.5594241368782071\n",
            "Epoch: 3 \t\tvalue loss:322.9053312174479 \t\tpolicy loss:1.022778768936793 \t\tavg entropy:0.9982670623346701 \t\tstd entropy:0.5652462747289135\n",
            "Epoch: 4 \t\tvalue loss:369.7138936360677 \t\tpolicy loss:1.3781618038813273 \t\tavg entropy:1.0215568885888975 \t\tstd entropy:0.5699400927537982\n",
            "Epoch: 5 \t\tvalue loss:377.1296565755208 \t\tpolicy loss:1.109370885292689 \t\tavg entropy:1.124142726525122 \t\tstd entropy:0.5494645294844709\n",
            "Epoch: 6 \t\tvalue loss:354.28046834309896 \t\tpolicy loss:0.8314417131741841 \t\tavg entropy:1.102246008855755 \t\tstd entropy:0.553712843796432\n",
            "Epoch: 7 \t\tvalue loss:324.7076542154948 \t\tpolicy loss:0.904103681643804 \t\tavg entropy:1.0445261510599635 \t\tstd entropy:0.5630767615916589\n",
            "Epoch: 8 \t\tvalue loss:326.3173103841146 \t\tpolicy loss:1.08304856300354 \t\tavg entropy:0.9770021320442165 \t\tstd entropy:0.5670901626295329\n",
            "Epoch: 9 \t\tvalue loss:377.21422912597654 \t\tpolicy loss:1.4116180268923442 \t\tavg entropy:1.0310393343973312 \t\tstd entropy:0.5710887813625029\n",
            "Epoch: 10 \t\tvalue loss:371.5792293294271 \t\tpolicy loss:1.0420402236779531 \t\tavg entropy:1.1209171046519457 \t\tstd entropy:0.5538157699048238\n",
            "Episode 71 finished after 200 timesteps - cumulative reward = -567.4798383709559\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:347.94528849283853 \t\tpolicy loss:0.8863777728875478 \t\tavg entropy:1.1069177947130795 \t\tstd entropy:0.5572729686969053\n",
            "Epoch: 2 \t\tvalue loss:321.5242352294922 \t\tpolicy loss:0.9468062833944957 \t\tavg entropy:1.0517944784533053 \t\tstd entropy:0.5651866327260917\n",
            "Epoch: 3 \t\tvalue loss:340.28538798014324 \t\tpolicy loss:1.1866587106386821 \t\tavg entropy:1.000300204316805 \t\tstd entropy:0.5679315244559838\n",
            "Epoch: 4 \t\tvalue loss:383.8484501139323 \t\tpolicy loss:1.5075980559984843 \t\tavg entropy:1.0901023148393298 \t\tstd entropy:0.5601674359195092\n",
            "Epoch: 5 \t\tvalue loss:363.4062827555339 \t\tpolicy loss:0.967242834965388 \t\tavg entropy:1.1390542834905046 \t\tstd entropy:0.5441055598700283\n",
            "Epoch: 6 \t\tvalue loss:337.71782836914065 \t\tpolicy loss:0.9429192399978638 \t\tavg entropy:1.1202051427840225 \t\tstd entropy:0.5463352980143366\n",
            "Epoch: 7 \t\tvalue loss:320.2735652669271 \t\tpolicy loss:1.024893708229065 \t\tavg entropy:1.062643782492647 \t\tstd entropy:0.5531183885641191\n",
            "Epoch: 8 \t\tvalue loss:360.152861328125 \t\tpolicy loss:1.3326188373565673 \t\tavg entropy:1.0481936349420338 \t\tstd entropy:0.5612765058612169\n",
            "Epoch: 9 \t\tvalue loss:382.25984334309896 \t\tpolicy loss:1.175491058031718 \t\tavg entropy:1.1285398468487364 \t\tstd entropy:0.5472037523264506\n",
            "Epoch: 10 \t\tvalue loss:351.83581115722654 \t\tpolicy loss:0.8430806525548299 \t\tavg entropy:1.1055153422231376 \t\tstd entropy:0.5467400029416819\n",
            "Episode 72 finished after 200 timesteps - cumulative reward = -753.0633894713511\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:323.86845052083333 \t\tpolicy loss:0.9628623096148173 \t\tavg entropy:1.0828724467097695 \t\tstd entropy:0.5442178644212702\n",
            "Epoch: 2 \t\tvalue loss:322.23985961914065 \t\tpolicy loss:1.1237076314290364 \t\tavg entropy:1.0232564148221954 \t\tstd entropy:0.5480750490221946\n",
            "Epoch: 3 \t\tvalue loss:374.2585978190104 \t\tpolicy loss:1.4354179271062215 \t\tavg entropy:1.0751883535906073 \t\tstd entropy:0.5535044332441685\n",
            "Epoch: 4 \t\tvalue loss:370.6702998860677 \t\tpolicy loss:0.9875427047411601 \t\tavg entropy:1.1449238628607452 \t\tstd entropy:0.5405023023724884\n",
            "Epoch: 5 \t\tvalue loss:339.97367513020833 \t\tpolicy loss:0.8733997742335001 \t\tavg entropy:1.109212177035626 \t\tstd entropy:0.5469162183270596\n",
            "Epoch: 6 \t\tvalue loss:311.64402201334633 \t\tpolicy loss:0.9828158410390219 \t\tavg entropy:1.0603444479167181 \t\tstd entropy:0.5475496941563789\n",
            "Epoch: 7 \t\tvalue loss:332.8918096923828 \t\tpolicy loss:1.1742078872521717 \t\tavg entropy:1.008113463028676 \t\tstd entropy:0.5506001208385665\n",
            "Epoch: 8 \t\tvalue loss:382.22929728190104 \t\tpolicy loss:1.401893085638682 \t\tavg entropy:1.0888332426624907 \t\tstd entropy:0.5512225400044947\n",
            "Epoch: 9 \t\tvalue loss:359.35715372721353 \t\tpolicy loss:0.9419195679823558 \t\tavg entropy:1.1380385870399607 \t\tstd entropy:0.5381934737845719\n",
            "Epoch: 10 \t\tvalue loss:330.5919950358073 \t\tpolicy loss:0.9709166514873505 \t\tavg entropy:1.1180914412875318 \t\tstd entropy:0.5424242519292444\n",
            "Episode 73 finished after 200 timesteps - cumulative reward = -838.5431817380993\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:311.36807535807293 \t\tpolicy loss:1.0290910240014395 \t\tavg entropy:1.0632294315709807 \t\tstd entropy:0.550254439287611\n",
            "Epoch: 2 \t\tvalue loss:355.1423468017578 \t\tpolicy loss:1.3114621710777283 \t\tavg entropy:1.0514137131328332 \t\tstd entropy:0.5593295082223032\n",
            "Epoch: 3 \t\tvalue loss:380.1405932617188 \t\tpolicy loss:1.1662140925725302 \t\tavg entropy:1.1323380146913709 \t\tstd entropy:0.542847424214867\n",
            "Epoch: 4 \t\tvalue loss:347.60847513834636 \t\tpolicy loss:0.8379822206497193 \t\tavg entropy:1.1231547432953621 \t\tstd entropy:0.5396075695153362\n",
            "Epoch: 5 \t\tvalue loss:316.1137953694661 \t\tpolicy loss:0.9930110923449198 \t\tavg entropy:1.0834483185525048 \t\tstd entropy:0.5447225190053765\n",
            "Epoch: 6 \t\tvalue loss:316.4539685058594 \t\tpolicy loss:1.1243763077259064 \t\tavg entropy:1.0265451386852817 \t\tstd entropy:0.5458803003914244\n",
            "Epoch: 7 \t\tvalue loss:373.4854457600911 \t\tpolicy loss:1.4600659724076588 \t\tavg entropy:1.0811982920771852 \t\tstd entropy:0.5487628139313047\n",
            "Epoch: 8 \t\tvalue loss:370.35797627766925 \t\tpolicy loss:1.0550581204891205 \t\tavg entropy:1.1662547753759247 \t\tstd entropy:0.5284310642740888\n",
            "Epoch: 9 \t\tvalue loss:341.49096537272135 \t\tpolicy loss:0.8999395259221394 \t\tavg entropy:1.1400948403183033 \t\tstd entropy:0.5348553100317024\n",
            "Epoch: 10 \t\tvalue loss:311.57664408365883 \t\tpolicy loss:0.9654852358500162 \t\tavg entropy:1.0805713016802276 \t\tstd entropy:0.5438084127581586\n",
            "Episode 74 finished after 200 timesteps - cumulative reward = -610.8387746273464\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:333.36012084960936 \t\tpolicy loss:1.1727444609006246 \t\tavg entropy:1.0252540837388238 \t\tstd entropy:0.5471395760956352\n",
            "Epoch: 2 \t\tvalue loss:385.8459385172526 \t\tpolicy loss:1.46119393547376 \t\tavg entropy:1.1059677412559092 \t\tstd entropy:0.5442431643939102\n",
            "Epoch: 3 \t\tvalue loss:363.16434488932293 \t\tpolicy loss:1.0126351710160573 \t\tavg entropy:1.1685885410918793 \t\tstd entropy:0.52229776110295\n",
            "Epoch: 4 \t\tvalue loss:335.18589477539064 \t\tpolicy loss:1.016441601117452 \t\tavg entropy:1.1603500291752815 \t\tstd entropy:0.5268391372881028\n",
            "Epoch: 5 \t\tvalue loss:315.1616455078125 \t\tpolicy loss:1.0474722969532013 \t\tavg entropy:1.0948181091523668 \t\tstd entropy:0.5409894915654732\n",
            "Epoch: 6 \t\tvalue loss:359.2208262125651 \t\tpolicy loss:1.3343052836259206 \t\tavg entropy:1.0788342309967678 \t\tstd entropy:0.5501637317804964\n",
            "Epoch: 7 \t\tvalue loss:382.8711442057292 \t\tpolicy loss:1.2229060292243958 \t\tavg entropy:1.1634326545299092 \t\tstd entropy:0.5281574527880024\n",
            "Epoch: 8 \t\tvalue loss:351.7419313557943 \t\tpolicy loss:0.8769688705603281 \t\tavg entropy:1.1629186376589538 \t\tstd entropy:0.5173158029654291\n",
            "Epoch: 9 \t\tvalue loss:319.5832800292969 \t\tpolicy loss:1.0039717729886373 \t\tavg entropy:1.114834862102866 \t\tstd entropy:0.532645366932816\n",
            "Epoch: 10 \t\tvalue loss:318.97137552897135 \t\tpolicy loss:1.1246269416809083 \t\tavg entropy:1.052215651316444 \t\tstd entropy:0.5356513710116054\n",
            "Episode 75 finished after 200 timesteps - cumulative reward = -499.01183988881075\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:374.0481097572728 \t\tpolicy loss:1.4793242141604424 \t\tavg entropy:1.1019323263177647 \t\tstd entropy:0.5362547874665199\n",
            "Epoch: 2 \t\tvalue loss:359.79093511481034 \t\tpolicy loss:1.0138760663961108 \t\tavg entropy:1.1731727366146454 \t\tstd entropy:0.5124951953750909\n",
            "Epoch: 3 \t\tvalue loss:331.1714186417429 \t\tpolicy loss:0.9444193314564856 \t\tavg entropy:1.1538271939045095 \t\tstd entropy:0.5158068216667774\n",
            "Epoch: 4 \t\tvalue loss:301.97972970259815 \t\tpolicy loss:0.9812941076724153 \t\tavg entropy:1.0931838129117015 \t\tstd entropy:0.5303194732012708\n",
            "Epoch: 5 \t\tvalue loss:318.53834854929073 \t\tpolicy loss:1.1448256545945217 \t\tavg entropy:1.038149407865539 \t\tstd entropy:0.5305047156141898\n",
            "Epoch: 6 \t\tvalue loss:364.98925640708524 \t\tpolicy loss:1.3380941078066826 \t\tavg entropy:1.0901959254474756 \t\tstd entropy:0.5299535906314228\n",
            "Epoch: 7 \t\tvalue loss:343.25066857588916 \t\tpolicy loss:0.944357448110455 \t\tavg entropy:1.1326290045818315 \t\tstd entropy:0.5168576607596278\n",
            "Epoch: 8 \t\tvalue loss:317.47592223318 \t\tpolicy loss:0.9989062190840119 \t\tavg entropy:1.1197931503006158 \t\tstd entropy:0.5211381544200602\n",
            "Epoch: 9 \t\tvalue loss:296.34245139674135 \t\tpolicy loss:0.9797137387489018 \t\tavg entropy:1.06000585882818 \t\tstd entropy:0.5358374356738115\n",
            "Epoch: 10 \t\tvalue loss:333.79802723934773 \t\tpolicy loss:1.2256038777138059 \t\tavg entropy:1.0347031719201898 \t\tstd entropy:0.5401603576818303\n",
            "Episode 76 finished after 200 timesteps - cumulative reward = -611.0368936288876\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:363.71953703227797 \t\tpolicy loss:1.1226214072421978 \t\tavg entropy:1.0983394223933827 \t\tstd entropy:0.5248100730954272\n",
            "Epoch: 2 \t\tvalue loss:333.1226810656096 \t\tpolicy loss:0.8768469126600968 \t\tavg entropy:1.0943960440947875 \t\tstd entropy:0.5207250712196198\n",
            "Epoch: 3 \t\tvalue loss:303.41495554070724 \t\tpolicy loss:0.9989247227969923 \t\tavg entropy:1.0712495430920579 \t\tstd entropy:0.5307747014429488\n",
            "Epoch: 4 \t\tvalue loss:301.1559753417969 \t\tpolicy loss:1.1028360689156933 \t\tavg entropy:1.022840503097539 \t\tstd entropy:0.5328044205749173\n",
            "Epoch: 5 \t\tvalue loss:361.4351955213045 \t\tpolicy loss:1.460576646422085 \t\tavg entropy:1.0766002405926447 \t\tstd entropy:0.5311215976185785\n",
            "Epoch: 6 \t\tvalue loss:360.21975567466336 \t\tpolicy loss:1.0977532196986048 \t\tavg entropy:1.1686364470663375 \t\tstd entropy:0.49841523253931064\n",
            "Epoch: 7 \t\tvalue loss:329.62139772114 \t\tpolicy loss:0.9894877652588644 \t\tavg entropy:1.1669615422423425 \t\tstd entropy:0.49671506409264726\n",
            "Epoch: 8 \t\tvalue loss:299.28519740857575 \t\tpolicy loss:0.9580454144038653 \t\tavg entropy:1.0993457323237166 \t\tstd entropy:0.5208126128890675\n",
            "Epoch: 9 \t\tvalue loss:325.5950947811729 \t\tpolicy loss:1.2082564626869403 \t\tavg entropy:1.0481448144806593 \t\tstd entropy:0.5239540141978959\n",
            "Epoch: 10 \t\tvalue loss:378.237934915643 \t\tpolicy loss:1.4453394185555608 \t\tavg entropy:1.1266381503157417 \t\tstd entropy:0.5137471203674004\n",
            "Episode 77 finished after 200 timesteps - cumulative reward = -732.4161748960854\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:347.8944009479724 \t\tpolicy loss:1.0070786209482896 \t\tavg entropy:1.1792426520195745 \t\tstd entropy:0.4877294644424852\n",
            "Epoch: 2 \t\tvalue loss:316.8071120412726 \t\tpolicy loss:1.034053667595512 \t\tavg entropy:1.1609945090642326 \t\tstd entropy:0.5028719233248352\n",
            "Epoch: 3 \t\tvalue loss:302.6885783546849 \t\tpolicy loss:1.0773380669324022 \t\tavg entropy:1.0918075362396815 \t\tstd entropy:0.5204237896278185\n",
            "Epoch: 4 \t\tvalue loss:358.66266471461245 \t\tpolicy loss:1.4114945836757358 \t\tavg entropy:1.1022117973963617 \t\tstd entropy:0.525213505430861\n",
            "Epoch: 5 \t\tvalue loss:371.742346914191 \t\tpolicy loss:1.1869742242913497 \t\tavg entropy:1.1880296063112716 \t\tstd entropy:0.49360660421237407\n",
            "Epoch: 6 \t\tvalue loss:337.6376465245297 \t\tpolicy loss:0.943939095264987 \t\tavg entropy:1.1888355726966506 \t\tstd entropy:0.4818426275078947\n",
            "Epoch: 7 \t\tvalue loss:301.42069585699784 \t\tpolicy loss:0.971801273916897 \t\tavg entropy:1.1192846003289596 \t\tstd entropy:0.5112243486813409\n",
            "Epoch: 8 \t\tvalue loss:317.5671605561909 \t\tpolicy loss:1.1629854805375401 \t\tavg entropy:1.0565788769176496 \t\tstd entropy:0.5138885034299052\n",
            "Epoch: 9 \t\tvalue loss:377.8018481605931 \t\tpolicy loss:1.5269221596811946 \t\tavg entropy:1.1259652376175913 \t\tstd entropy:0.5094348742406913\n",
            "Epoch: 10 \t\tvalue loss:357.86479508249386 \t\tpolicy loss:1.0882341751926823 \t\tavg entropy:1.1959503318695597 \t\tstd entropy:0.4838176439737165\n",
            "Episode 78 finished after 200 timesteps - cumulative reward = -464.9390944878412\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:325.77336220992237 \t\tpolicy loss:1.0478028278601796 \t\tavg entropy:1.1973450488378945 \t\tstd entropy:0.4859256590378742\n",
            "Epoch: 2 \t\tvalue loss:303.1029843782124 \t\tpolicy loss:1.0226316326542904 \t\tavg entropy:1.1249300949058758 \t\tstd entropy:0.5088904422728194\n",
            "Epoch: 3 \t\tvalue loss:347.0897082278603 \t\tpolicy loss:1.298357181251049 \t\tavg entropy:1.098731288231349 \t\tstd entropy:0.5145045442256685\n",
            "Epoch: 4 \t\tvalue loss:374.9338121916118 \t\tpolicy loss:1.2422763415073093 \t\tavg entropy:1.1671305473981362 \t\tstd entropy:0.4930048214034769\n",
            "Epoch: 5 \t\tvalue loss:340.7049668964587 \t\tpolicy loss:0.9160395699896311 \t\tavg entropy:1.1722405097328894 \t\tstd entropy:0.47753386019291\n",
            "Epoch: 6 \t\tvalue loss:305.529130032188 \t\tpolicy loss:0.9953177908533498 \t\tavg entropy:1.1219562592681762 \t\tstd entropy:0.505207872195264\n",
            "Epoch: 7 \t\tvalue loss:305.3243572837428 \t\tpolicy loss:1.123287085639803 \t\tavg entropy:1.063627255308469 \t\tstd entropy:0.5104770099676454\n",
            "Epoch: 8 \t\tvalue loss:365.02128741615695 \t\tpolicy loss:1.4932709312752674 \t\tavg entropy:1.1096621946155003 \t\tstd entropy:0.5120359329149118\n",
            "Epoch: 9 \t\tvalue loss:360.26770641929227 \t\tpolicy loss:1.0994781451789957 \t\tavg entropy:1.18959430047333 \t\tstd entropy:0.48638147392433406\n",
            "Epoch: 10 \t\tvalue loss:330.3378075047543 \t\tpolicy loss:0.9857802736131769 \t\tavg entropy:1.1827888175613175 \t\tstd entropy:0.4857863914660097\n",
            "Episode 79 finished after 200 timesteps - cumulative reward = -736.3265718996379\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:300.02277173494036 \t\tpolicy loss:0.9574006913523925 \t\tavg entropy:1.1146176966453265 \t\tstd entropy:0.5107435275288627\n",
            "Epoch: 2 \t\tvalue loss:325.66269563373766 \t\tpolicy loss:1.194817584988318 \t\tavg entropy:1.0642340451519152 \t\tstd entropy:0.5118900418103453\n",
            "Epoch: 3 \t\tvalue loss:376.49876122725635 \t\tpolicy loss:1.4415883261122202 \t\tavg entropy:1.1316096549401256 \t\tstd entropy:0.5043359659163441\n",
            "Epoch: 4 \t\tvalue loss:349.2019410384329 \t\tpolicy loss:1.0139786922617962 \t\tavg entropy:1.1782126850557366 \t\tstd entropy:0.48338311613232765\n",
            "Epoch: 5 \t\tvalue loss:317.86693552920696 \t\tpolicy loss:1.0250044825829958 \t\tavg entropy:1.163826120916656 \t\tstd entropy:0.4941641521507074\n",
            "Epoch: 6 \t\tvalue loss:299.80919888145047 \t\tpolicy loss:1.0457686130937778 \t\tavg entropy:1.0987063715279495 \t\tstd entropy:0.5121597941677974\n",
            "Epoch: 7 \t\tvalue loss:347.988727168033 \t\tpolicy loss:1.3265361425123716 \t\tavg entropy:1.0947859563328521 \t\tstd entropy:0.515411100174877\n",
            "Epoch: 8 \t\tvalue loss:368.78713848716336 \t\tpolicy loss:1.1816066738806272 \t\tavg entropy:1.1637964117230946 \t\tstd entropy:0.48866286387582863\n",
            "Epoch: 9 \t\tvalue loss:334.7681525380988 \t\tpolicy loss:0.9047711934698256 \t\tavg entropy:1.1589874147530645 \t\tstd entropy:0.47700986112347754\n",
            "Epoch: 10 \t\tvalue loss:299.346682297556 \t\tpolicy loss:0.9762282002913324 \t\tavg entropy:1.102743897524123 \t\tstd entropy:0.5053685304899388\n",
            "Episode 80 finished after 200 timesteps - cumulative reward = -390.89223508055125\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:303.817538372882 \t\tpolicy loss:1.1351337189024144 \t\tavg entropy:1.0509622717461014 \t\tstd entropy:0.507003988915468\n",
            "Epoch: 2 \t\tvalue loss:356.2564152308873 \t\tpolicy loss:1.430534674749746 \t\tavg entropy:1.0962888865724811 \t\tstd entropy:0.5052703361193207\n",
            "Epoch: 3 \t\tvalue loss:345.93726140802556 \t\tpolicy loss:1.168165343535411 \t\tavg entropy:1.1828211295474662 \t\tstd entropy:0.47811690667752565\n",
            "Epoch: 4 \t\tvalue loss:325.2358132894937 \t\tpolicy loss:0.9676300109206856 \t\tavg entropy:1.1836964107197256 \t\tstd entropy:0.47740024334249065\n",
            "Epoch: 5 \t\tvalue loss:295.6077062433416 \t\tpolicy loss:0.9582411534600443 \t\tavg entropy:1.1069023796644188 \t\tstd entropy:0.5086954461391494\n",
            "Epoch: 6 \t\tvalue loss:320.01661186713676 \t\tpolicy loss:1.1596360071138903 \t\tavg entropy:1.0463925199091846 \t\tstd entropy:0.5127729336045126\n",
            "Epoch: 7 \t\tvalue loss:360.39729725230825 \t\tpolicy loss:1.3061656955774728 \t\tavg entropy:1.096649692572966 \t\tstd entropy:0.5090365164683013\n",
            "Epoch: 8 \t\tvalue loss:332.9990912103034 \t\tpolicy loss:1.043068630354745 \t\tavg entropy:1.1425950882676745 \t\tstd entropy:0.48548311818481127\n",
            "Epoch: 9 \t\tvalue loss:311.44829331435164 \t\tpolicy loss:1.000251211903312 \t\tavg entropy:1.142097173873775 \t\tstd entropy:0.496663864152728\n",
            "Epoch: 10 \t\tvalue loss:293.72802734375 \t\tpolicy loss:0.9921532760966908 \t\tavg entropy:1.066267667145681 \t\tstd entropy:0.517350949664295\n",
            "Episode 81 finished after 200 timesteps - cumulative reward = -503.5905931276065\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:335.4584126658254 \t\tpolicy loss:1.2524150284853848 \t\tavg entropy:1.051554304158793 \t\tstd entropy:0.5206213554204684\n",
            "Epoch: 2 \t\tvalue loss:345.4856789328835 \t\tpolicy loss:1.1065360543789802 \t\tavg entropy:1.1137734261851426 \t\tstd entropy:0.5029297441552286\n",
            "Epoch: 3 \t\tvalue loss:320.5762444037896 \t\tpolicy loss:0.8820639515851999 \t\tavg entropy:1.1153059677525146 \t\tstd entropy:0.48746427634278056\n",
            "Epoch: 4 \t\tvalue loss:290.3705749511719 \t\tpolicy loss:0.9741329767487266 \t\tavg entropy:1.062755184119655 \t\tstd entropy:0.5147668019600616\n",
            "Epoch: 5 \t\tvalue loss:296.8641200870662 \t\tpolicy loss:1.1076823999355365 \t\tavg entropy:1.0187507636427235 \t\tstd entropy:0.5081481868686416\n",
            "Epoch: 6 \t\tvalue loss:347.7792655647575 \t\tpolicy loss:1.3940075984248868 \t\tavg entropy:1.0731712261786324 \t\tstd entropy:0.5061954651897708\n",
            "Epoch: 7 \t\tvalue loss:332.850017795315 \t\tpolicy loss:1.0172144437765147 \t\tavg entropy:1.1347847496951242 \t\tstd entropy:0.4873879515120402\n",
            "Epoch: 8 \t\tvalue loss:309.40535864891945 \t\tpolicy loss:0.9841556270401199 \t\tavg entropy:1.1301904669073435 \t\tstd entropy:0.48870329975814725\n",
            "Epoch: 9 \t\tvalue loss:285.8632655948787 \t\tpolicy loss:0.9617137727025268 \t\tavg entropy:1.064426299117759 \t\tstd entropy:0.5069720792887811\n",
            "Epoch: 10 \t\tvalue loss:322.04598146909245 \t\tpolicy loss:1.2262846103736333 \t\tavg entropy:1.0403261957418524 \t\tstd entropy:0.5050927261826824\n",
            "Episode 82 finished after 200 timesteps - cumulative reward = -407.73757489720043\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:350.07400948660717 \t\tpolicy loss:1.178322492094783 \t\tavg entropy:1.1080157069897878 \t\tstd entropy:0.4912590769255024\n",
            "Epoch: 2 \t\tvalue loss:320.78363017292764 \t\tpolicy loss:0.8804782067800497 \t\tavg entropy:1.1142043566943072 \t\tstd entropy:0.4752003400140974\n",
            "Epoch: 3 \t\tvalue loss:287.8942653111049 \t\tpolicy loss:0.9801113992542415 \t\tavg entropy:1.0639892251785679 \t\tstd entropy:0.5026140063620126\n",
            "Epoch: 4 \t\tvalue loss:291.4175216872971 \t\tpolicy loss:1.1095893866830058 \t\tavg entropy:1.022847264945439 \t\tstd entropy:0.5007448501629969\n",
            "Epoch: 5 \t\tvalue loss:348.9905003138951 \t\tpolicy loss:1.4368860427435342 \t\tavg entropy:1.080897187550866 \t\tstd entropy:0.5014717634489491\n",
            "Epoch: 6 \t\tvalue loss:335.69283661285004 \t\tpolicy loss:1.0288057683350205 \t\tavg entropy:1.1485581658051078 \t\tstd entropy:0.48459281664152903\n",
            "Epoch: 7 \t\tvalue loss:308.7604219510958 \t\tpolicy loss:1.011408272114667 \t\tavg entropy:1.1404907877125605 \t\tstd entropy:0.4851724410172931\n",
            "Epoch: 8 \t\tvalue loss:283.7740880792791 \t\tpolicy loss:0.9711515214536097 \t\tavg entropy:1.0790523716679947 \t\tstd entropy:0.5027981375752889\n",
            "Epoch: 9 \t\tvalue loss:322.55045942826706 \t\tpolicy loss:1.2277217770551707 \t\tavg entropy:1.0528610201254875 \t\tstd entropy:0.5007591300256784\n",
            "Epoch: 10 \t\tvalue loss:353.42321460278004 \t\tpolicy loss:1.2113899844033378 \t\tavg entropy:1.122619538924878 \t\tstd entropy:0.48702341413249917\n",
            "Episode 83 finished after 200 timesteps - cumulative reward = -483.29749206921593\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:321.12665290337105 \t\tpolicy loss:0.9110861486428744 \t\tavg entropy:1.1321883156489154 \t\tstd entropy:0.47065759218055014\n",
            "Epoch: 2 \t\tvalue loss:285.19891813203884 \t\tpolicy loss:0.9754307827392181 \t\tavg entropy:1.0817019021360708 \t\tstd entropy:0.49715362920663475\n",
            "Epoch: 3 \t\tvalue loss:288.4895574396307 \t\tpolicy loss:1.1112902671485752 \t\tavg entropy:1.0361748800475714 \t\tstd entropy:0.49454207807662987\n",
            "Epoch: 4 \t\tvalue loss:350.40563766677656 \t\tpolicy loss:1.4887612105964065 \t\tavg entropy:1.0977252747982857 \t\tstd entropy:0.4928901074577256\n",
            "Epoch: 5 \t\tvalue loss:337.29856416776585 \t\tpolicy loss:1.0634522546421399 \t\tavg entropy:1.1697378503795093 \t\tstd entropy:0.475233217138047\n",
            "Epoch: 6 \t\tvalue loss:308.58593432934254 \t\tpolicy loss:1.0505521150378438 \t\tavg entropy:1.1626501628465484 \t\tstd entropy:0.4739454295417028\n",
            "Epoch: 7 \t\tvalue loss:281.5913531811206 \t\tpolicy loss:0.963906603200095 \t\tavg entropy:1.1012284879397096 \t\tstd entropy:0.49192892710263236\n",
            "Epoch: 8 \t\tvalue loss:320.7027233173321 \t\tpolicy loss:1.2268406562990957 \t\tavg entropy:1.0676044421622162 \t\tstd entropy:0.49040336069446966\n",
            "Epoch: 9 \t\tvalue loss:353.92019534420655 \t\tpolicy loss:1.2464729106271422 \t\tavg entropy:1.1375664958865188 \t\tstd entropy:0.47509056440372727\n",
            "Epoch: 10 \t\tvalue loss:321.78785091251524 \t\tpolicy loss:0.9479291988657667 \t\tavg entropy:1.1527721762421423 \t\tstd entropy:0.4550216164197322\n",
            "Episode 84 finished after 200 timesteps - cumulative reward = -415.0184745194486\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:285.06522597275773 \t\tpolicy loss:0.9801672426137057 \t\tavg entropy:1.1029241544796855 \t\tstd entropy:0.48466668025477405\n",
            "Epoch: 2 \t\tvalue loss:282.0425761829723 \t\tpolicy loss:1.0757879998002733 \t\tavg entropy:1.0497858813204741 \t\tstd entropy:0.4864102356541525\n",
            "Epoch: 3 \t\tvalue loss:341.0668251731179 \t\tpolicy loss:1.4328023980964313 \t\tavg entropy:1.0914497837845187 \t\tstd entropy:0.48664695041272393\n",
            "Epoch: 4 \t\tvalue loss:338.1256283846769 \t\tpolicy loss:1.1107202427727836 \t\tavg entropy:1.167755334867479 \t\tstd entropy:0.4702469275074531\n",
            "Epoch: 5 \t\tvalue loss:312.17260088239397 \t\tpolicy loss:1.0109793497370434 \t\tavg entropy:1.1657534808467154 \t\tstd entropy:0.4619326379905499\n",
            "Epoch: 6 \t\tvalue loss:279.439921985973 \t\tpolicy loss:0.9484198108896033 \t\tavg entropy:1.09894719162893 \t\tstd entropy:0.486833284813235\n",
            "Epoch: 7 \t\tvalue loss:302.89514814104353 \t\tpolicy loss:1.147902240226795 \t\tavg entropy:1.0521786086633802 \t\tstd entropy:0.4817862601473011\n",
            "Epoch: 8 \t\tvalue loss:352.9721459723138 \t\tpolicy loss:1.4054953177253922 \t\tavg entropy:1.1172692632091703 \t\tstd entropy:0.47864029831145016\n",
            "Epoch: 9 \t\tvalue loss:328.85823197798294 \t\tpolicy loss:1.0240489136088977 \t\tavg entropy:1.1575476540063883 \t\tstd entropy:0.46401110993077505\n",
            "Epoch: 10 \t\tvalue loss:298.723784013228 \t\tpolicy loss:1.0320642900157284 \t\tavg entropy:1.145815322563052 \t\tstd entropy:0.47108279090822625\n",
            "Episode 85 finished after 200 timesteps - cumulative reward = -697.8454767980116\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:277.7687808305789 \t\tpolicy loss:1.004838608014278 \t\tavg entropy:1.0832339889551343 \t\tstd entropy:0.4842176221589834\n",
            "Epoch: 2 \t\tvalue loss:323.92403333615033 \t\tpolicy loss:1.342320971764051 \t\tavg entropy:1.085283710610826 \t\tstd entropy:0.47748020464654156\n",
            "Epoch: 3 \t\tvalue loss:341.2504661755684 \t\tpolicy loss:1.2547529232807648 \t\tavg entropy:1.1662275298925147 \t\tstd entropy:0.4561997903731963\n",
            "Epoch: 4 \t\tvalue loss:318.5801923702925 \t\tpolicy loss:0.9855905221058772 \t\tavg entropy:1.1717786633690404 \t\tstd entropy:0.4361027317445269\n",
            "Epoch: 5 \t\tvalue loss:281.5909691835061 \t\tpolicy loss:1.0124507393592443 \t\tavg entropy:1.13119335838415 \t\tstd entropy:0.4564998934620234\n",
            "Epoch: 6 \t\tvalue loss:283.216059366862 \t\tpolicy loss:1.0999450607177539 \t\tavg entropy:1.0684254868510976 \t\tstd entropy:0.46127807210306715\n",
            "Epoch: 7 \t\tvalue loss:340.8380383222531 \t\tpolicy loss:1.4799870474216266 \t\tavg entropy:1.119402633166819 \t\tstd entropy:0.4658112503428394\n",
            "Epoch: 8 \t\tvalue loss:334.6253362802359 \t\tpolicy loss:1.1624091183527923 \t\tavg entropy:1.1815810976580867 \t\tstd entropy:0.45905996245230307\n",
            "Epoch: 9 \t\tvalue loss:312.8946323883839 \t\tpolicy loss:1.0581313142409692 \t\tavg entropy:1.1921072842165288 \t\tstd entropy:0.44476627678333164\n",
            "Epoch: 10 \t\tvalue loss:278.1189223069411 \t\tpolicy loss:1.00191485728973 \t\tavg entropy:1.1403723577539306 \t\tstd entropy:0.4594705443764179\n",
            "Episode 86 finished after 200 timesteps - cumulative reward = -613.6532723384174\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:302.3885100927108 \t\tpolicy loss:1.1615643791663341 \t\tavg entropy:1.0852745007207611 \t\tstd entropy:0.4586417659995667\n",
            "Epoch: 2 \t\tvalue loss:344.1199636214819 \t\tpolicy loss:1.3196041102592762 \t\tavg entropy:1.1325563247690522 \t\tstd entropy:0.46647951846700575\n",
            "Epoch: 3 \t\tvalue loss:321.0257253402319 \t\tpolicy loss:1.009365398150224 \t\tavg entropy:1.1533701835710373 \t\tstd entropy:0.46309837908288953\n",
            "Epoch: 4 \t\tvalue loss:295.4585485213842 \t\tpolicy loss:1.0576669191702819 \t\tavg entropy:1.1478560095894552 \t\tstd entropy:0.4639948328115758\n",
            "Epoch: 5 \t\tvalue loss:272.0190621400491 \t\tpolicy loss:0.9926054836847843 \t\tavg entropy:1.0913345643379353 \t\tstd entropy:0.47278150315366013\n",
            "Epoch: 6 \t\tvalue loss:313.25536718124 \t\tpolicy loss:1.2468005605997183 \t\tavg entropy:1.0770729048281562 \t\tstd entropy:0.47002065843906443\n",
            "Epoch: 7 \t\tvalue loss:333.8789056631235 \t\tpolicy loss:1.1514602204163868 \t\tavg entropy:1.132548156741246 \t\tstd entropy:0.4663478067744369\n",
            "Epoch: 8 \t\tvalue loss:310.1518775744316 \t\tpolicy loss:0.9183194186442938 \t\tavg entropy:1.1288729598542593 \t\tstd entropy:0.452078431850423\n",
            "Epoch: 9 \t\tvalue loss:276.4502227000701 \t\tpolicy loss:0.9907470719936566 \t\tavg entropy:1.0848834929201339 \t\tstd entropy:0.47189939705385126\n",
            "Epoch: 10 \t\tvalue loss:271.72290802001953 \t\tpolicy loss:1.0619239914111602 \t\tavg entropy:1.0364690323112393 \t\tstd entropy:0.4713506222974535\n",
            "Episode 87 finished after 200 timesteps - cumulative reward = -499.44177621806256\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:323.4913220527845 \t\tpolicy loss:1.386232398259334 \t\tavg entropy:1.0823626804025843 \t\tstd entropy:0.47184527677811927\n",
            "Epoch: 2 \t\tvalue loss:320.72082265218097 \t\tpolicy loss:1.0212522672537045 \t\tavg entropy:1.1427205569204903 \t\tstd entropy:0.46771330641158604\n",
            "Epoch: 3 \t\tvalue loss:299.07998539851263 \t\tpolicy loss:0.958672875777269 \t\tavg entropy:1.1265058028909196 \t\tstd entropy:0.46274180881827\n",
            "Epoch: 4 \t\tvalue loss:266.87335107265375 \t\tpolicy loss:0.939607384113165 \t\tavg entropy:1.0623477804736647 \t\tstd entropy:0.48209298727180977\n",
            "Epoch: 5 \t\tvalue loss:281.75548377403845 \t\tpolicy loss:1.1114548719846284 \t\tavg entropy:1.0209418761578013 \t\tstd entropy:0.4740265414579677\n",
            "Epoch: 6 \t\tvalue loss:329.9824028993264 \t\tpolicy loss:1.3592172203919826 \t\tavg entropy:1.087110736684048 \t\tstd entropy:0.47577933753432294\n",
            "Epoch: 7 \t\tvalue loss:313.4582024598733 \t\tpolicy loss:0.9802977817180829 \t\tavg entropy:1.1320441752219976 \t\tstd entropy:0.4682406469962413\n",
            "Epoch: 8 \t\tvalue loss:287.31749431903546 \t\tpolicy loss:0.9952168946082776 \t\tavg entropy:1.1118669506701997 \t\tstd entropy:0.4707012567376086\n",
            "Epoch: 9 \t\tvalue loss:261.693239065317 \t\tpolicy loss:0.924230083441123 \t\tavg entropy:1.0451577228386455 \t\tstd entropy:0.48147172597682086\n",
            "Epoch: 10 \t\tvalue loss:298.17708959334936 \t\tpolicy loss:1.2013062624595103 \t\tavg entropy:1.0253722209107496 \t\tstd entropy:0.4743773265166416\n",
            "Episode 88 finished after 200 timesteps - cumulative reward = -482.2219418498054\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:328.11685630602716 \t\tpolicy loss:1.1857546323384993 \t\tavg entropy:1.0992752863750206 \t\tstd entropy:0.46828685755571153\n",
            "Epoch: 2 \t\tvalue loss:303.6801022260617 \t\tpolicy loss:0.9084401126855459 \t\tavg entropy:1.1068974800750728 \t\tstd entropy:0.45564273494667057\n",
            "Epoch: 3 \t\tvalue loss:269.3405642387194 \t\tpolicy loss:0.9660924558456128 \t\tavg entropy:1.0638523038814758 \t\tstd entropy:0.47439724457907784\n",
            "Epoch: 4 \t\tvalue loss:261.3695295284956 \t\tpolicy loss:1.0011314146029644 \t\tavg entropy:1.0114037951520598 \t\tstd entropy:0.4733339211587939\n",
            "Epoch: 5 \t\tvalue loss:312.57537978734723 \t\tpolicy loss:1.341308125318625 \t\tavg entropy:1.0455391063458814 \t\tstd entropy:0.4714817876077947\n",
            "Epoch: 6 \t\tvalue loss:315.900268359062 \t\tpolicy loss:1.065520037825291 \t\tavg entropy:1.1192302899120092 \t\tstd entropy:0.4641498239280228\n",
            "Epoch: 7 \t\tvalue loss:293.6816648825621 \t\tpolicy loss:0.933696768222711 \t\tavg entropy:1.1105732782125703 \t\tstd entropy:0.45471219494825216\n",
            "Epoch: 8 \t\tvalue loss:259.2531935863006 \t\tpolicy loss:0.9325469671151577 \t\tavg entropy:1.048479112739409 \t\tstd entropy:0.4752765063187023\n",
            "Epoch: 9 \t\tvalue loss:273.6524984897711 \t\tpolicy loss:1.0916636005426064 \t\tavg entropy:1.0062117151233696 \t\tstd entropy:0.46536125197788636\n",
            "Epoch: 10 \t\tvalue loss:323.08472892565607 \t\tpolicy loss:1.3661793951804821 \t\tavg entropy:1.0712329218580667 \t\tstd entropy:0.4669501155986002\n",
            "Episode 89 finished after 200 timesteps - cumulative reward = -496.8066224306918\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:307.9738949506711 \t\tpolicy loss:1.0010091738823133 \t\tavg entropy:1.1212230792354123 \t\tstd entropy:0.46156841689421824\n",
            "Epoch: 2 \t\tvalue loss:282.3100810906826 \t\tpolicy loss:0.9902463517127893 \t\tavg entropy:1.1097603483926026 \t\tstd entropy:0.4624853851238863\n",
            "Epoch: 3 \t\tvalue loss:257.44774275559644 \t\tpolicy loss:0.9229766252713326 \t\tavg entropy:1.0451421405872665 \t\tstd entropy:0.4735295397235211\n",
            "Epoch: 4 \t\tvalue loss:295.13936282426886 \t\tpolicy loss:1.20318852747098 \t\tavg entropy:1.0237615511388733 \t\tstd entropy:0.4662711992934565\n",
            "Epoch: 5 \t\tvalue loss:320.4596481323242 \t\tpolicy loss:1.152535427839328 \t\tavg entropy:1.0922986112729862 \t\tstd entropy:0.46359975790482055\n",
            "Epoch: 6 \t\tvalue loss:297.6327571379833 \t\tpolicy loss:0.8950498646650559 \t\tavg entropy:1.0948123204772098 \t\tstd entropy:0.4510726183898008\n",
            "Epoch: 7 \t\tvalue loss:262.53871565598706 \t\tpolicy loss:0.9688283067483169 \t\tavg entropy:1.053743248491715 \t\tstd entropy:0.4719562599003269\n",
            "Epoch: 8 \t\tvalue loss:260.0670522054036 \t\tpolicy loss:1.0324694865789168 \t\tavg entropy:1.005856872880688 \t\tstd entropy:0.46821969225299115\n",
            "Epoch: 9 \t\tvalue loss:311.4123951838567 \t\tpolicy loss:1.3547556900825255 \t\tavg entropy:1.0491678328882807 \t\tstd entropy:0.4674362152650603\n",
            "Epoch: 10 \t\tvalue loss:309.02481431227466 \t\tpolicy loss:1.0319515218337376 \t\tavg entropy:1.112340161051028 \t\tstd entropy:0.4660663175283741\n",
            "Episode 90 finished after 200 timesteps - cumulative reward = -251.4477329759075\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:286.03155517578125 \t\tpolicy loss:0.9689916689184648 \t\tavg entropy:1.1071518848546782 \t\tstd entropy:0.45945387018108125\n",
            "Epoch: 2 \t\tvalue loss:251.226793120179 \t\tpolicy loss:0.9217340493504005 \t\tavg entropy:1.0449585947668767 \t\tstd entropy:0.4726146170193685\n",
            "Epoch: 3 \t\tvalue loss:271.3337914189206 \t\tpolicy loss:1.138949633021898 \t\tavg entropy:1.00920422372725 \t\tstd entropy:0.45524470837076647\n",
            "Epoch: 4 \t\tvalue loss:302.32562584213065 \t\tpolicy loss:1.3015978570225872 \t\tavg entropy:1.0783504667616142 \t\tstd entropy:0.45637293699859455\n",
            "Epoch: 5 \t\tvalue loss:287.4735472473917 \t\tpolicy loss:0.952464348153223 \t\tavg entropy:1.1048385750806278 \t\tstd entropy:0.45087295397846866\n",
            "Epoch: 6 \t\tvalue loss:260.6003539652764 \t\tpolicy loss:0.9496272787263121 \t\tavg entropy:1.0817411394927965 \t\tstd entropy:0.45210086500749314\n",
            "Epoch: 7 \t\tvalue loss:243.3376632883579 \t\tpolicy loss:0.8927833181393298 \t\tavg entropy:1.01504579495086 \t\tstd entropy:0.4518938555164589\n",
            "Epoch: 8 \t\tvalue loss:278.5323629258554 \t\tpolicy loss:1.2441021837765658 \t\tavg entropy:1.0070552859021842 \t\tstd entropy:0.44603942122475754\n",
            "Epoch: 9 \t\tvalue loss:289.76079047480715 \t\tpolicy loss:1.025926476792444 \t\tavg entropy:1.0720141922670139 \t\tstd entropy:0.45143896832851244\n",
            "Epoch: 10 \t\tvalue loss:273.67794935009147 \t\tpolicy loss:0.8983878463129454 \t\tavg entropy:1.060523000842489 \t\tstd entropy:0.4472118537116547\n",
            "Episode 91 finished after 200 timesteps - cumulative reward = -496.31265724216973\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:241.4745014045812 \t\tpolicy loss:0.9063958797273757 \t\tavg entropy:1.0222631834421725 \t\tstd entropy:0.4514591680447145\n",
            "Epoch: 2 \t\tvalue loss:249.54382073124754 \t\tpolicy loss:0.9995717334596417 \t\tavg entropy:0.9682649547671804 \t\tstd entropy:0.4444717013879144\n",
            "Epoch: 3 \t\tvalue loss:289.9882939978491 \t\tpolicy loss:1.25256110972996 \t\tavg entropy:1.0132115155420438 \t\tstd entropy:0.45782628060757313\n",
            "Epoch: 4 \t\tvalue loss:282.1257722106161 \t\tpolicy loss:0.9319708584984646 \t\tavg entropy:1.0568409976307112 \t\tstd entropy:0.46044241153327076\n",
            "Epoch: 5 \t\tvalue loss:260.1057155947142 \t\tpolicy loss:0.9260063850426976 \t\tavg entropy:1.0509722078642902 \t\tstd entropy:0.45712196681892203\n",
            "Epoch: 6 \t\tvalue loss:240.4103963344912 \t\tpolicy loss:0.8864660066894338 \t\tavg entropy:0.9890610698463244 \t\tstd entropy:0.4558527485450452\n",
            "Epoch: 7 \t\tvalue loss:274.7870809820634 \t\tpolicy loss:1.1838093958323515 \t\tavg entropy:0.9747023589257626 \t\tstd entropy:0.45064370110620466\n",
            "Epoch: 8 \t\tvalue loss:286.902958737144 \t\tpolicy loss:1.0231360000900076 \t\tavg entropy:1.0399595872684362 \t\tstd entropy:0.46054312303553496\n",
            "Epoch: 9 \t\tvalue loss:269.5033020792128 \t\tpolicy loss:0.877949976468388 \t\tavg entropy:1.0373059411675727 \t\tstd entropy:0.4545895797495943\n",
            "Epoch: 10 \t\tvalue loss:238.15841848638993 \t\tpolicy loss:0.9289618994616256 \t\tavg entropy:0.9950900190670461 \t\tstd entropy:0.4625461901933829\n",
            "Episode 92 finished after 200 timesteps - cumulative reward = -252.445664256469\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:251.98364141922963 \t\tpolicy loss:1.0465555304213414 \t\tavg entropy:0.9545733196918235 \t\tstd entropy:0.45106317844007143\n",
            "Epoch: 2 \t\tvalue loss:290.8521902349931 \t\tpolicy loss:1.258520262150825 \t\tavg entropy:1.020240135122536 \t\tstd entropy:0.4624883519424399\n",
            "Epoch: 3 \t\tvalue loss:277.7697927740556 \t\tpolicy loss:0.931025321347804 \t\tavg entropy:1.055271936934394 \t\tstd entropy:0.4588858332089151\n",
            "Epoch: 4 \t\tvalue loss:252.11076721964002 \t\tpolicy loss:0.9637294313575648 \t\tavg entropy:1.0477041518718535 \t\tstd entropy:0.45897094675815797\n",
            "Epoch: 5 \t\tvalue loss:239.2938873677314 \t\tpolicy loss:0.9466940255104741 \t\tavg entropy:0.9900741233207456 \t\tstd entropy:0.45700324473674897\n",
            "Epoch: 6 \t\tvalue loss:281.4194079049026 \t\tpolicy loss:1.2756947056402135 \t\tavg entropy:1.005345229704369 \t\tstd entropy:0.4570938700006983\n",
            "Epoch: 7 \t\tvalue loss:284.50587791732596 \t\tpolicy loss:0.9898926155476631 \t\tavg entropy:1.0672243648383568 \t\tstd entropy:0.4675136772751679\n",
            "Epoch: 8 \t\tvalue loss:266.7989469117756 \t\tpolicy loss:0.9305236045318314 \t\tavg entropy:1.0570982046255666 \t\tstd entropy:0.46126798076582104\n",
            "Epoch: 9 \t\tvalue loss:235.4842556337767 \t\tpolicy loss:0.9179203502739532 \t\tavg entropy:1.0030779388246456 \t\tstd entropy:0.4635539313696573\n",
            "Epoch: 10 \t\tvalue loss:257.9720872323724 \t\tpolicy loss:1.1143446330782734 \t\tavg entropy:0.9663981148576407 \t\tstd entropy:0.4502041451856275\n",
            "Episode 93 finished after 200 timesteps - cumulative reward = -255.89806803030345\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:293.28521573996244 \t\tpolicy loss:1.2283891182911546 \t\tavg entropy:1.0445077288879772 \t\tstd entropy:0.4642467115671027\n",
            "Epoch: 2 \t\tvalue loss:276.6233022182803 \t\tpolicy loss:0.9263514253912093 \t\tavg entropy:1.061200743648909 \t\tstd entropy:0.46273774931052425\n",
            "Epoch: 3 \t\tvalue loss:246.66531912888152 \t\tpolicy loss:0.9606489622140233 \t\tavg entropy:1.0498834429609443 \t\tstd entropy:0.46079687498853805\n",
            "Epoch: 4 \t\tvalue loss:237.04776358302635 \t\tpolicy loss:0.9698491266256646 \t\tavg entropy:0.9934996069213233 \t\tstd entropy:0.4546029186604775\n",
            "Epoch: 5 \t\tvalue loss:281.3357555534266 \t\tpolicy loss:1.3023043916195254 \t\tavg entropy:1.0163618817865887 \t\tstd entropy:0.45812763119424277\n",
            "Epoch: 6 \t\tvalue loss:283.9127942821648 \t\tpolicy loss:0.9707966454421417 \t\tavg entropy:1.075224239758977 \t\tstd entropy:0.4712359145980517\n",
            "Epoch: 7 \t\tvalue loss:264.78730851185475 \t\tpolicy loss:0.9427863102924975 \t\tavg entropy:1.061357212159877 \t\tstd entropy:0.467713363685925\n",
            "Epoch: 8 \t\tvalue loss:233.14301609087593 \t\tpolicy loss:0.8990215772314917 \t\tavg entropy:1.0185122604623702 \t\tstd entropy:0.46453881829995025\n",
            "Epoch: 9 \t\tvalue loss:251.12002698681022 \t\tpolicy loss:1.0680083971989305 \t\tavg entropy:0.9684680848474128 \t\tstd entropy:0.4528943093769095\n",
            "Epoch: 10 \t\tvalue loss:288.98782831505883 \t\tpolicy loss:1.2531935655618016 \t\tavg entropy:1.0335792805253514 \t\tstd entropy:0.4673043961665146\n",
            "Episode 94 finished after 200 timesteps - cumulative reward = -361.045115313482\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:275.7998632117163 \t\tpolicy loss:0.9219792934158181 \t\tavg entropy:1.059856956412898 \t\tstd entropy:0.4707009935261716\n",
            "Epoch: 2 \t\tvalue loss:247.25079596797121 \t\tpolicy loss:0.9603065667273123 \t\tavg entropy:1.0534361561463226 \t\tstd entropy:0.4638991556273552\n",
            "Epoch: 3 \t\tvalue loss:230.38825447951692 \t\tpolicy loss:0.9230247377594815 \t\tavg entropy:1.000518366547891 \t\tstd entropy:0.4578246838223956\n",
            "Epoch: 4 \t\tvalue loss:268.545955030224 \t\tpolicy loss:1.2612064375153071 \t\tavg entropy:1.001054147090244 \t\tstd entropy:0.45311314503223815\n",
            "Epoch: 5 \t\tvalue loss:282.2234726917895 \t\tpolicy loss:0.9988119413581076 \t\tavg entropy:1.0581538411846643 \t\tstd entropy:0.46620198549539016\n",
            "Epoch: 6 \t\tvalue loss:264.34615084491196 \t\tpolicy loss:0.902453150930284 \t\tavg entropy:1.0412768917609996 \t\tstd entropy:0.46595206638999015\n",
            "Epoch: 7 \t\tvalue loss:231.59580201740508 \t\tpolicy loss:0.9115368942671185 \t\tavg entropy:1.018382516988565 \t\tstd entropy:0.4590731334304869\n",
            "Epoch: 8 \t\tvalue loss:236.87999792944026 \t\tpolicy loss:1.0341028842744948 \t\tavg entropy:0.9702147443065349 \t\tstd entropy:0.44977246800399995\n",
            "Epoch: 9 \t\tvalue loss:278.3632843403877 \t\tpolicy loss:1.2955458503735215 \t\tavg entropy:1.0195181853772153 \t\tstd entropy:0.46471794449867965\n",
            "Epoch: 10 \t\tvalue loss:273.66940578026106 \t\tpolicy loss:0.9233938048157511 \t\tavg entropy:1.0608880510552774 \t\tstd entropy:0.48043807663210086\n",
            "Episode 95 finished after 200 timesteps - cumulative reward = -449.5516409308552\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:250.44093437194823 \t\tpolicy loss:0.931828124821186 \t\tavg entropy:1.0507488353104528 \t\tstd entropy:0.4709078715504888\n",
            "Epoch: 2 \t\tvalue loss:222.26663465499877 \t\tpolicy loss:0.8439515717327595 \t\tavg entropy:0.9938882185381395 \t\tstd entropy:0.4613607891799964\n",
            "Epoch: 3 \t\tvalue loss:245.70287055969237 \t\tpolicy loss:1.1339835323393346 \t\tavg entropy:0.9519555993062078 \t\tstd entropy:0.4575789420059088\n",
            "Epoch: 4 \t\tvalue loss:266.79663276672363 \t\tpolicy loss:1.0294202744960785 \t\tavg entropy:1.0112762834678282 \t\tstd entropy:0.4774367592978613\n",
            "Epoch: 5 \t\tvalue loss:255.7401165008545 \t\tpolicy loss:0.8838978923857213 \t\tavg entropy:1.0052738768625251 \t\tstd entropy:0.48828301398314955\n",
            "Epoch: 6 \t\tvalue loss:224.598858833313 \t\tpolicy loss:0.9091821238398552 \t\tavg entropy:1.0005065696856779 \t\tstd entropy:0.4712571326029337\n",
            "Epoch: 7 \t\tvalue loss:217.6499270439148 \t\tpolicy loss:0.8672386452555656 \t\tavg entropy:0.9399524206056664 \t\tstd entropy:0.45970117435793906\n",
            "Epoch: 8 \t\tvalue loss:252.28075542449952 \t\tpolicy loss:1.1538257777690888 \t\tavg entropy:0.9363382823593124 \t\tstd entropy:0.48020664807338487\n",
            "Epoch: 9 \t\tvalue loss:257.0536178588867 \t\tpolicy loss:0.8825845062732697 \t\tavg entropy:0.984089336117184 \t\tstd entropy:0.5037738711211378\n",
            "Epoch: 10 \t\tvalue loss:239.42445201873778 \t\tpolicy loss:0.8857339970767498 \t\tavg entropy:0.9864369721642009 \t\tstd entropy:0.4940815741619884\n",
            "Episode 96 finished after 200 timesteps - cumulative reward = -123.19161437731545\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:210.3706274986267 \t\tpolicy loss:0.8129192657768727 \t\tavg entropy:0.9489088348543239 \t\tstd entropy:0.4730059180270663\n",
            "Epoch: 2 \t\tvalue loss:223.55828828811644 \t\tpolicy loss:0.9824986912310123 \t\tavg entropy:0.8903927207702034 \t\tstd entropy:0.4678133484687453\n",
            "Epoch: 3 \t\tvalue loss:250.94032249450683 \t\tpolicy loss:1.047187240421772 \t\tavg entropy:0.943899700159831 \t\tstd entropy:0.4908844740130058\n",
            "Epoch: 4 \t\tvalue loss:243.46007270812987 \t\tpolicy loss:0.8476343680173158 \t\tavg entropy:0.9660987665718164 \t\tstd entropy:0.4959356001730864\n",
            "Epoch: 5 \t\tvalue loss:216.6116174697876 \t\tpolicy loss:0.8628866344690322 \t\tavg entropy:0.9607887728578441 \t\tstd entropy:0.4844680735601682\n",
            "Epoch: 6 \t\tvalue loss:205.35723667144777 \t\tpolicy loss:0.7764373291283846 \t\tavg entropy:0.8904036118347732 \t\tstd entropy:0.4733701706183804\n",
            "Epoch: 7 \t\tvalue loss:235.17401809692382 \t\tpolicy loss:1.100741545855999 \t\tavg entropy:0.8769165889080379 \t\tstd entropy:0.489870507299873\n",
            "Epoch: 8 \t\tvalue loss:243.53065299987793 \t\tpolicy loss:0.8536612547934055 \t\tavg entropy:0.9389094460659405 \t\tstd entropy:0.5040561277908862\n",
            "Epoch: 9 \t\tvalue loss:228.2953987121582 \t\tpolicy loss:0.8339385591447354 \t\tavg entropy:0.9425454216153462 \t\tstd entropy:0.49614557511756247\n",
            "Epoch: 10 \t\tvalue loss:201.25827083587646 \t\tpolicy loss:0.7638522915542125 \t\tavg entropy:0.8913776684853645 \t\tstd entropy:0.48271983468679613\n",
            "Episode 97 finished after 200 timesteps - cumulative reward = -253.0410783399328\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:215.683172416687 \t\tpolicy loss:0.969296807423234 \t\tavg entropy:0.8405921073092251 \t\tstd entropy:0.4804239135049168\n",
            "Epoch: 2 \t\tvalue loss:241.25655422210693 \t\tpolicy loss:1.0197686217725277 \t\tavg entropy:0.9126296548201379 \t\tstd entropy:0.4949051449222247\n",
            "Epoch: 3 \t\tvalue loss:233.8204303741455 \t\tpolicy loss:0.8405868764966726 \t\tavg entropy:0.93640908859731 \t\tstd entropy:0.49125366173377283\n",
            "Epoch: 4 \t\tvalue loss:207.4029224395752 \t\tpolicy loss:0.8553166829049588 \t\tavg entropy:0.9296458748926001 \t\tstd entropy:0.4816486543393066\n",
            "Epoch: 5 \t\tvalue loss:204.78733730316162 \t\tpolicy loss:0.8490095928311348 \t\tavg entropy:0.8683767239504667 \t\tstd entropy:0.4729556198924345\n",
            "Epoch: 6 \t\tvalue loss:237.91229286193848 \t\tpolicy loss:1.124858418852091 \t\tavg entropy:0.8918390819184651 \t\tstd entropy:0.4917807317127063\n",
            "Epoch: 7 \t\tvalue loss:236.49365692138673 \t\tpolicy loss:0.8636046331375837 \t\tavg entropy:0.9438497719869728 \t\tstd entropy:0.5008802383552421\n",
            "Epoch: 8 \t\tvalue loss:216.14733028411865 \t\tpolicy loss:0.8665533296763896 \t\tavg entropy:0.9503236926167773 \t\tstd entropy:0.4905731596699222\n",
            "Epoch: 9 \t\tvalue loss:198.9812886238098 \t\tpolicy loss:0.7708726674318314 \t\tavg entropy:0.8831782469342009 \t\tstd entropy:0.4806074754597064\n",
            "Epoch: 10 \t\tvalue loss:227.96649475097655 \t\tpolicy loss:1.1262233957648278 \t\tavg entropy:0.8706788447437968 \t\tstd entropy:0.4903111037475539\n",
            "Episode 98 finished after 200 timesteps - cumulative reward = -123.92185271492049\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:238.78805408477783 \t\tpolicy loss:0.8704059697687626 \t\tavg entropy:0.9388533164542122 \t\tstd entropy:0.5052571344537844\n",
            "Epoch: 2 \t\tvalue loss:223.96025409698487 \t\tpolicy loss:0.8673254944384098 \t\tavg entropy:0.9422986288274869 \t\tstd entropy:0.49901345641420003\n",
            "Epoch: 3 \t\tvalue loss:197.6051833152771 \t\tpolicy loss:0.7685299430042505 \t\tavg entropy:0.8994850204519479 \t\tstd entropy:0.4854417744271342\n",
            "Epoch: 4 \t\tvalue loss:215.05532922744752 \t\tpolicy loss:1.0410740602761508 \t\tavg entropy:0.8503212568408957 \t\tstd entropy:0.48469705292409354\n",
            "Epoch: 5 \t\tvalue loss:240.10554943084716 \t\tpolicy loss:0.9976544737815857 \t\tavg entropy:0.9284986974208856 \t\tstd entropy:0.5028750876156243\n",
            "Epoch: 6 \t\tvalue loss:230.6842191696167 \t\tpolicy loss:0.870883560180664 \t\tavg entropy:0.9380010080647498 \t\tstd entropy:0.5072456374086647\n",
            "Epoch: 7 \t\tvalue loss:201.701314163208 \t\tpolicy loss:0.8391787379980087 \t\tavg entropy:0.9371763308608162 \t\tstd entropy:0.492295689390762\n",
            "Epoch: 8 \t\tvalue loss:203.13776025772094 \t\tpolicy loss:0.907120394334197 \t\tavg entropy:0.871699921787392 \t\tstd entropy:0.48959153930947896\n",
            "Epoch: 9 \t\tvalue loss:238.44088344573976 \t\tpolicy loss:1.1672660082578659 \t\tavg entropy:0.9137490255070061 \t\tstd entropy:0.5102517328685965\n",
            "Epoch: 10 \t\tvalue loss:235.48330669403077 \t\tpolicy loss:0.8757756896317005 \t\tavg entropy:0.9608072772471323 \t\tstd entropy:0.51873255795845\n",
            "Episode 99 finished after 200 timesteps - cumulative reward = -252.74431424334566\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:212.1811803817749 \t\tpolicy loss:0.9051681637763977 \t\tavg entropy:0.9735246979961172 \t\tstd entropy:0.5039487196051589\n",
            "Epoch: 2 \t\tvalue loss:196.82312602996825 \t\tpolicy loss:0.8043761927634477 \t\tavg entropy:0.9123793406690937 \t\tstd entropy:0.49158757950380255\n",
            "Epoch: 3 \t\tvalue loss:227.6605954170227 \t\tpolicy loss:1.1658055312931537 \t\tavg entropy:0.9041590774947778 \t\tstd entropy:0.5001587018203049\n",
            "Epoch: 4 \t\tvalue loss:238.7470817565918 \t\tpolicy loss:0.8804874792695045 \t\tavg entropy:0.9656379892731085 \t\tstd entropy:0.5122758880045177\n",
            "Epoch: 5 \t\tvalue loss:223.64033555984497 \t\tpolicy loss:0.8881224989891052 \t\tavg entropy:0.9595646746090614 \t\tstd entropy:0.510233177735624\n",
            "Epoch: 6 \t\tvalue loss:195.89556102752687 \t\tpolicy loss:0.7761651735752821 \t\tavg entropy:0.9237225928728003 \t\tstd entropy:0.4960893214332579\n",
            "Epoch: 7 \t\tvalue loss:208.92684841156006 \t\tpolicy loss:1.0059776794165374 \t\tavg entropy:0.8634228950486053 \t\tstd entropy:0.49663704598324876\n",
            "Epoch: 8 \t\tvalue loss:236.95122556686403 \t\tpolicy loss:1.0396813176572324 \t\tavg entropy:0.9286431670381687 \t\tstd entropy:0.514407034953519\n",
            "Epoch: 9 \t\tvalue loss:230.49706382751464 \t\tpolicy loss:0.8680860213935375 \t\tavg entropy:0.943181183900917 \t\tstd entropy:0.5223201852109837\n",
            "Epoch: 10 \t\tvalue loss:203.11666774749756 \t\tpolicy loss:0.8637124441564084 \t\tavg entropy:0.9555510568264872 \t\tstd entropy:0.5043143494967065\n",
            "Episode 100 finished after 200 timesteps - cumulative reward = -3.7620518525920668\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:194.99845443537205 \t\tpolicy loss:0.8370380471518011 \t\tavg entropy:0.8883203509619502 \t\tstd entropy:0.5004877119337162\n",
            "Epoch: 2 \t\tvalue loss:218.6715658682364 \t\tpolicy loss:1.2220173860773629 \t\tavg entropy:0.9111738476167407 \t\tstd entropy:0.5044351495752787\n",
            "Epoch: 3 \t\tvalue loss:221.36267052167727 \t\tpolicy loss:0.8848000632392036 \t\tavg entropy:0.9803851885067536 \t\tstd entropy:0.5025271414984052\n",
            "Epoch: 4 \t\tvalue loss:205.83622214234904 \t\tpolicy loss:0.8975033443651081 \t\tavg entropy:0.9697384509628411 \t\tstd entropy:0.5052405192468288\n",
            "Epoch: 5 \t\tvalue loss:182.16188181182486 \t\tpolicy loss:0.7485106293066048 \t\tavg entropy:0.9070387211075271 \t\tstd entropy:0.5011815245385537\n",
            "Epoch: 6 \t\tvalue loss:193.01346437430675 \t\tpolicy loss:0.9512606550146032 \t\tavg entropy:0.8342137870712768 \t\tstd entropy:0.5097245423993919\n",
            "Epoch: 7 \t\tvalue loss:212.10203684112173 \t\tpolicy loss:0.9263625969121485 \t\tavg entropy:0.8852434614816788 \t\tstd entropy:0.524441019524373\n",
            "Epoch: 8 \t\tvalue loss:208.6449940999349 \t\tpolicy loss:0.8243780283280361 \t\tavg entropy:0.9014254467069935 \t\tstd entropy:0.5234274152734233\n",
            "Epoch: 9 \t\tvalue loss:181.43440717532312 \t\tpolicy loss:0.8138808740509881 \t\tavg entropy:0.902329664065333 \t\tstd entropy:0.5092752174317752\n",
            "Epoch: 10 \t\tvalue loss:177.19460023479698 \t\tpolicy loss:0.7764261272954353 \t\tavg entropy:0.8297999094604536 \t\tstd entropy:0.5133417496831105\n",
            "Episode 101 finished after 200 timesteps - cumulative reward = -233.26468454085762\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:202.3812604362582 \t\tpolicy loss:0.9656169186403722 \t\tavg entropy:0.8319245266094711 \t\tstd entropy:0.5305625835931052\n",
            "Epoch: 2 \t\tvalue loss:209.629391140408 \t\tpolicy loss:0.8209402112313259 \t\tavg entropy:0.8755127961080132 \t\tstd entropy:0.5309897729946274\n",
            "Epoch: 3 \t\tvalue loss:188.6964168784059 \t\tpolicy loss:0.8277057291548929 \t\tavg entropy:0.8952248735146235 \t\tstd entropy:0.5177411499540322\n",
            "Epoch: 4 \t\tvalue loss:173.86200619921271 \t\tpolicy loss:0.7025633185733984 \t\tavg entropy:0.8247146656550699 \t\tstd entropy:0.5151477187409156\n",
            "Epoch: 5 \t\tvalue loss:197.40940762743537 \t\tpolicy loss:1.0057330352288705 \t\tavg entropy:0.8069540741811483 \t\tstd entropy:0.5332348621067338\n",
            "Epoch: 6 \t\tvalue loss:208.68631857412834 \t\tpolicy loss:0.8367459008723129 \t\tavg entropy:0.8750010033988797 \t\tstd entropy:0.5348030814127799\n",
            "Epoch: 7 \t\tvalue loss:192.11771025481048 \t\tpolicy loss:0.827935673572399 \t\tavg entropy:0.8960739567618474 \t\tstd entropy:0.5203131223098936\n",
            "Epoch: 8 \t\tvalue loss:172.29073032332056 \t\tpolicy loss:0.6985039585902367 \t\tavg entropy:0.8310752655469594 \t\tstd entropy:0.5123852308742325\n",
            "Epoch: 9 \t\tvalue loss:193.43632036373938 \t\tpolicy loss:0.9983126903757636 \t\tavg entropy:0.7968035905160686 \t\tstd entropy:0.5298629441664529\n",
            "Epoch: 10 \t\tvalue loss:207.58784239969137 \t\tpolicy loss:0.8494258677517926 \t\tavg entropy:0.8734185066981593 \t\tstd entropy:0.5362436480709319\n",
            "Episode 102 finished after 200 timesteps - cumulative reward = -249.04877959688264\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:194.2120934003665 \t\tpolicy loss:0.8446703781316309 \t\tavg entropy:0.8938531043922191 \t\tstd entropy:0.523118382410969\n",
            "Epoch: 2 \t\tvalue loss:171.3093206146617 \t\tpolicy loss:0.7114326887660556 \t\tavg entropy:0.8440002881015464 \t\tstd entropy:0.5110990380935817\n",
            "Epoch: 3 \t\tvalue loss:190.6993021081995 \t\tpolicy loss:0.990569606239413 \t\tavg entropy:0.798725968815696 \t\tstd entropy:0.5255351260889278\n",
            "Epoch: 4 \t\tvalue loss:207.62660226704162 \t\tpolicy loss:0.8681421029714891 \t\tavg entropy:0.87927687895995 \t\tstd entropy:0.5358774108711188\n",
            "Epoch: 5 \t\tvalue loss:196.76508086404684 \t\tpolicy loss:0.8665178690427615 \t\tavg entropy:0.8990717953544748 \t\tstd entropy:0.5238028045551779\n",
            "Epoch: 6 \t\tvalue loss:171.3386839878412 \t\tpolicy loss:0.7207936015393999 \t\tavg entropy:0.8524228752896689 \t\tstd entropy:0.5113296914450275\n",
            "Epoch: 7 \t\tvalue loss:187.4940822271653 \t\tpolicy loss:0.9760520495014426 \t\tavg entropy:0.7908826440121637 \t\tstd entropy:0.5267367055902283\n",
            "Epoch: 8 \t\tvalue loss:207.61716800265842 \t\tpolicy loss:0.8981357834957264 \t\tavg entropy:0.8762677800975204 \t\tstd entropy:0.5419396245327078\n",
            "Epoch: 9 \t\tvalue loss:198.88159142011477 \t\tpolicy loss:0.8729858383720304 \t\tavg entropy:0.8987089565234079 \t\tstd entropy:0.5298593820799646\n",
            "Epoch: 10 \t\tvalue loss:171.45317397882908 \t\tpolicy loss:0.7466118063455747 \t\tavg entropy:0.8680542949793939 \t\tstd entropy:0.5125230528239152\n",
            "Episode 103 finished after 200 timesteps - cumulative reward = -238.67152215414183\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:182.8966943246347 \t\tpolicy loss:0.9337783934157572 \t\tavg entropy:0.7942115431155152 \t\tstd entropy:0.5278302145333645\n",
            "Epoch: 2 \t\tvalue loss:207.95370257342304 \t\tpolicy loss:0.9707152740454968 \t\tavg entropy:0.8758875497278877 \t\tstd entropy:0.5434158546085494\n",
            "Epoch: 3 \t\tvalue loss:202.70389104772497 \t\tpolicy loss:0.8614990254979075 \t\tavg entropy:0.9059030422615348 \t\tstd entropy:0.5368766648638731\n",
            "Epoch: 4 \t\tvalue loss:175.00844809449748 \t\tpolicy loss:0.7936060237295833 \t\tavg entropy:0.8937026916011837 \t\tstd entropy:0.5161547574436909\n",
            "Epoch: 5 \t\tvalue loss:178.90140363905164 \t\tpolicy loss:0.8685265720626454 \t\tavg entropy:0.809865331300419 \t\tstd entropy:0.5293762753165993\n",
            "Epoch: 6 \t\tvalue loss:205.477712466393 \t\tpolicy loss:1.0338019942059928 \t\tavg entropy:0.870643248811899 \t\tstd entropy:0.5459921418487091\n",
            "Epoch: 7 \t\tvalue loss:205.46076701599875 \t\tpolicy loss:0.8506979898170188 \t\tavg entropy:0.9109400224806004 \t\tstd entropy:0.5469447749303757\n",
            "Epoch: 8 \t\tvalue loss:180.61579386393228 \t\tpolicy loss:0.8389980255821605 \t\tavg entropy:0.9167846226476658 \t\tstd entropy:0.5263643990958443\n",
            "Epoch: 9 \t\tvalue loss:174.51652338475355 \t\tpolicy loss:0.8226022602599344 \t\tavg entropy:0.8363575360987054 \t\tstd entropy:0.5326060816536837\n",
            "Epoch: 10 \t\tvalue loss:201.1899762565707 \t\tpolicy loss:1.0456499692834453 \t\tavg entropy:0.8635521354256336 \t\tstd entropy:0.550060803619229\n",
            "Episode 104 finished after 200 timesteps - cumulative reward = -363.9037954999707\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:206.25064011562017 \t\tpolicy loss:0.8351459356001866 \t\tavg entropy:0.9147028164750218 \t\tstd entropy:0.5536771096284415\n",
            "Epoch: 2 \t\tvalue loss:185.488346900469 \t\tpolicy loss:0.86066206296285 \t\tavg entropy:0.9256413642908964 \t\tstd entropy:0.5339153067940471\n",
            "Epoch: 3 \t\tvalue loss:171.38840456362124 \t\tpolicy loss:0.7574041511541532 \t\tavg entropy:0.852921705002839 \t\tstd entropy:0.5318869718609329\n",
            "Epoch: 4 \t\tvalue loss:195.73665995656708 \t\tpolicy loss:1.0198818349543912 \t\tavg entropy:0.8426635983628623 \t\tstd entropy:0.5538111723968906\n",
            "Epoch: 5 \t\tvalue loss:204.70119655279467 \t\tpolicy loss:0.8322402952629843 \t\tavg entropy:0.9003215007849129 \t\tstd entropy:0.5583711112995736\n",
            "Epoch: 6 \t\tvalue loss:187.9322548383548 \t\tpolicy loss:0.853258748849233 \t\tavg entropy:0.9121410539229803 \t\tstd entropy:0.5396903445466817\n",
            "Epoch: 7 \t\tvalue loss:168.22800473813658 \t\tpolicy loss:0.7027740327664364 \t\tavg entropy:0.8460020306733333 \t\tstd entropy:0.5332622686241328\n",
            "Epoch: 8 \t\tvalue loss:188.57157031400703 \t\tpolicy loss:0.9918766190976273 \t\tavg entropy:0.8122019829097791 \t\tstd entropy:0.5517311206514328\n",
            "Epoch: 9 \t\tvalue loss:203.04304947087795 \t\tpolicy loss:0.838313587653784 \t\tavg entropy:0.8808740049147242 \t\tstd entropy:0.5597275857149586\n",
            "Epoch: 10 \t\tvalue loss:190.23642994445046 \t\tpolicy loss:0.8313844785278226 \t\tavg entropy:0.8930314948212493 \t\tstd entropy:0.5439078624203758\n",
            "Episode 105 finished after 200 timesteps - cumulative reward = -365.65616222756427\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:165.14268354090248 \t\tpolicy loss:0.7000666882206754 \t\tavg entropy:0.8436234828711171 \t\tstd entropy:0.532446033945896\n",
            "Epoch: 2 \t\tvalue loss:178.01701150289395 \t\tpolicy loss:1.0452143608070001 \t\tavg entropy:0.8085686200902926 \t\tstd entropy:0.5313038853469302\n",
            "Epoch: 3 \t\tvalue loss:194.6937345179116 \t\tpolicy loss:0.9414251315884474 \t\tavg entropy:0.8963796535488034 \t\tstd entropy:0.530324352246434\n",
            "Epoch: 4 \t\tvalue loss:188.36039212854897 \t\tpolicy loss:0.8322026300721053 \t\tavg entropy:0.9084475698643261 \t\tstd entropy:0.5156161937433598\n",
            "Epoch: 5 \t\tvalue loss:164.50385842672208 \t\tpolicy loss:0.7780528562824901 \t\tavg entropy:0.8786179520398736 \t\tstd entropy:0.5036029220416055\n",
            "Epoch: 6 \t\tvalue loss:169.92768738909464 \t\tpolicy loss:0.8922652485893994 \t\tavg entropy:0.8011071739858131 \t\tstd entropy:0.5200114309152598\n",
            "Epoch: 7 \t\tvalue loss:188.1583529216487 \t\tpolicy loss:0.9537593489739953 \t\tavg entropy:0.853492690147247 \t\tstd entropy:0.5413935528379056\n",
            "Epoch: 8 \t\tvalue loss:188.17698436830102 \t\tpolicy loss:0.8010148355146733 \t\tavg entropy:0.8862055049377856 \t\tstd entropy:0.5349638097277701\n",
            "Epoch: 9 \t\tvalue loss:163.11388974073458 \t\tpolicy loss:0.7965577433748943 \t\tavg entropy:0.8787826544165226 \t\tstd entropy:0.5180243438711446\n",
            "Epoch: 10 \t\tvalue loss:163.49373384801353 \t\tpolicy loss:0.8397339793240152 \t\tavg entropy:0.7970999936386484 \t\tstd entropy:0.5274085688217306\n",
            "Episode 106 finished after 200 timesteps - cumulative reward = -121.10793265296401\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:185.07557194407394 \t\tpolicy loss:0.9767820980490708 \t\tavg entropy:0.8364380892301184 \t\tstd entropy:0.5514239336248264\n",
            "Epoch: 2 \t\tvalue loss:188.30399862149866 \t\tpolicy loss:0.8154646296326707 \t\tavg entropy:0.8850301340660103 \t\tstd entropy:0.5487477803623831\n",
            "Epoch: 3 \t\tvalue loss:163.77898928014244 \t\tpolicy loss:0.8256677171079124 \t\tavg entropy:0.8865683745158291 \t\tstd entropy:0.5321955678400467\n",
            "Epoch: 4 \t\tvalue loss:159.06653343758933 \t\tpolicy loss:0.783884659772966 \t\tavg entropy:0.8028653673865233 \t\tstd entropy:0.5378869233181155\n",
            "Epoch: 5 \t\tvalue loss:180.5321670160061 \t\tpolicy loss:0.9839043435527057 \t\tavg entropy:0.8285857480653179 \t\tstd entropy:0.5565557478028796\n",
            "Epoch: 6 \t\tvalue loss:187.2779995988055 \t\tpolicy loss:0.8084047851039142 \t\tavg entropy:0.8788832619851706 \t\tstd entropy:0.5561271646770264\n",
            "Epoch: 7 \t\tvalue loss:164.0026795922256 \t\tpolicy loss:0.834398116280393 \t\tavg entropy:0.8835975214927309 \t\tstd entropy:0.5376462301963942\n",
            "Epoch: 8 \t\tvalue loss:155.68131935305712 \t\tpolicy loss:0.7614359855651855 \t\tavg entropy:0.8039763189361978 \t\tstd entropy:0.5406091304643718\n",
            "Epoch: 9 \t\tvalue loss:176.67897889672255 \t\tpolicy loss:0.9616695177264329 \t\tavg entropy:0.8185930318243033 \t\tstd entropy:0.5599383447582257\n",
            "Epoch: 10 \t\tvalue loss:185.65478078330435 \t\tpolicy loss:0.8055988859112669 \t\tavg entropy:0.8702175279081906 \t\tstd entropy:0.5607963322746468\n",
            "Episode 107 finished after 200 timesteps - cumulative reward = -115.6130739695431\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:163.57236285325956 \t\tpolicy loss:0.8361581483992134 \t\tavg entropy:0.880526166724032 \t\tstd entropy:0.5418564043639357\n",
            "Epoch: 2 \t\tvalue loss:152.5678765831924 \t\tpolicy loss:0.753109493270153 \t\tavg entropy:0.8029229419871733 \t\tstd entropy:0.5429781148144007\n",
            "Epoch: 3 \t\tvalue loss:172.83542958701528 \t\tpolicy loss:0.9520580216151912 \t\tavg entropy:0.8101332495307582 \t\tstd entropy:0.5636717132523688\n",
            "Epoch: 4 \t\tvalue loss:183.72301743670207 \t\tpolicy loss:0.804164765811548 \t\tavg entropy:0.8630311379474291 \t\tstd entropy:0.5669310102600124\n",
            "Epoch: 5 \t\tvalue loss:162.1872419962069 \t\tpolicy loss:0.8362694702497343 \t\tavg entropy:0.8737905193621055 \t\tstd entropy:0.547444204805956\n",
            "Epoch: 6 \t\tvalue loss:149.6490339883944 \t\tpolicy loss:0.7330997153753187 \t\tavg entropy:0.8006160323567318 \t\tstd entropy:0.5456883918067191\n",
            "Epoch: 7 \t\tvalue loss:169.53871201305856 \t\tpolicy loss:0.9329874602759757 \t\tavg entropy:0.8018212179323703 \t\tstd entropy:0.5662902608397752\n",
            "Epoch: 8 \t\tvalue loss:182.14442388022817 \t\tpolicy loss:0.8017749110373055 \t\tavg entropy:0.8539207919781306 \t\tstd entropy:0.5704006898861836\n",
            "Epoch: 9 \t\tvalue loss:161.35832823776616 \t\tpolicy loss:0.8379968099477815 \t\tavg entropy:0.8678387577636807 \t\tstd entropy:0.5506907756821605\n",
            "Epoch: 10 \t\tvalue loss:147.54702926263576 \t\tpolicy loss:0.7242555531059823 \t\tavg entropy:0.7973538980166731 \t\tstd entropy:0.5468279159883247\n",
            "Episode 108 finished after 200 timesteps - cumulative reward = -6.557092249456165\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:167.07541284328553 \t\tpolicy loss:0.9240994380741585 \t\tavg entropy:0.7979658605764429 \t\tstd entropy:0.5679653796348714\n",
            "Epoch: 2 \t\tvalue loss:180.2054895540563 \t\tpolicy loss:0.7886695887257413 \t\tavg entropy:0.8464870755421054 \t\tstd entropy:0.576136920762861\n",
            "Epoch: 3 \t\tvalue loss:159.5459267220846 \t\tpolicy loss:0.8450823666118994 \t\tavg entropy:0.8620025323056969 \t\tstd entropy:0.5574800627676427\n",
            "Epoch: 4 \t\tvalue loss:144.76428538996998 \t\tpolicy loss:0.719051306930984 \t\tavg entropy:0.7955998117081686 \t\tstd entropy:0.552429069685177\n",
            "Epoch: 5 \t\tvalue loss:164.21220109520888 \t\tpolicy loss:0.9101041286456876 \t\tavg entropy:0.7911323272453366 \t\tstd entropy:0.5709885121690147\n",
            "Epoch: 6 \t\tvalue loss:178.8051594059642 \t\tpolicy loss:0.787027433878038 \t\tavg entropy:0.8434415612555641 \t\tstd entropy:0.5775442599329323\n",
            "Epoch: 7 \t\tvalue loss:158.54211909596512 \t\tpolicy loss:0.8371299308974568 \t\tavg entropy:0.8577917813071252 \t\tstd entropy:0.5572235305563638\n",
            "Epoch: 8 \t\tvalue loss:143.15848066748643 \t\tpolicy loss:0.7090260789888662 \t\tavg entropy:0.7917311575858623 \t\tstd entropy:0.5514724969742902\n",
            "Epoch: 9 \t\tvalue loss:162.1644916999631 \t\tpolicy loss:0.9025685627286028 \t\tavg entropy:0.7856266084687117 \t\tstd entropy:0.569321241026068\n",
            "Epoch: 10 \t\tvalue loss:176.8941199139851 \t\tpolicy loss:0.7842072750010142 \t\tavg entropy:0.8375443929933583 \t\tstd entropy:0.5767781908458827\n",
            "Episode 109 finished after 200 timesteps - cumulative reward = -124.60152178673827\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:156.85914649032964 \t\tpolicy loss:0.8358793702067399 \t\tavg entropy:0.8535506597820033 \t\tstd entropy:0.5585904594004464\n",
            "Epoch: 2 \t\tvalue loss:141.62710520116295 \t\tpolicy loss:0.702672921302842 \t\tavg entropy:0.7880956903501988 \t\tstd entropy:0.5528196163177735\n",
            "Epoch: 3 \t\tvalue loss:160.6428137058165 \t\tpolicy loss:0.8977706214276756 \t\tavg entropy:0.782028192934008 \t\tstd entropy:0.569732218881823\n",
            "Epoch: 4 \t\tvalue loss:175.62972492124976 \t\tpolicy loss:0.7827088491218847 \t\tavg entropy:0.8363003500245807 \t\tstd entropy:0.5785111240292076\n",
            "Epoch: 5 \t\tvalue loss:155.04924602043337 \t\tpolicy loss:0.8312645496391668 \t\tavg entropy:0.8493030148508913 \t\tstd entropy:0.5592128047764988\n",
            "Epoch: 6 \t\tvalue loss:140.3189550725425 \t\tpolicy loss:0.6968715230866176 \t\tavg entropy:0.7837494164290838 \t\tstd entropy:0.5524713472375105\n",
            "Epoch: 7 \t\tvalue loss:159.49433824492664 \t\tpolicy loss:0.8944513012723225 \t\tavg entropy:0.7809519106739147 \t\tstd entropy:0.5695983074117602\n",
            "Epoch: 8 \t\tvalue loss:173.63758077854064 \t\tpolicy loss:0.7757021497662474 \t\tavg entropy:0.8334818813761742 \t\tstd entropy:0.5771972867443843\n",
            "Epoch: 9 \t\tvalue loss:152.43845455820968 \t\tpolicy loss:0.820800698385006 \t\tavg entropy:0.8427122646745567 \t\tstd entropy:0.558828165434063\n",
            "Epoch: 10 \t\tvalue loss:139.05384640577364 \t\tpolicy loss:0.6933813000597605 \t\tavg entropy:0.7771928977926799 \t\tstd entropy:0.5526381941209269\n",
            "Episode 110 finished after 200 timesteps - cumulative reward = -366.84052039821404\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:157.25037728734765 \t\tpolicy loss:0.8973789315625845 \t\tavg entropy:0.7802336184778169 \t\tstd entropy:0.5702448129070512\n",
            "Epoch: 2 \t\tvalue loss:168.27925891186817 \t\tpolicy loss:0.7576831144740782 \t\tavg entropy:0.8308078769182473 \t\tstd entropy:0.5810626964604214\n",
            "Epoch: 3 \t\tvalue loss:145.44802291134755 \t\tpolicy loss:0.7782508791929268 \t\tavg entropy:0.8267478549885541 \t\tstd entropy:0.5669962788039907\n",
            "Epoch: 4 \t\tvalue loss:134.3031326202025 \t\tpolicy loss:0.6811699755938656 \t\tavg entropy:0.7550947761079543 \t\tstd entropy:0.5612243079176154\n",
            "Epoch: 5 \t\tvalue loss:152.12427552924098 \t\tpolicy loss:0.8104098607976753 \t\tavg entropy:0.7536631649820942 \t\tstd entropy:0.5756402105999422\n",
            "Epoch: 6 \t\tvalue loss:161.92332463092114 \t\tpolicy loss:0.731574529624847 \t\tavg entropy:0.8003316992542615 \t\tstd entropy:0.5824276606148616\n",
            "Epoch: 7 \t\tvalue loss:137.5429016480963 \t\tpolicy loss:0.7364962079438818 \t\tavg entropy:0.7941480870281036 \t\tstd entropy:0.5670410547309771\n",
            "Epoch: 8 \t\tvalue loss:133.0156714657703 \t\tpolicy loss:0.6898628150124148 \t\tavg entropy:0.7269840234657559 \t\tstd entropy:0.5570878470311201\n",
            "Epoch: 9 \t\tvalue loss:152.18490278864482 \t\tpolicy loss:0.7431215069380152 \t\tavg entropy:0.7389596044760102 \t\tstd entropy:0.5725088508313919\n",
            "Epoch: 10 \t\tvalue loss:154.25354302647602 \t\tpolicy loss:0.7314684764448419 \t\tavg entropy:0.7742249116630049 \t\tstd entropy:0.5697643144146076\n",
            "Episode 111 finished after 200 timesteps - cumulative reward = -354.2542652020504\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:129.71921387637954 \t\tpolicy loss:0.6591429538037403 \t\tavg entropy:0.7543767779458693 \t\tstd entropy:0.5558586860263602\n",
            "Epoch: 2 \t\tvalue loss:134.95512440692949 \t\tpolicy loss:0.7131733301892338 \t\tavg entropy:0.702678069213666 \t\tstd entropy:0.5469817533095498\n",
            "Epoch: 3 \t\tvalue loss:153.82469930993506 \t\tpolicy loss:0.7080112483846136 \t\tavg entropy:0.7379879904418234 \t\tstd entropy:0.5643612949458919\n",
            "Epoch: 4 \t\tvalue loss:142.23945585503637 \t\tpolicy loss:0.7467484445456999 \t\tavg entropy:0.763192563626974 \t\tstd entropy:0.5541302130979686\n",
            "Epoch: 5 \t\tvalue loss:126.3099365234375 \t\tpolicy loss:0.6195281919944717 \t\tavg entropy:0.7085598756707898 \t\tstd entropy:0.5410553312967735\n",
            "Epoch: 6 \t\tvalue loss:141.735688496785 \t\tpolicy loss:0.7553667250167893 \t\tavg entropy:0.7056869470260617 \t\tstd entropy:0.5520564758153466\n",
            "Epoch: 7 \t\tvalue loss:151.53660252582597 \t\tpolicy loss:0.7075530014124262 \t\tavg entropy:0.7465508530318109 \t\tstd entropy:0.5584764344544438\n",
            "Epoch: 8 \t\tvalue loss:128.38539459044674 \t\tpolicy loss:0.6821831328323088 \t\tavg entropy:0.7360831826231914 \t\tstd entropy:0.5443053440103456\n",
            "Epoch: 9 \t\tvalue loss:130.31419896504966 \t\tpolicy loss:0.6935455888868814 \t\tavg entropy:0.6826044278506771 \t\tstd entropy:0.5344665979776151\n",
            "Epoch: 10 \t\tvalue loss:149.5589100021914 \t\tpolicy loss:0.7130362262208778 \t\tavg entropy:0.7222989007478753 \t\tstd entropy:0.5554190359146342\n",
            "Episode 112 finished after 200 timesteps - cumulative reward = -247.31845149520444\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:138.07273405144014 \t\tpolicy loss:0.745989430381591 \t\tavg entropy:0.7509023257919117 \t\tstd entropy:0.5460392446279162\n",
            "Epoch: 2 \t\tvalue loss:123.91577369046499 \t\tpolicy loss:0.6259453856801412 \t\tavg entropy:0.6907479423278862 \t\tstd entropy:0.5337345281659817\n",
            "Epoch: 3 \t\tvalue loss:141.8714526073042 \t\tpolicy loss:0.7393845061221754 \t\tavg entropy:0.7046149501317978 \t\tstd entropy:0.5483266736896574\n",
            "Epoch: 4 \t\tvalue loss:146.89129652459937 \t\tpolicy loss:0.7336201104054968 \t\tavg entropy:0.7430519887956353 \t\tstd entropy:0.5464005515093882\n",
            "Epoch: 5 \t\tvalue loss:122.83627535348916 \t\tpolicy loss:0.6297833208578179 \t\tavg entropy:0.7043759732042114 \t\tstd entropy:0.5328441797437584\n",
            "Epoch: 6 \t\tvalue loss:132.5784247984369 \t\tpolicy loss:0.7247535884380341 \t\tavg entropy:0.67810210965695 \t\tstd entropy:0.5335007283601642\n",
            "Epoch: 7 \t\tvalue loss:148.6139322763466 \t\tpolicy loss:0.7032270542828434 \t\tavg entropy:0.7276092519903807 \t\tstd entropy:0.5469227546262267\n",
            "Epoch: 8 \t\tvalue loss:127.59973287007895 \t\tpolicy loss:0.7152965316571385 \t\tavg entropy:0.7323612002033465 \t\tstd entropy:0.5363993320851779\n",
            "Epoch: 9 \t\tvalue loss:125.75645837439112 \t\tpolicy loss:0.6763433727873377 \t\tavg entropy:0.6728785827287298 \t\tstd entropy:0.5248489631061303\n",
            "Epoch: 10 \t\tvalue loss:146.05327380996152 \t\tpolicy loss:0.7253407229142017 \t\tavg entropy:0.7158608769565316 \t\tstd entropy:0.5464108965115112\n",
            "Episode 113 finished after 200 timesteps - cumulative reward = -129.17134326880867\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:133.3834962959749 \t\tpolicy loss:0.7616113743868219 \t\tavg entropy:0.7507104963121755 \t\tstd entropy:0.5357462447757328\n",
            "Epoch: 2 \t\tvalue loss:121.04074092083667 \t\tpolicy loss:0.6463095114891788 \t\tavg entropy:0.6848285025308 \t\tstd entropy:0.5251716252409052\n",
            "Epoch: 3 \t\tvalue loss:141.01647503404732 \t\tpolicy loss:0.7485699520771762 \t\tavg entropy:0.7100702524544459 \t\tstd entropy:0.5428372762189136\n",
            "Epoch: 4 \t\tvalue loss:140.1915655021208 \t\tpolicy loss:0.7689175289797495 \t\tavg entropy:0.756746896589002 \t\tstd entropy:0.5342937385377671\n",
            "Epoch: 5 \t\tvalue loss:118.86794979601021 \t\tpolicy loss:0.6179819739008524 \t\tavg entropy:0.691738896955785 \t\tstd entropy:0.5251628390353029\n",
            "Epoch: 6 \t\tvalue loss:133.2413002841444 \t\tpolicy loss:0.7453001737594604 \t\tavg entropy:0.6892291310522182 \t\tstd entropy:0.5350966293391073\n",
            "Epoch: 7 \t\tvalue loss:143.32992254969585 \t\tpolicy loss:0.7269556274615139 \t\tavg entropy:0.7413666189649659 \t\tstd entropy:0.5382711570056011\n",
            "Epoch: 8 \t\tvalue loss:119.43920948993728 \t\tpolicy loss:0.6408859990447401 \t\tavg entropy:0.7016618576567882 \t\tstd entropy:0.5294710270063612\n",
            "Epoch: 9 \t\tvalue loss:126.452264211264 \t\tpolicy loss:0.6947278710732977 \t\tavg entropy:0.6713546874855769 \t\tstd entropy:0.5240308232818843\n",
            "Epoch: 10 \t\tvalue loss:142.11060792854033 \t\tpolicy loss:0.6787026801023138 \t\tavg entropy:0.7196391943747993 \t\tstd entropy:0.5359147073355287\n",
            "Episode 114 finished after 200 timesteps - cumulative reward = -124.65866873879622\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:121.82708381744753 \t\tpolicy loss:0.6876953495554177 \t\tavg entropy:0.7165958682400243 \t\tstd entropy:0.5275936740281081\n",
            "Epoch: 2 \t\tvalue loss:121.88989758778767 \t\tpolicy loss:0.6666435624461576 \t\tavg entropy:0.6646588060698387 \t\tstd entropy:0.5145107412978073\n",
            "Epoch: 3 \t\tvalue loss:139.8588499965438 \t\tpolicy loss:0.6884455267923424 \t\tavg entropy:0.7063889474311882 \t\tstd entropy:0.5337917762360966\n",
            "Epoch: 4 \t\tvalue loss:123.36661524944995 \t\tpolicy loss:0.7244684394583645 \t\tavg entropy:0.7269841162513385 \t\tstd entropy:0.5264075895192057\n",
            "Epoch: 5 \t\tvalue loss:118.83988925060594 \t\tpolicy loss:0.6668620500938002 \t\tavg entropy:0.6669634654360913 \t\tstd entropy:0.5148959630731179\n",
            "Epoch: 6 \t\tvalue loss:139.02091851291885 \t\tpolicy loss:0.7164577209805868 \t\tavg entropy:0.7045139473623219 \t\tstd entropy:0.5344938547755168\n",
            "Epoch: 7 \t\tvalue loss:125.98546632513943 \t\tpolicy loss:0.7656329737370273 \t\tavg entropy:0.7403469935243491 \t\tstd entropy:0.5270429597857574\n",
            "Epoch: 8 \t\tvalue loss:115.86437036904944 \t\tpolicy loss:0.6659190564988607 \t\tavg entropy:0.6737913324355936 \t\tstd entropy:0.5197160411319137\n",
            "Epoch: 9 \t\tvalue loss:137.71502956712104 \t\tpolicy loss:0.7561490489057747 \t\tavg entropy:0.7100545419302705 \t\tstd entropy:0.5386558491746837\n",
            "Epoch: 10 \t\tvalue loss:131.87347458069584 \t\tpolicy loss:0.7991143680480589 \t\tavg entropy:0.7563843174613325 \t\tstd entropy:0.5309912985706214\n",
            "Episode 115 finished after 200 timesteps - cumulative reward = -123.20434670256896\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:112.0014768782116 \t\tpolicy loss:0.6596440754476047 \t\tavg entropy:0.6905449123112017 \t\tstd entropy:0.5257280738877318\n",
            "Epoch: 2 \t\tvalue loss:129.95339915865944 \t\tpolicy loss:0.7908003401188624 \t\tavg entropy:0.7137798634061675 \t\tstd entropy:0.5401360564992476\n",
            "Epoch: 3 \t\tvalue loss:135.7446263631185 \t\tpolicy loss:0.7690673633700326 \t\tavg entropy:0.7576748383429172 \t\tstd entropy:0.5349592528719039\n",
            "Epoch: 4 \t\tvalue loss:109.19272763388497 \t\tpolicy loss:0.6711951660968009 \t\tavg entropy:0.7024587884247918 \t\tstd entropy:0.5323072939418932\n",
            "Epoch: 5 \t\tvalue loss:118.32318941752116 \t\tpolicy loss:0.70842288008758 \t\tavg entropy:0.6849123474402724 \t\tstd entropy:0.5316759146312663\n",
            "Epoch: 6 \t\tvalue loss:135.01682494935534 \t\tpolicy loss:0.6720291901202429 \t\tavg entropy:0.7265264819407933 \t\tstd entropy:0.5397377939474606\n",
            "Epoch: 7 \t\tvalue loss:112.26107740402222 \t\tpolicy loss:0.6699403645027251 \t\tavg entropy:0.7084426463489395 \t\tstd entropy:0.5321897142343076\n",
            "Epoch: 8 \t\tvalue loss:110.58611456553142 \t\tpolicy loss:0.6134698014883768 \t\tavg entropy:0.6518402079998981 \t\tstd entropy:0.5178858548340878\n",
            "Epoch: 9 \t\tvalue loss:128.72428389957972 \t\tpolicy loss:0.6367750455226217 \t\tavg entropy:0.682508091026116 \t\tstd entropy:0.5314804477417446\n",
            "Epoch: 10 \t\tvalue loss:114.0421306292216 \t\tpolicy loss:0.6756570761402448 \t\tavg entropy:0.6985620500098846 \t\tstd entropy:0.5258088325802318\n",
            "Episode 116 finished after 200 timesteps - cumulative reward = -124.62733407151399\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:106.00868161519368 \t\tpolicy loss:0.5882899566065698 \t\tavg entropy:0.6345570402097874 \t\tstd entropy:0.51035027663931\n",
            "Epoch: 2 \t\tvalue loss:123.58284178234283 \t\tpolicy loss:0.6353741486867269 \t\tavg entropy:0.659389122806562 \t\tstd entropy:0.5265361773962742\n",
            "Epoch: 3 \t\tvalue loss:114.49731928961617 \t\tpolicy loss:0.6622947741832051 \t\tavg entropy:0.6835907234118291 \t\tstd entropy:0.5215723658172314\n",
            "Epoch: 4 \t\tvalue loss:102.39172835577102 \t\tpolicy loss:0.5690150739891189 \t\tavg entropy:0.6153512098604333 \t\tstd entropy:0.5060130674416887\n",
            "Epoch: 5 \t\tvalue loss:119.73310674939837 \t\tpolicy loss:0.6274505271798089 \t\tavg entropy:0.6403879382907631 \t\tstd entropy:0.5233556919323333\n",
            "Epoch: 6 \t\tvalue loss:113.84465599060059 \t\tpolicy loss:0.6482047715357372 \t\tavg entropy:0.6684140265336692 \t\tstd entropy:0.5191871505557588\n",
            "Epoch: 7 \t\tvalue loss:100.11399811790103 \t\tpolicy loss:0.5530931389047986 \t\tavg entropy:0.5998065398880504 \t\tstd entropy:0.5014972551301201\n",
            "Epoch: 8 \t\tvalue loss:116.789933295477 \t\tpolicy loss:0.6011264278065591 \t\tavg entropy:0.6227065922552897 \t\tstd entropy:0.5195869149529126\n",
            "Epoch: 9 \t\tvalue loss:110.97713468188331 \t\tpolicy loss:0.6530868414612043 \t\tavg entropy:0.6510315092821195 \t\tstd entropy:0.518719562590664\n",
            "Epoch: 10 \t\tvalue loss:99.3110598609561 \t\tpolicy loss:0.5611251975808825 \t\tavg entropy:0.5928870242771769 \t\tstd entropy:0.4981309526619036\n",
            "Episode 117 finished after 200 timesteps - cumulative reward = -126.97258319103645\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:115.30175168173653 \t\tpolicy loss:0.5668962487862224 \t\tavg entropy:0.6193430709865118 \t\tstd entropy:0.5171449732322141\n",
            "Epoch: 2 \t\tvalue loss:106.32885642278762 \t\tpolicy loss:0.5795126589281219 \t\tavg entropy:0.6255739913432067 \t\tstd entropy:0.5124459880485003\n",
            "Epoch: 3 \t\tvalue loss:98.5304733230954 \t\tpolicy loss:0.5270456001162529 \t\tavg entropy:0.566465304129716 \t\tstd entropy:0.4896763935491954\n",
            "Epoch: 4 \t\tvalue loss:112.45149873551868 \t\tpolicy loss:0.5268496115292821 \t\tavg entropy:0.5938441603270753 \t\tstd entropy:0.5125791482174973\n",
            "Epoch: 5 \t\tvalue loss:100.43565089362008 \t\tpolicy loss:0.513814640896661 \t\tavg entropy:0.5789974228132415 \t\tstd entropy:0.502247152081623\n",
            "Epoch: 6 \t\tvalue loss:98.30183240345546 \t\tpolicy loss:0.5151275172829628 \t\tavg entropy:0.5380905317532612 \t\tstd entropy:0.4863962093440351\n",
            "Epoch: 7 \t\tvalue loss:108.869139171782 \t\tpolicy loss:0.5139853957863081 \t\tavg entropy:0.5709084773071307 \t\tstd entropy:0.5073677667513707\n",
            "Epoch: 8 \t\tvalue loss:96.58360980805897 \t\tpolicy loss:0.4698055432665916 \t\tavg entropy:0.5401823626241752 \t\tstd entropy:0.48882402922149776\n",
            "Epoch: 9 \t\tvalue loss:98.57106297356742 \t\tpolicy loss:0.5376580151773634 \t\tavg entropy:0.5174028313533703 \t\tstd entropy:0.48580050652047885\n",
            "Epoch: 10 \t\tvalue loss:107.71191192808605 \t\tpolicy loss:0.555517638368266 \t\tavg entropy:0.5638666760587796 \t\tstd entropy:0.509039520944004\n",
            "Episode 118 finished after 200 timesteps - cumulative reward = -128.74561964952608\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:92.86450899214972 \t\tpolicy loss:0.5870335730058807 \t\tavg entropy:0.5543474177711943 \t\tstd entropy:0.4979955561824659\n",
            "Epoch: 2 \t\tvalue loss:103.16968209402901 \t\tpolicy loss:0.6357445174029895 \t\tavg entropy:0.5401322513353722 \t\tstd entropy:0.497423572454049\n",
            "Epoch: 3 \t\tvalue loss:114.5658243724278 \t\tpolicy loss:0.7036691006450426 \t\tavg entropy:0.6153075166425864 \t\tstd entropy:0.5178723115002677\n",
            "Epoch: 4 \t\tvalue loss:90.94482794262115 \t\tpolicy loss:0.8883433118462563 \t\tavg entropy:0.5854130030342476 \t\tstd entropy:0.5157543980465593\n",
            "Epoch: 5 \t\tvalue loss:113.33113132204328 \t\tpolicy loss:0.8801694274658248 \t\tavg entropy:0.6590861282547046 \t\tstd entropy:0.5519157561812669\n",
            "Epoch: 6 \t\tvalue loss:119.76556362424579 \t\tpolicy loss:0.897720723989464 \t\tavg entropy:0.7427612194557415 \t\tstd entropy:0.5548340887991429\n",
            "Epoch: 7 \t\tvalue loss:89.41116821198236 \t\tpolicy loss:0.735921629128002 \t\tavg entropy:0.6720005688758134 \t\tstd entropy:0.544606973107173\n",
            "Epoch: 8 \t\tvalue loss:113.45685159592401 \t\tpolicy loss:0.8181803368386769 \t\tavg entropy:0.6930012686006062 \t\tstd entropy:0.5666280468709175\n",
            "Epoch: 9 \t\tvalue loss:119.60873331342425 \t\tpolicy loss:0.7805917273674693 \t\tavg entropy:0.7622979593137528 \t\tstd entropy:0.5776952268826675\n",
            "Epoch: 10 \t\tvalue loss:89.46891671135312 \t\tpolicy loss:0.561770699208691 \t\tavg entropy:0.6751987654113353 \t\tstd entropy:0.5457673824105121\n",
            "Episode 119 finished after 200 timesteps - cumulative reward = -119.85397496916757\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:102.41535713559105 \t\tpolicy loss:0.6298211820068813 \t\tavg entropy:0.6347988897998934 \t\tstd entropy:0.5453956639080996\n",
            "Epoch: 2 \t\tvalue loss:111.37499361946469 \t\tpolicy loss:0.6199889622983479 \t\tavg entropy:0.6803444720708995 \t\tstd entropy:0.5630896095066268\n",
            "Epoch: 3 \t\tvalue loss:94.43392762683686 \t\tpolicy loss:0.5728647989176568 \t\tavg entropy:0.6373371050288126 \t\tstd entropy:0.5385024459724581\n",
            "Epoch: 4 \t\tvalue loss:94.02605887821743 \t\tpolicy loss:0.6195317116521654 \t\tavg entropy:0.6127574834697275 \t\tstd entropy:0.5406429324005846\n",
            "Epoch: 5 \t\tvalue loss:110.91649536859421 \t\tpolicy loss:0.7319438170109477 \t\tavg entropy:0.6603017561313244 \t\tstd entropy:0.5584900494663471\n",
            "Epoch: 6 \t\tvalue loss:104.03185081481934 \t\tpolicy loss:0.7503954186325982 \t\tavg entropy:0.6830683064707267 \t\tstd entropy:0.5417848026690629\n",
            "Epoch: 7 \t\tvalue loss:88.480178242638 \t\tpolicy loss:0.7479293672811418 \t\tavg entropy:0.6372525503031283 \t\tstd entropy:0.5459608694849771\n",
            "Epoch: 8 \t\tvalue loss:113.71163184302193 \t\tpolicy loss:0.9219689873002824 \t\tavg entropy:0.7079362707443722 \t\tstd entropy:0.5789484206942399\n",
            "Epoch: 9 \t\tvalue loss:120.1806670370556 \t\tpolicy loss:0.8335527809602874 \t\tavg entropy:0.7665252440160734 \t\tstd entropy:0.5789880257615243\n",
            "Epoch: 10 \t\tvalue loss:86.91815433048066 \t\tpolicy loss:0.7539292701653072 \t\tavg entropy:0.7146943715919991 \t\tstd entropy:0.5714897099695342\n",
            "Episode 120 finished after 200 timesteps - cumulative reward = -126.55638978075946\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:97.20271157657399 \t\tpolicy loss:0.8017200498019947 \t\tavg entropy:0.711267138726875 \t\tstd entropy:0.5813871170519948\n",
            "Epoch: 2 \t\tvalue loss:125.37963983872358 \t\tpolicy loss:0.7337133358506596 \t\tavg entropy:0.7660132987284517 \t\tstd entropy:0.5922552218860981\n",
            "Epoch: 3 \t\tvalue loss:92.45605192745433 \t\tpolicy loss:0.6952088412116556 \t\tavg entropy:0.7397931662213383 \t\tstd entropy:0.5801304585434063\n",
            "Epoch: 4 \t\tvalue loss:84.3102731592515 \t\tpolicy loss:0.5647307420478148 \t\tavg entropy:0.659642138776032 \t\tstd entropy:0.5536698022118819\n",
            "Epoch: 5 \t\tvalue loss:106.9452870312859 \t\tpolicy loss:0.6406433694502887 \t\tavg entropy:0.6754799433336274 \t\tstd entropy:0.5680340480727837\n",
            "Epoch: 6 \t\tvalue loss:101.42490070567412 \t\tpolicy loss:0.6104511864045087 \t\tavg entropy:0.6874586228503469 \t\tstd entropy:0.5662707816317968\n",
            "Epoch: 7 \t\tvalue loss:80.71189303678625 \t\tpolicy loss:0.5218064020661747 \t\tavg entropy:0.6211898561754147 \t\tstd entropy:0.5370936075378282\n",
            "Epoch: 8 \t\tvalue loss:91.05488537059111 \t\tpolicy loss:0.5781512600534102 \t\tavg entropy:0.6100905844833027 \t\tstd entropy:0.5449127409150744\n",
            "Epoch: 9 \t\tvalue loss:101.89256313548368 \t\tpolicy loss:0.5577423053629258 \t\tavg entropy:0.6344967574398565 \t\tstd entropy:0.5512144050720484\n",
            "Epoch: 10 \t\tvalue loss:83.96721643560073 \t\tpolicy loss:0.588282792708453 \t\tavg entropy:0.6191530776761467 \t\tstd entropy:0.5352995176106717\n",
            "Episode 121 finished after 200 timesteps - cumulative reward = -122.90599901037969\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:83.67633599674001 \t\tpolicy loss:0.5609264240545385 \t\tavg entropy:0.5835074193515424 \t\tstd entropy:0.5297542902956613\n",
            "Epoch: 2 \t\tvalue loss:103.00961667229147 \t\tpolicy loss:0.6284253411433276 \t\tavg entropy:0.6243080307846521 \t\tstd entropy:0.5482112138247093\n",
            "Epoch: 3 \t\tvalue loss:88.5943093916949 \t\tpolicy loss:0.6670310893479515 \t\tavg entropy:0.6465423732954616 \t\tstd entropy:0.5455707159211242\n",
            "Epoch: 4 \t\tvalue loss:77.92882268569049 \t\tpolicy loss:0.5688845487201915 \t\tavg entropy:0.5820111760018252 \t\tstd entropy:0.5226111693482175\n",
            "Epoch: 5 \t\tvalue loss:105.26486253177418 \t\tpolicy loss:0.6679191189653734 \t\tavg entropy:0.633922070244761 \t\tstd entropy:0.5525279944692012\n",
            "Epoch: 6 \t\tvalue loss:91.08390139411478 \t\tpolicy loss:0.6424013099249671 \t\tavg entropy:0.6507524493085942 \t\tstd entropy:0.5482933802051784\n",
            "Epoch: 7 \t\tvalue loss:73.97350739871754 \t\tpolicy loss:0.5479609917191898 \t\tavg entropy:0.5784474112682131 \t\tstd entropy:0.517292689654812\n",
            "Epoch: 8 \t\tvalue loss:99.36480885673971 \t\tpolicy loss:0.6582279342062333 \t\tavg entropy:0.620656055119278 \t\tstd entropy:0.5527240423345245\n",
            "Epoch: 9 \t\tvalue loss:93.87907082052791 \t\tpolicy loss:0.5984592784853543 \t\tavg entropy:0.6239366779819451 \t\tstd entropy:0.5463594375877937\n",
            "Epoch: 10 \t\tvalue loss:75.27552728091969 \t\tpolicy loss:0.6709278797402102 \t\tavg entropy:0.5923786624758426 \t\tstd entropy:0.5317790857825686\n",
            "Episode 122 finished after 200 timesteps - cumulative reward = -125.34432095420986\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:89.16403285755831 \t\tpolicy loss:0.7341884833924911 \t\tavg entropy:0.61327823589557 \t\tstd entropy:0.5592221903124696\n",
            "Epoch: 2 \t\tvalue loss:104.87390538383933 \t\tpolicy loss:0.7261285589021795 \t\tavg entropy:0.675016629411301 \t\tstd entropy:0.5817302958889138\n",
            "Epoch: 3 \t\tvalue loss:79.39662235484404 \t\tpolicy loss:0.7244134934509502 \t\tavg entropy:0.6592501150663534 \t\tstd entropy:0.5732828713078598\n",
            "Epoch: 4 \t\tvalue loss:81.0760259067311 \t\tpolicy loss:0.7001194645376766 \t\tavg entropy:0.6448109798942706 \t\tstd entropy:0.5682475866974331\n",
            "Epoch: 5 \t\tvalue loss:113.05297885221593 \t\tpolicy loss:0.726746974622502 \t\tavg entropy:0.7082403151294185 \t\tstd entropy:0.5933779362458635\n",
            "Epoch: 6 \t\tvalue loss:81.53127421210795 \t\tpolicy loss:0.6279524333336775 \t\tavg entropy:0.678673313935273 \t\tstd entropy:0.5762365122527922\n",
            "Epoch: 7 \t\tvalue loss:74.53519229888916 \t\tpolicy loss:0.56801217584049 \t\tavg entropy:0.6067005333951674 \t\tstd entropy:0.542292430975389\n",
            "Epoch: 8 \t\tvalue loss:104.70614359238569 \t\tpolicy loss:0.6746737431077396 \t\tavg entropy:0.6592883313753573 \t\tstd entropy:0.5791380962271185\n",
            "Epoch: 9 \t\tvalue loss:89.99440473668716 \t\tpolicy loss:0.6156618454877067 \t\tavg entropy:0.6455540145009095 \t\tstd entropy:0.5621766185741284\n",
            "Epoch: 10 \t\tvalue loss:74.71088619232178 \t\tpolicy loss:0.6604524314403534 \t\tavg entropy:0.6166940546353354 \t\tstd entropy:0.552148576607741\n",
            "Episode 123 finished after 200 timesteps - cumulative reward = -247.93514921099526\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:89.77854663624483 \t\tpolicy loss:0.7199742860653822 \t\tavg entropy:0.6440770810631947 \t\tstd entropy:0.5790464320074336\n",
            "Epoch: 2 \t\tvalue loss:106.53387960546156 \t\tpolicy loss:0.7350476675173816 \t\tavg entropy:0.6880183622516904 \t\tstd entropy:0.5864586037225629\n",
            "Epoch: 3 \t\tvalue loss:79.77081598394058 \t\tpolicy loss:0.6797332076465382 \t\tavg entropy:0.6771715216200879 \t\tstd entropy:0.5830209900100688\n",
            "Epoch: 4 \t\tvalue loss:74.32688392190373 \t\tpolicy loss:0.7150215723935296 \t\tavg entropy:0.633503916737791 \t\tstd entropy:0.5630171203834388\n",
            "Epoch: 5 \t\tvalue loss:110.3367068234612 \t\tpolicy loss:0.735084114705815 \t\tavg entropy:0.71185998798964 \t\tstd entropy:0.601699014993481\n",
            "Epoch: 6 \t\tvalue loss:83.35230337030747 \t\tpolicy loss:0.5843941257280462 \t\tavg entropy:0.6877237964700892 \t\tstd entropy:0.5890314810368791\n",
            "Epoch: 7 \t\tvalue loss:69.45738683588364 \t\tpolicy loss:0.5261420751319212 \t\tavg entropy:0.5967005015450089 \t\tstd entropy:0.5442429966028055\n",
            "Epoch: 8 \t\tvalue loss:93.94199950274299 \t\tpolicy loss:0.6612339608809528 \t\tavg entropy:0.6305422130958269 \t\tstd entropy:0.574415310956159\n",
            "Epoch: 9 \t\tvalue loss:88.70055400623994 \t\tpolicy loss:0.5660819881102618 \t\tavg entropy:0.6247059779960857 \t\tstd entropy:0.5665543722131374\n",
            "Epoch: 10 \t\tvalue loss:73.81247546252082 \t\tpolicy loss:0.6787205576896668 \t\tavg entropy:0.619867865155894 \t\tstd entropy:0.5593071715607297\n",
            "Episode 124 finished after 200 timesteps - cumulative reward = -246.9570101940041\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:78.43885197358973 \t\tpolicy loss:0.649203604109147 \t\tavg entropy:0.6147703559523636 \t\tstd entropy:0.5699729784736716\n",
            "Epoch: 2 \t\tvalue loss:101.36758456510655 \t\tpolicy loss:0.7151742924662198 \t\tavg entropy:0.654824608926389 \t\tstd entropy:0.5746414695013669\n",
            "Epoch: 3 \t\tvalue loss:86.45942354763255 \t\tpolicy loss:0.7804051427280202 \t\tavg entropy:0.7059453853363251 \t\tstd entropy:0.5940455007988785\n",
            "Epoch: 4 \t\tvalue loss:67.08511328977697 \t\tpolicy loss:0.6252660639145795 \t\tavg entropy:0.6418383413331383 \t\tstd entropy:0.5682859672984302\n",
            "Epoch: 5 \t\tvalue loss:94.14023148031795 \t\tpolicy loss:0.7799017247031717 \t\tavg entropy:0.6757136054942867 \t\tstd entropy:0.5794001671246158\n",
            "Epoch: 6 \t\tvalue loss:98.95685710906983 \t\tpolicy loss:0.6986673239399405 \t\tavg entropy:0.7173403695939147 \t\tstd entropy:0.6007221856253039\n",
            "Epoch: 7 \t\tvalue loss:65.31231175590963 \t\tpolicy loss:0.4994648425018086 \t\tavg entropy:0.6235967895306699 \t\tstd entropy:0.5564166888577733\n",
            "Epoch: 8 \t\tvalue loss:74.8325709511252 \t\tpolicy loss:0.6288713605964885 \t\tavg entropy:0.6145353505948253 \t\tstd entropy:0.5576732023921522\n",
            "Epoch: 9 \t\tvalue loss:92.10400656531839 \t\tpolicy loss:0.5750408361939823 \t\tavg entropy:0.6450890110502833 \t\tstd entropy:0.5771900833455302\n",
            "Epoch: 10 \t\tvalue loss:75.03474012262681 \t\tpolicy loss:0.5174191387260662 \t\tavg entropy:0.5864435245265177 \t\tstd entropy:0.5383359772285001\n",
            "Episode 125 finished after 200 timesteps - cumulative reward = -1.9513297180048956\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:71.26286929152732 \t\tpolicy loss:0.6886933019687963 \t\tavg entropy:0.5997615446986726 \t\tstd entropy:0.5594075102826916\n",
            "Epoch: 2 \t\tvalue loss:79.00793532438057 \t\tpolicy loss:0.6940659852915032 \t\tavg entropy:0.6414622998177095 \t\tstd entropy:0.5803469184719143\n",
            "Epoch: 3 \t\tvalue loss:90.26394980452781 \t\tpolicy loss:0.6665325116279514 \t\tavg entropy:0.647696147630494 \t\tstd entropy:0.5757560262106001\n",
            "Epoch: 4 \t\tvalue loss:75.4500360599784 \t\tpolicy loss:0.7480218382768853 \t\tavg entropy:0.6617940047392106 \t\tstd entropy:0.5906659795055703\n",
            "Epoch: 5 \t\tvalue loss:68.53376846535261 \t\tpolicy loss:0.6855578446804091 \t\tavg entropy:0.6455949742428021 \t\tstd entropy:0.5813829705349177\n",
            "Epoch: 6 \t\tvalue loss:98.84099106455959 \t\tpolicy loss:0.7581614571255307 \t\tavg entropy:0.6838067254840997 \t\tstd entropy:0.5824005364077018\n",
            "Epoch: 7 \t\tvalue loss:86.9477523759354 \t\tpolicy loss:0.754556940738545 \t\tavg entropy:0.7179442960307679 \t\tstd entropy:0.6029745353992378\n",
            "Epoch: 8 \t\tvalue loss:61.565639540206554 \t\tpolicy loss:0.577092933446862 \t\tavg entropy:0.6447962827655823 \t\tstd entropy:0.5788229483239803\n",
            "Epoch: 9 \t\tvalue loss:78.36240572153136 \t\tpolicy loss:0.7725126826485922 \t\tavg entropy:0.6607776468321366 \t\tstd entropy:0.575557741075\n",
            "Epoch: 10 \t\tvalue loss:97.0937089642813 \t\tpolicy loss:0.7269626315249953 \t\tavg entropy:0.7123090470824722 \t\tstd entropy:0.602768642975249\n",
            "Episode 126 finished after 200 timesteps - cumulative reward = -120.50435365477516\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:66.88792358997256 \t\tpolicy loss:0.5261289231305899 \t\tavg entropy:0.6348938687596625 \t\tstd entropy:0.5683301111019284\n",
            "Epoch: 2 \t\tvalue loss:65.85042273166567 \t\tpolicy loss:0.5306151311757953 \t\tavg entropy:0.5958382392695396 \t\tstd entropy:0.5500267953308275\n",
            "Epoch: 3 \t\tvalue loss:85.24628098620924 \t\tpolicy loss:0.586256297521813 \t\tavg entropy:0.6255984317735269 \t\tstd entropy:0.5709734138967003\n",
            "Epoch: 4 \t\tvalue loss:75.00145377114762 \t\tpolicy loss:0.5391349387030269 \t\tavg entropy:0.5933195412569633 \t\tstd entropy:0.5435080698747997\n",
            "Epoch: 5 \t\tvalue loss:65.91114489422289 \t\tpolicy loss:0.6136134991119074 \t\tavg entropy:0.5863030362578544 \t\tstd entropy:0.5502593141655774\n",
            "Epoch: 6 \t\tvalue loss:72.72433698454569 \t\tpolicy loss:0.6380006664021071 \t\tavg entropy:0.6109963479360571 \t\tstd entropy:0.568516394724272\n",
            "Epoch: 7 \t\tvalue loss:87.46255302429199 \t\tpolicy loss:0.6871499831593314 \t\tavg entropy:0.6336936728337239 \t\tstd entropy:0.569335882695973\n",
            "Epoch: 8 \t\tvalue loss:72.23736643791199 \t\tpolicy loss:0.7209855927977451 \t\tavg entropy:0.6498022312674581 \t\tstd entropy:0.5877137423645913\n",
            "Epoch: 9 \t\tvalue loss:61.32139016306677 \t\tpolicy loss:0.6921312504036482 \t\tavg entropy:0.6410594300399993 \t\tstd entropy:0.5769399158112749\n",
            "Epoch: 10 \t\tvalue loss:93.52770686704059 \t\tpolicy loss:0.7659230925316034 \t\tavg entropy:0.696961082933131 \t\tstd entropy:0.5925148705951463\n",
            "Episode 127 finished after 200 timesteps - cumulative reward = -249.38481992000652\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:80.17904374765796 \t\tpolicy loss:0.691409885190254 \t\tavg entropy:0.6994393762487189 \t\tstd entropy:0.6078073636958964\n",
            "Epoch: 2 \t\tvalue loss:54.89207289385241 \t\tpolicy loss:0.6130632366551909 \t\tavg entropy:0.6349041211382367 \t\tstd entropy:0.5700251388383988\n",
            "Epoch: 3 \t\tvalue loss:80.45462322235107 \t\tpolicy loss:0.7119173463000807 \t\tavg entropy:0.6805121106127072 \t\tstd entropy:0.5854217372405288\n",
            "Epoch: 4 \t\tvalue loss:83.83082331613053 \t\tpolicy loss:0.6528760711121004 \t\tavg entropy:0.6766181768412346 \t\tstd entropy:0.5954639782944026\n",
            "Epoch: 5 \t\tvalue loss:58.998221070267434 \t\tpolicy loss:0.5451208332250285 \t\tavg entropy:0.5998940644527011 \t\tstd entropy:0.5544959308199141\n",
            "Epoch: 6 \t\tvalue loss:70.55548590283061 \t\tpolicy loss:0.644180963898814 \t\tavg entropy:0.6162808699011362 \t\tstd entropy:0.5781388902050699\n",
            "Epoch: 7 \t\tvalue loss:86.03811775251876 \t\tpolicy loss:0.6765066801115523 \t\tavg entropy:0.6477752168517791 \t\tstd entropy:0.5784953862887021\n",
            "Epoch: 8 \t\tvalue loss:73.63497813357863 \t\tpolicy loss:0.7546171247959137 \t\tavg entropy:0.6546007017633175 \t\tstd entropy:0.5783859950264282\n",
            "Epoch: 9 \t\tvalue loss:63.69647440799447 \t\tpolicy loss:0.6256838652283646 \t\tavg entropy:0.6366772593536024 \t\tstd entropy:0.5914108605653076\n",
            "Epoch: 10 \t\tvalue loss:84.8951088883156 \t\tpolicy loss:0.7874750266241473 \t\tavg entropy:0.6801262053050305 \t\tstd entropy:0.5887291118460425\n",
            "Episode 128 finished after 200 timesteps - cumulative reward = -299.2839105614224\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:87.5493870058725 \t\tpolicy loss:0.7746161638304244 \t\tavg entropy:0.7211344527566669 \t\tstd entropy:0.6040339297309673\n",
            "Epoch: 2 \t\tvalue loss:57.83092508759609 \t\tpolicy loss:0.580713496998299 \t\tavg entropy:0.6603918659541987 \t\tstd entropy:0.5981612165902249\n",
            "Epoch: 3 \t\tvalue loss:63.87819327310074 \t\tpolicy loss:0.6670148552850236 \t\tavg entropy:0.6503760925930631 \t\tstd entropy:0.5804136452795905\n",
            "Epoch: 4 \t\tvalue loss:89.03910835953646 \t\tpolicy loss:0.6549086033604866 \t\tavg entropy:0.6752014317054603 \t\tstd entropy:0.5934156790509187\n",
            "Epoch: 5 \t\tvalue loss:62.16382311665735 \t\tpolicy loss:0.5464379129021667 \t\tavg entropy:0.6074309023282852 \t\tstd entropy:0.5725144966001808\n",
            "Epoch: 6 \t\tvalue loss:59.40605516211931 \t\tpolicy loss:0.508861324814863 \t\tavg entropy:0.585128020196548 \t\tstd entropy:0.5666678670882747\n",
            "Epoch: 7 \t\tvalue loss:78.78280605271806 \t\tpolicy loss:0.5820806698743687 \t\tavg entropy:0.5984266551270014 \t\tstd entropy:0.579291391020708\n",
            "Epoch: 8 \t\tvalue loss:72.30143328045689 \t\tpolicy loss:0.5943409447060075 \t\tavg entropy:0.5885881841521914 \t\tstd entropy:0.5547629732836081\n",
            "Epoch: 9 \t\tvalue loss:66.61460705690605 \t\tpolicy loss:0.669240074795346 \t\tavg entropy:0.5885132846873607 \t\tstd entropy:0.5829828417608703\n",
            "Epoch: 10 \t\tvalue loss:68.86359584608743 \t\tpolicy loss:0.7332535091527673 \t\tavg entropy:0.6350903117789596 \t\tstd entropy:0.5886330023390368\n",
            "Episode 129 finished after 200 timesteps - cumulative reward = -244.07109675533445\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:87.9812882944595 \t\tpolicy loss:0.7923995952966602 \t\tavg entropy:0.6835406621421456 \t\tstd entropy:0.592744332548333\n",
            "Epoch: 2 \t\tvalue loss:75.21324424965437 \t\tpolicy loss:0.7203352600336075 \t\tavg entropy:0.6602444704096971 \t\tstd entropy:0.6146872981480636\n",
            "Epoch: 3 \t\tvalue loss:59.72444371844447 \t\tpolicy loss:0.8470486049042192 \t\tavg entropy:0.6860372519721909 \t\tstd entropy:0.5946370131201388\n",
            "Epoch: 4 \t\tvalue loss:88.39256522821826 \t\tpolicy loss:0.7795178304577983 \t\tavg entropy:0.7415284509115471 \t\tstd entropy:0.6056410294746282\n",
            "Epoch: 5 \t\tvalue loss:79.74296764994777 \t\tpolicy loss:0.6403537569350974 \t\tavg entropy:0.7041737630263458 \t\tstd entropy:0.6296790146219756\n",
            "Epoch: 6 \t\tvalue loss:57.123851193938144 \t\tpolicy loss:0.6405518273281496 \t\tavg entropy:0.6627847529901152 \t\tstd entropy:0.5900837202450094\n",
            "Epoch: 7 \t\tvalue loss:77.03631626173507 \t\tpolicy loss:0.6765628398157829 \t\tavg entropy:0.6822498649798766 \t\tstd entropy:0.5941325804919808\n",
            "Epoch: 8 \t\tvalue loss:78.90067240249279 \t\tpolicy loss:0.5678539515234703 \t\tavg entropy:0.6579566608994832 \t\tstd entropy:0.6171932356348742\n",
            "Epoch: 9 \t\tvalue loss:61.58158833481545 \t\tpolicy loss:0.5264861472817355 \t\tavg entropy:0.6054157236387906 \t\tstd entropy:0.5678322144878323\n",
            "Epoch: 10 \t\tvalue loss:71.2812486526578 \t\tpolicy loss:0.6250535443771718 \t\tavg entropy:0.6203061305761803 \t\tstd entropy:0.576160170595774\n",
            "Episode 130 finished after 200 timesteps - cumulative reward = -229.88078591688432\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:73.70450148089179 \t\tpolicy loss:0.5964063649204956 \t\tavg entropy:0.609373291528553 \t\tstd entropy:0.6006042573257304\n",
            "Epoch: 2 \t\tvalue loss:71.37702443133826 \t\tpolicy loss:0.5550679954989203 \t\tavg entropy:0.5936877349967618 \t\tstd entropy:0.5650728351216433\n",
            "Epoch: 3 \t\tvalue loss:67.13385757358594 \t\tpolicy loss:0.7720279536028017 \t\tavg entropy:0.6200711070610322 \t\tstd entropy:0.5859313852315559\n",
            "Epoch: 4 \t\tvalue loss:80.574816868223 \t\tpolicy loss:0.884878950900045 \t\tavg entropy:0.6652279706608489 \t\tstd entropy:0.6194962029310066\n",
            "Epoch: 5 \t\tvalue loss:99.0396772801191 \t\tpolicy loss:0.8234223195876198 \t\tavg entropy:0.7289758811922424 \t\tstd entropy:0.6087678307637003\n",
            "Epoch: 6 \t\tvalue loss:79.13251868061636 \t\tpolicy loss:0.9475235939025879 \t\tavg entropy:0.7279979293966642 \t\tstd entropy:0.6184177711453193\n",
            "Epoch: 7 \t\tvalue loss:84.4042147713146 \t\tpolicy loss:0.9804197179860082 \t\tavg entropy:0.7512925348827635 \t\tstd entropy:0.6540105230800703\n",
            "Epoch: 8 \t\tvalue loss:121.52658975535425 \t\tpolicy loss:1.1009411256888817 \t\tavg entropy:0.8666948684924185 \t\tstd entropy:0.6438138933710457\n",
            "Epoch: 9 \t\tvalue loss:101.96155800216499 \t\tpolicy loss:0.9730432725500786 \t\tavg entropy:0.8746996617809722 \t\tstd entropy:0.6387866332973411\n",
            "Epoch: 10 \t\tvalue loss:94.65327732042334 \t\tpolicy loss:0.7780595044980104 \t\tavg entropy:0.8437543375654216 \t\tstd entropy:0.658041856075105\n",
            "Episode 131 finished after 200 timesteps - cumulative reward = -125.87006925109057\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:78.46409232040932 \t\tpolicy loss:0.9807086655463295 \t\tavg entropy:0.8071887594935903 \t\tstd entropy:0.6604206090039745\n",
            "Epoch: 2 \t\tvalue loss:95.86642060334655 \t\tpolicy loss:0.9588370261521175 \t\tavg entropy:0.8726905414302059 \t\tstd entropy:0.6284673376241031\n",
            "Epoch: 3 \t\tvalue loss:104.52120640634121 \t\tpolicy loss:0.8930347411111853 \t\tavg entropy:0.8712115318848319 \t\tstd entropy:0.6187557531057503\n",
            "Epoch: 4 \t\tvalue loss:93.45986489043838 \t\tpolicy loss:0.7892712964408699 \t\tavg entropy:0.8434841759683938 \t\tstd entropy:0.6491179941956486\n",
            "Epoch: 5 \t\tvalue loss:72.61783751125994 \t\tpolicy loss:0.7695117599662693 \t\tavg entropy:0.791171167291718 \t\tstd entropy:0.6439532056031372\n",
            "Epoch: 6 \t\tvalue loss:73.68617521483323 \t\tpolicy loss:0.7225436444940239 \t\tavg entropy:0.7894313567573596 \t\tstd entropy:0.6115744083661209\n",
            "Epoch: 7 \t\tvalue loss:86.19410559774815 \t\tpolicy loss:0.7625710196878718 \t\tavg entropy:0.7822704537832834 \t\tstd entropy:0.6064088546988805\n",
            "Epoch: 8 \t\tvalue loss:85.01143804089776 \t\tpolicy loss:0.6998524891919103 \t\tavg entropy:0.7630193524447902 \t\tstd entropy:0.6419906199035799\n",
            "Epoch: 9 \t\tvalue loss:70.12557060417087 \t\tpolicy loss:0.630016016891633 \t\tavg entropy:0.7236998014398582 \t\tstd entropy:0.6366052609602809\n",
            "Epoch: 10 \t\tvalue loss:69.63436246192319 \t\tpolicy loss:0.6029858760450079 \t\tavg entropy:0.700923697548553 \t\tstd entropy:0.6064919176551895\n",
            "Episode 132 finished after 200 timesteps - cumulative reward = -123.10997780831106\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:74.99955106603689 \t\tpolicy loss:0.6575599031886835 \t\tavg entropy:0.6966702037105513 \t\tstd entropy:0.6058080518860312\n",
            "Epoch: 2 \t\tvalue loss:75.18745080904029 \t\tpolicy loss:0.6210630118161783 \t\tavg entropy:0.6782056181876589 \t\tstd entropy:0.6343969400927187\n",
            "Epoch: 3 \t\tvalue loss:68.49020108409312 \t\tpolicy loss:0.6093203774813948 \t\tavg entropy:0.6674497309261879 \t\tstd entropy:0.6327799236340005\n",
            "Epoch: 4 \t\tvalue loss:70.81657042996636 \t\tpolicy loss:0.5848206367300844 \t\tavg entropy:0.6547247842712529 \t\tstd entropy:0.6126516742412967\n",
            "Epoch: 5 \t\tvalue loss:69.19175242829597 \t\tpolicy loss:0.6178620717991358 \t\tavg entropy:0.6421001352444067 \t\tstd entropy:0.6210111671048344\n",
            "Epoch: 6 \t\tvalue loss:72.163020287437 \t\tpolicy loss:0.6450954655121113 \t\tavg entropy:0.6485562334726029 \t\tstd entropy:0.6355028556741703\n",
            "Epoch: 7 \t\tvalue loss:79.15575827675305 \t\tpolicy loss:0.6370805060041362 \t\tavg entropy:0.6461134710469786 \t\tstd entropy:0.6165696722535432\n",
            "Epoch: 8 \t\tvalue loss:65.74709232922258 \t\tpolicy loss:0.7092368099196203 \t\tavg entropy:0.6450401766823997 \t\tstd entropy:0.6133051307724039\n",
            "Epoch: 9 \t\tvalue loss:74.72054191019343 \t\tpolicy loss:0.8580360953835235 \t\tavg entropy:0.6591362593630781 \t\tstd entropy:0.6339685673062764\n",
            "Epoch: 10 \t\tvalue loss:98.59846314616587 \t\tpolicy loss:0.852679680133688 \t\tavg entropy:0.7444783529681209 \t\tstd entropy:0.6315392059372634\n",
            "Episode 133 finished after 200 timesteps - cumulative reward = -250.414105734236\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:76.99438517121062 \t\tpolicy loss:0.8668215535152918 \t\tavg entropy:0.7527317054679192 \t\tstd entropy:0.609890645783278\n",
            "Epoch: 2 \t\tvalue loss:79.00745261400596 \t\tpolicy loss:0.7807309024635403 \t\tavg entropy:0.7266791986534396 \t\tstd entropy:0.6348813030954674\n",
            "Epoch: 3 \t\tvalue loss:79.90531223121731 \t\tpolicy loss:1.0557187918958992 \t\tavg entropy:0.7894201977767558 \t\tstd entropy:0.6539964240788307\n",
            "Epoch: 4 \t\tvalue loss:113.34594452518157 \t\tpolicy loss:0.9443227347286268 \t\tavg entropy:0.8611356856556938 \t\tstd entropy:0.6513952506100421\n",
            "Epoch: 5 \t\tvalue loss:84.64471445412471 \t\tpolicy loss:0.8842687223149442 \t\tavg entropy:0.8411222803916224 \t\tstd entropy:0.6442225656876202\n",
            "Epoch: 6 \t\tvalue loss:83.50143552100522 \t\tpolicy loss:0.7746495796346117 \t\tavg entropy:0.802586269077465 \t\tstd entropy:0.6546706053167507\n",
            "Epoch: 7 \t\tvalue loss:75.80759165752893 \t\tpolicy loss:0.95076608726348 \t\tavg entropy:0.8056444651123734 \t\tstd entropy:0.6530607168420456\n",
            "Epoch: 8 \t\tvalue loss:97.60351447401375 \t\tpolicy loss:0.8205406508226504 \t\tavg entropy:0.8424886069912785 \t\tstd entropy:0.6362011607534434\n",
            "Epoch: 9 \t\tvalue loss:86.30908732578672 \t\tpolicy loss:0.8300858585313818 \t\tavg entropy:0.8212367451370238 \t\tstd entropy:0.6355677150385383\n",
            "Epoch: 10 \t\tvalue loss:82.90439537749893 \t\tpolicy loss:0.7877709296928055 \t\tavg entropy:0.8053903114295877 \t\tstd entropy:0.6498906888988862\n",
            "Episode 134 finished after 200 timesteps - cumulative reward = -116.37700612046254\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:73.58017879793013 \t\tpolicy loss:0.8845994027181604 \t\tavg entropy:0.8228112456193408 \t\tstd entropy:0.6458938931521393\n",
            "Epoch: 2 \t\tvalue loss:79.66514741963354 \t\tpolicy loss:0.8344785598502762 \t\tavg entropy:0.8331962060895735 \t\tstd entropy:0.6142626177518946\n",
            "Epoch: 3 \t\tvalue loss:78.40189260175859 \t\tpolicy loss:0.7960736532320921 \t\tavg entropy:0.8209686242550379 \t\tstd entropy:0.6060340874550685\n",
            "Epoch: 4 \t\tvalue loss:80.18034411572862 \t\tpolicy loss:0.7545174524701875 \t\tavg entropy:0.7791866986168169 \t\tstd entropy:0.6396836150393034\n",
            "Epoch: 5 \t\tvalue loss:77.74232925765816 \t\tpolicy loss:0.7658665741997204 \t\tavg entropy:0.8039260294446468 \t\tstd entropy:0.6442598046362403\n",
            "Epoch: 6 \t\tvalue loss:73.05620120037561 \t\tpolicy loss:0.6803963540614336 \t\tavg entropy:0.7658039014288993 \t\tstd entropy:0.6148796809635141\n",
            "Epoch: 7 \t\tvalue loss:66.85890710764917 \t\tpolicy loss:0.6758064035026506 \t\tavg entropy:0.7266761102984846 \t\tstd entropy:0.6085931051235569\n",
            "Epoch: 8 \t\tvalue loss:77.3331721459312 \t\tpolicy loss:0.7499865430524979 \t\tavg entropy:0.7290337946827385 \t\tstd entropy:0.6339701779243638\n",
            "Epoch: 9 \t\tvalue loss:82.8330848189606 \t\tpolicy loss:0.7174505618796951 \t\tavg entropy:0.7575894145398464 \t\tstd entropy:0.6417190825685897\n",
            "Epoch: 10 \t\tvalue loss:64.25456052538992 \t\tpolicy loss:0.7084310784422118 \t\tavg entropy:0.73737809367753 \t\tstd entropy:0.6160167844119754\n",
            "Episode 135 finished after 200 timesteps - cumulative reward = -3.119092741385032\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:63.94824024222114 \t\tpolicy loss:0.8257311677390878 \t\tavg entropy:0.7051982974120219 \t\tstd entropy:0.6231720581104575\n",
            "Epoch: 2 \t\tvalue loss:88.12155145948583 \t\tpolicy loss:0.906665098938075 \t\tavg entropy:0.8274964784165599 \t\tstd entropy:0.6448430974751903\n",
            "Epoch: 3 \t\tvalue loss:84.25599416819486 \t\tpolicy loss:0.7927541367032311 \t\tavg entropy:0.807565213884233 \t\tstd entropy:0.6295826675112033\n",
            "Epoch: 4 \t\tvalue loss:59.27196590737863 \t\tpolicy loss:0.7089401723986323 \t\tavg entropy:0.7299743186513177 \t\tstd entropy:0.6190455956916385\n",
            "Epoch: 5 \t\tvalue loss:73.35230213403702 \t\tpolicy loss:0.9993170252577825 \t\tavg entropy:0.7670380347266753 \t\tstd entropy:0.6458452235294967\n",
            "Epoch: 6 \t\tvalue loss:109.84758014028722 \t\tpolicy loss:1.0311835116960786 \t\tavg entropy:0.8936639554040432 \t\tstd entropy:0.6521799369043321\n",
            "Epoch: 7 \t\tvalue loss:82.8004943945191 \t\tpolicy loss:0.9190979837016626 \t\tavg entropy:0.8902839632547833 \t\tstd entropy:0.6373844165861434\n",
            "Epoch: 8 \t\tvalue loss:67.50937229936773 \t\tpolicy loss:0.8080310645428571 \t\tavg entropy:0.8299113093476826 \t\tstd entropy:0.6357554884388065\n",
            "Epoch: 9 \t\tvalue loss:76.86923195015301 \t\tpolicy loss:0.9486971842971715 \t\tavg entropy:0.8633477700634116 \t\tstd entropy:0.6500274061724763\n",
            "Epoch: 10 \t\tvalue loss:105.36512471329083 \t\tpolicy loss:0.9615906043486162 \t\tavg entropy:0.8988558469459591 \t\tstd entropy:0.6411407904378013\n",
            "Episode 136 finished after 200 timesteps - cumulative reward = -128.3855160146251\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:88.67861604690552 \t\tpolicy loss:0.9081123809922825 \t\tavg entropy:0.8834977779903754 \t\tstd entropy:0.6359597619511566\n",
            "Epoch: 2 \t\tvalue loss:71.58632824637674 \t\tpolicy loss:0.823074939914725 \t\tavg entropy:0.8483407916382765 \t\tstd entropy:0.6414310693961957\n",
            "Epoch: 3 \t\tvalue loss:73.06099822304465 \t\tpolicy loss:0.8567120717330412 \t\tavg entropy:0.8838903508255779 \t\tstd entropy:0.6414040798603238\n",
            "Epoch: 4 \t\tvalue loss:86.39535587484187 \t\tpolicy loss:0.9091892845251344 \t\tavg entropy:0.8710855091153464 \t\tstd entropy:0.6287931763746218\n",
            "Epoch: 5 \t\tvalue loss:91.57320408929478 \t\tpolicy loss:0.9172968993132765 \t\tavg entropy:0.8745781636595975 \t\tstd entropy:0.6332052713822097\n",
            "Epoch: 6 \t\tvalue loss:72.71222376281565 \t\tpolicy loss:0.8140433518724008 \t\tavg entropy:0.8377918284296461 \t\tstd entropy:0.6436003167737043\n",
            "Epoch: 7 \t\tvalue loss:71.41850861094214 \t\tpolicy loss:0.8714112544601614 \t\tavg entropy:0.8702463937817952 \t\tstd entropy:0.6421419441796004\n",
            "Epoch: 8 \t\tvalue loss:73.26759738271886 \t\tpolicy loss:0.9310098574920134 \t\tavg entropy:0.8813292491679504 \t\tstd entropy:0.6258746975717369\n",
            "Epoch: 9 \t\tvalue loss:75.862808406353 \t\tpolicy loss:0.823958195745945 \t\tavg entropy:0.8791876987915673 \t\tstd entropy:0.6159050642420384\n",
            "Epoch: 10 \t\tvalue loss:69.80895459651947 \t\tpolicy loss:0.7441586018963293 \t\tavg entropy:0.8351667264858031 \t\tstd entropy:0.6365815616067249\n",
            "Episode 137 finished after 200 timesteps - cumulative reward = -2.8778208397448237\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:69.62483595718037 \t\tpolicy loss:0.8010119856758551 \t\tavg entropy:0.8043444320105116 \t\tstd entropy:0.650449790934584\n",
            "Epoch: 2 \t\tvalue loss:73.50526217438959 \t\tpolicy loss:0.7386578348549929 \t\tavg entropy:0.8285683270053101 \t\tstd entropy:0.6383010620630033\n",
            "Epoch: 3 \t\tvalue loss:68.02931756323034 \t\tpolicy loss:0.687210857190869 \t\tavg entropy:0.7858533788029168 \t\tstd entropy:0.6197508052902734\n",
            "Epoch: 4 \t\tvalue loss:63.02641622586684 \t\tpolicy loss:0.6854733282869513 \t\tavg entropy:0.7370002479095363 \t\tstd entropy:0.6222646074750947\n",
            "Epoch: 5 \t\tvalue loss:71.63779893246564 \t\tpolicy loss:0.6818742210214789 \t\tavg entropy:0.7365074525509242 \t\tstd entropy:0.6281208739677469\n",
            "Epoch: 6 \t\tvalue loss:70.60970417477868 \t\tpolicy loss:0.6101608482951467 \t\tavg entropy:0.712602229581056 \t\tstd entropy:0.6108380771978621\n",
            "Epoch: 7 \t\tvalue loss:53.31942941112952 \t\tpolicy loss:0.6725871891460635 \t\tavg entropy:0.6784494342454742 \t\tstd entropy:0.5859761537822251\n",
            "Epoch: 8 \t\tvalue loss:61.40453312071887 \t\tpolicy loss:0.7800683849914507 \t\tavg entropy:0.6642742087845621 \t\tstd entropy:0.5954936263737353\n",
            "Epoch: 9 \t\tvalue loss:79.92037478360263 \t\tpolicy loss:0.6962721300396052 \t\tavg entropy:0.7446445775654966 \t\tstd entropy:0.6104713949011434\n",
            "Epoch: 10 \t\tvalue loss:56.28972315788269 \t\tpolicy loss:0.6997531845488332 \t\tavg entropy:0.6951581485591521 \t\tstd entropy:0.5802447815528718\n",
            "Episode 138 finished after 200 timesteps - cumulative reward = -244.47939099456354\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:52.998020405119114 \t\tpolicy loss:0.8094825128262694 \t\tavg entropy:0.6655643373572537 \t\tstd entropy:0.576716032800215\n",
            "Epoch: 2 \t\tvalue loss:79.85324879126115 \t\tpolicy loss:0.8173848986625671 \t\tavg entropy:0.7858863954029917 \t\tstd entropy:0.6048639762918266\n",
            "Epoch: 3 \t\tvalue loss:70.4011321121996 \t\tpolicy loss:0.683209645815871 \t\tavg entropy:0.7478148144909073 \t\tstd entropy:0.5816228943099282\n",
            "Epoch: 4 \t\tvalue loss:49.564846908504315 \t\tpolicy loss:0.8637209829281677 \t\tavg entropy:0.6907640184958256 \t\tstd entropy:0.5608250390304315\n",
            "Epoch: 5 \t\tvalue loss:71.5779198624871 \t\tpolicy loss:0.8653544675220143 \t\tavg entropy:0.7904339723003573 \t\tstd entropy:0.6074241436606065\n",
            "Epoch: 6 \t\tvalue loss:90.60488255457444 \t\tpolicy loss:0.8619241704317656 \t\tavg entropy:0.8120153541919692 \t\tstd entropy:0.6120095420831829\n",
            "Epoch: 7 \t\tvalue loss:55.84896900437095 \t\tpolicy loss:0.8240095248276537 \t\tavg entropy:0.7755108943974656 \t\tstd entropy:0.5998553294823995\n",
            "Epoch: 8 \t\tvalue loss:59.334774591706015 \t\tpolicy loss:1.026129887862639 \t\tavg entropy:0.7978872388622087 \t\tstd entropy:0.6238416279713085\n",
            "Epoch: 9 \t\tvalue loss:84.73004100539468 \t\tpolicy loss:1.0543919205665588 \t\tavg entropy:0.905092916433817 \t\tstd entropy:0.649684102526402\n",
            "Epoch: 10 \t\tvalue loss:94.78087483210997 \t\tpolicy loss:0.9556802416389639 \t\tavg entropy:0.9206655937452853 \t\tstd entropy:0.6420119559379832\n",
            "Episode 139 finished after 200 timesteps - cumulative reward = -4.220686544602639\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:62.218818282539196 \t\tpolicy loss:0.8671673062172803 \t\tavg entropy:0.858885465290914 \t\tstd entropy:0.6360689106707551\n",
            "Epoch: 2 \t\tvalue loss:66.52795533700423 \t\tpolicy loss:1.010860995135524 \t\tavg entropy:0.8924686725657229 \t\tstd entropy:0.6447841828764954\n",
            "Epoch: 3 \t\tvalue loss:81.69051744721152 \t\tpolicy loss:0.9967919852245938 \t\tavg entropy:0.927812053792039 \t\tstd entropy:0.6511375385378467\n",
            "Epoch: 4 \t\tvalue loss:90.82893258875066 \t\tpolicy loss:0.9290067953142253 \t\tavg entropy:0.9444293610223474 \t\tstd entropy:0.6563782535601416\n",
            "Epoch: 5 \t\tvalue loss:66.25914988734506 \t\tpolicy loss:0.8305335647680543 \t\tavg entropy:0.8856151774210242 \t\tstd entropy:0.6593527270532145\n",
            "Epoch: 6 \t\tvalue loss:65.26968656886707 \t\tpolicy loss:0.9531767483461987 \t\tavg entropy:0.8940835752534807 \t\tstd entropy:0.6550063617747143\n",
            "Epoch: 7 \t\tvalue loss:72.794079227881 \t\tpolicy loss:0.9403493140231479 \t\tavg entropy:0.9180984182530162 \t\tstd entropy:0.6443314697236975\n",
            "Epoch: 8 \t\tvalue loss:74.66527444666082 \t\tpolicy loss:0.8717061199925162 \t\tavg entropy:0.9145377538053243 \t\tstd entropy:0.6308002973736087\n",
            "Epoch: 9 \t\tvalue loss:63.37268491224809 \t\tpolicy loss:0.7409056228670207 \t\tavg entropy:0.8665159778648673 \t\tstd entropy:0.6512649599194401\n",
            "Epoch: 10 \t\tvalue loss:67.07040883194317 \t\tpolicy loss:0.9011229886250063 \t\tavg entropy:0.8427588078177062 \t\tstd entropy:0.6614168649860432\n",
            "Episode 140 finished after 200 timesteps - cumulative reward = -246.43003578148216\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:73.73666831884492 \t\tpolicy loss:0.7646235616019602 \t\tavg entropy:0.8773490680431277 \t\tstd entropy:0.6461304592103096\n",
            "Epoch: 2 \t\tvalue loss:62.239785692665016 \t\tpolicy loss:0.7348359578780914 \t\tavg entropy:0.8236724077611095 \t\tstd entropy:0.6172199589244062\n",
            "Epoch: 3 \t\tvalue loss:58.34866371583403 \t\tpolicy loss:0.7805859054742235 \t\tavg entropy:0.7803250749002336 \t\tstd entropy:0.626730486127694\n",
            "Epoch: 4 \t\tvalue loss:77.35050196058295 \t\tpolicy loss:0.7848155391350221 \t\tavg entropy:0.8247798478196808 \t\tstd entropy:0.6321237856862959\n",
            "Epoch: 5 \t\tvalue loss:78.33395613981097 \t\tpolicy loss:0.680484173338065 \t\tavg entropy:0.7814446890707037 \t\tstd entropy:0.614032712615961\n",
            "Epoch: 6 \t\tvalue loss:48.73509484194638 \t\tpolicy loss:0.7786242054419571 \t\tavg entropy:0.7335333456171379 \t\tstd entropy:0.5739146744302546\n",
            "Epoch: 7 \t\tvalue loss:59.39830975318223 \t\tpolicy loss:1.0095649160695879 \t\tavg entropy:0.7822139591525247 \t\tstd entropy:0.6205685041539494\n",
            "Epoch: 8 \t\tvalue loss:90.50476267096701 \t\tpolicy loss:0.8793909429164415 \t\tavg entropy:0.8912326484583017 \t\tstd entropy:0.6542459897538728\n",
            "Epoch: 9 \t\tvalue loss:67.61045203048192 \t\tpolicy loss:0.773826197961743 \t\tavg entropy:0.8327425429819105 \t\tstd entropy:0.6194281390646795\n",
            "Epoch: 10 \t\tvalue loss:46.67018717058589 \t\tpolicy loss:0.8471657905016052 \t\tavg entropy:0.7572731979958026 \t\tstd entropy:0.5908251075229973\n",
            "Episode 141 finished after 200 timesteps - cumulative reward = -119.46330910847873\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:64.70864036377895 \t\tpolicy loss:0.9270202326640654 \t\tavg entropy:0.8169067959751068 \t\tstd entropy:0.6351592716879755\n",
            "Epoch: 2 \t\tvalue loss:93.36711584584097 \t\tpolicy loss:0.8739805589900927 \t\tavg entropy:0.8549984990798313 \t\tstd entropy:0.6564158638236762\n",
            "Epoch: 3 \t\tvalue loss:71.80964872810279 \t\tpolicy loss:0.8419567172446948 \t\tavg entropy:0.8449303514965987 \t\tstd entropy:0.6278820227071047\n",
            "Epoch: 4 \t\tvalue loss:49.891086897153535 \t\tpolicy loss:0.8352838098333123 \t\tavg entropy:0.8050415417586182 \t\tstd entropy:0.605584812503714\n",
            "Epoch: 5 \t\tvalue loss:61.15275923321756 \t\tpolicy loss:0.9476044399015019 \t\tavg entropy:0.8069204208643732 \t\tstd entropy:0.6307539752200404\n",
            "Epoch: 6 \t\tvalue loss:86.57384762067473 \t\tpolicy loss:0.9781872743970892 \t\tavg entropy:0.8771612165209531 \t\tstd entropy:0.6738709455379022\n",
            "Epoch: 7 \t\tvalue loss:97.12468599469474 \t\tpolicy loss:0.9576261070337189 \t\tavg entropy:0.891390619944309 \t\tstd entropy:0.6679135022315557\n",
            "Epoch: 8 \t\tvalue loss:64.47968327061514 \t\tpolicy loss:0.825090747870756 \t\tavg entropy:0.8697499389117417 \t\tstd entropy:0.6444511763248675\n",
            "Epoch: 9 \t\tvalue loss:57.10836933703905 \t\tpolicy loss:0.9893744232949246 \t\tavg entropy:0.8651587850332456 \t\tstd entropy:0.6476956087798645\n",
            "Epoch: 10 \t\tvalue loss:71.83168092470491 \t\tpolicy loss:0.9392624972911363 \t\tavg entropy:0.8965160678854561 \t\tstd entropy:0.6734463449923603\n",
            "Episode 142 finished after 200 timesteps - cumulative reward = -233.85876711387164\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:82.81154576848063 \t\tpolicy loss:0.9878471828578563 \t\tavg entropy:0.9099810954514289 \t\tstd entropy:0.6750353416148153\n",
            "Epoch: 2 \t\tvalue loss:87.60611459646333 \t\tpolicy loss:0.952872609824277 \t\tavg entropy:0.9483126399811866 \t\tstd entropy:0.6563498234492878\n",
            "Epoch: 3 \t\tvalue loss:61.515314595083176 \t\tpolicy loss:0.7957327255372251 \t\tavg entropy:0.8819836678519656 \t\tstd entropy:0.6691420617422527\n",
            "Epoch: 4 \t\tvalue loss:60.782892090550966 \t\tpolicy loss:0.9713735563701458 \t\tavg entropy:0.8749282806426157 \t\tstd entropy:0.6803118282701984\n",
            "Epoch: 5 \t\tvalue loss:75.09261747960294 \t\tpolicy loss:0.8730287404542558 \t\tavg entropy:0.9138076679266063 \t\tstd entropy:0.6900529856124766\n",
            "Epoch: 6 \t\tvalue loss:72.77197947662867 \t\tpolicy loss:0.9145341984341654 \t\tavg entropy:0.8769911998860934 \t\tstd entropy:0.6542948920275398\n",
            "Epoch: 7 \t\tvalue loss:70.6889607879553 \t\tpolicy loss:0.8544617442602522 \t\tavg entropy:0.9040551654980028 \t\tstd entropy:0.6353056655265744\n",
            "Epoch: 8 \t\tvalue loss:59.49201873179232 \t\tpolicy loss:0.7973284118630913 \t\tavg entropy:0.818869847180289 \t\tstd entropy:0.6555291394502265\n",
            "Epoch: 9 \t\tvalue loss:75.86999818180384 \t\tpolicy loss:0.9165734775950399 \t\tavg entropy:0.839846378762583 \t\tstd entropy:0.6773524084195929\n",
            "Epoch: 10 \t\tvalue loss:95.00707420606291 \t\tpolicy loss:0.9222803480839461 \t\tavg entropy:0.8840730382311837 \t\tstd entropy:0.6925490127577868\n",
            "Episode 143 finished after 200 timesteps - cumulative reward = -3.5071504539828524\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:78.0117273705729 \t\tpolicy loss:0.7998107336880116 \t\tavg entropy:0.8452262751169585 \t\tstd entropy:0.6440867823617713\n",
            "Epoch: 2 \t\tvalue loss:57.881284199403915 \t\tpolicy loss:0.7639446141344778 \t\tavg entropy:0.845910975942615 \t\tstd entropy:0.6340132766254445\n",
            "Epoch: 3 \t\tvalue loss:63.013244776243575 \t\tpolicy loss:1.0172511854868256 \t\tavg entropy:0.8309803341584995 \t\tstd entropy:0.6691685694487836\n",
            "Epoch: 4 \t\tvalue loss:100.1389521480946 \t\tpolicy loss:0.9572145409798354 \t\tavg entropy:0.9042035001274937 \t\tstd entropy:0.7041143790319699\n",
            "Epoch: 5 \t\tvalue loss:112.4311323273048 \t\tpolicy loss:0.9850914083170087 \t\tavg entropy:0.8411758492085544 \t\tstd entropy:0.6785980925707634\n",
            "Epoch: 6 \t\tvalue loss:88.34003675653693 \t\tpolicy loss:0.9733707703901141 \t\tavg entropy:0.8668545639051202 \t\tstd entropy:0.6270696441844065\n",
            "Epoch: 7 \t\tvalue loss:80.13733853115124 \t\tpolicy loss:1.0885453505462475 \t\tavg entropy:0.9537960637086422 \t\tstd entropy:0.6397182632982683\n",
            "Epoch: 8 \t\tvalue loss:91.41145899858367 \t\tpolicy loss:1.035553715871961 \t\tavg entropy:0.966527940684743 \t\tstd entropy:0.6750880097260561\n",
            "Epoch: 9 \t\tvalue loss:87.68453662315112 \t\tpolicy loss:1.0219494633460313 \t\tavg entropy:0.9952809000625443 \t\tstd entropy:0.6827171572238978\n",
            "Epoch: 10 \t\tvalue loss:72.48399002632398 \t\tpolicy loss:0.9001243335477421 \t\tavg entropy:0.94370059088477 \t\tstd entropy:0.6807905939216916\n",
            "Episode 144 finished after 200 timesteps - cumulative reward = -2.8675777289804634\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:68.12532319915428 \t\tpolicy loss:0.8013350267088815 \t\tavg entropy:0.9126251469451131 \t\tstd entropy:0.6587489512637571\n",
            "Epoch: 2 \t\tvalue loss:64.43835012564499 \t\tpolicy loss:0.8522365595517534 \t\tavg entropy:0.8897658191332279 \t\tstd entropy:0.6678854573571961\n",
            "Epoch: 3 \t\tvalue loss:63.25715160369873 \t\tpolicy loss:0.7458125145247813 \t\tavg entropy:0.8145275740087735 \t\tstd entropy:0.6818084399140333\n",
            "Epoch: 4 \t\tvalue loss:82.96545968430766 \t\tpolicy loss:0.9901147036070235 \t\tavg entropy:0.8016892525278806 \t\tstd entropy:0.6929759336970717\n",
            "Epoch: 5 \t\tvalue loss:114.51494969142956 \t\tpolicy loss:1.0175281642528062 \t\tavg entropy:0.8685468544206728 \t\tstd entropy:0.7054657127605053\n",
            "Epoch: 6 \t\tvalue loss:131.58912697266996 \t\tpolicy loss:0.989150037591377 \t\tavg entropy:0.8598929747383632 \t\tstd entropy:0.7008371633596511\n",
            "Epoch: 7 \t\tvalue loss:123.75678533918402 \t\tpolicy loss:0.9250428870822607 \t\tavg entropy:0.8469556075035793 \t\tstd entropy:0.6580684153474438\n",
            "Epoch: 8 \t\tvalue loss:180.79961709226114 \t\tpolicy loss:1.5493531649032335 \t\tavg entropy:0.950401653753175 \t\tstd entropy:0.6499450285136209\n",
            "Epoch: 9 \t\tvalue loss:218.95787485529868 \t\tpolicy loss:1.2475973702548595 \t\tavg entropy:1.1071817488479225 \t\tstd entropy:0.678304237121424\n",
            "Epoch: 10 \t\tvalue loss:180.2135450598899 \t\tpolicy loss:1.2302423186516493 \t\tavg entropy:1.0816076716610024 \t\tstd entropy:0.6848834725693946\n",
            "Episode 145 finished after 200 timesteps - cumulative reward = -118.40610367846489\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:144.00856766170926 \t\tpolicy loss:1.1134491595957015 \t\tavg entropy:1.0674532787790136 \t\tstd entropy:0.6856366844562943\n",
            "Epoch: 2 \t\tvalue loss:124.14704228507148 \t\tpolicy loss:0.8553945051299201 \t\tavg entropy:1.0317480883875396 \t\tstd entropy:0.6242741780452592\n",
            "Epoch: 3 \t\tvalue loss:130.70908244450888 \t\tpolicy loss:1.0786873678366342 \t\tavg entropy:0.979036046074956 \t\tstd entropy:0.5913084192242225\n",
            "Epoch: 4 \t\tvalue loss:172.27660154766505 \t\tpolicy loss:1.0869328690899742 \t\tavg entropy:1.0308022378570771 \t\tstd entropy:0.6357658105113057\n",
            "Epoch: 5 \t\tvalue loss:190.43807248009577 \t\tpolicy loss:1.165869563817978 \t\tavg entropy:1.0998856327916384 \t\tstd entropy:0.6617951372857741\n",
            "Epoch: 6 \t\tvalue loss:148.6421562830607 \t\tpolicy loss:0.9471895317236583 \t\tavg entropy:1.0687154865864783 \t\tstd entropy:0.6627428543576002\n",
            "Epoch: 7 \t\tvalue loss:93.35837145911323 \t\tpolicy loss:0.994440641005834 \t\tavg entropy:1.0269664468902777 \t\tstd entropy:0.6372672309605554\n",
            "Epoch: 8 \t\tvalue loss:69.75041226810879 \t\tpolicy loss:0.8270121819443172 \t\tavg entropy:1.0261421560028448 \t\tstd entropy:0.6182678225102027\n",
            "Epoch: 9 \t\tvalue loss:55.14515405495961 \t\tpolicy loss:0.754783515797721 \t\tavg entropy:0.9320968770185791 \t\tstd entropy:0.5895855272885655\n",
            "Epoch: 10 \t\tvalue loss:49.95930607583788 \t\tpolicy loss:0.8136631204022302 \t\tavg entropy:0.8490295683066095 \t\tstd entropy:0.5686865388491433\n",
            "Episode 146 finished after 200 timesteps - cumulative reward = -125.28701393239878\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:54.816641616821286 \t\tpolicy loss:0.8224862078825633 \t\tavg entropy:0.8460111926223112 \t\tstd entropy:0.5963475876344771\n",
            "Epoch: 2 \t\tvalue loss:68.94061851236555 \t\tpolicy loss:0.7709576679600609 \t\tavg entropy:0.8395842503868888 \t\tstd entropy:0.6504177698476747\n",
            "Epoch: 3 \t\tvalue loss:87.37728610038758 \t\tpolicy loss:0.7032155901193619 \t\tavg entropy:0.7702805617178006 \t\tstd entropy:0.6586370671073316\n",
            "Epoch: 4 \t\tvalue loss:99.36124041875203 \t\tpolicy loss:0.716503350271119 \t\tavg entropy:0.7087915008836129 \t\tstd entropy:0.6463933067343273\n",
            "Epoch: 5 \t\tvalue loss:92.56854277716742 \t\tpolicy loss:0.7140676723586188 \t\tavg entropy:0.7042730767591701 \t\tstd entropy:0.6342722367852922\n",
            "Epoch: 6 \t\tvalue loss:80.77333196004231 \t\tpolicy loss:0.6245431154966354 \t\tavg entropy:0.6821755947408291 \t\tstd entropy:0.6054163687652692\n",
            "Epoch: 7 \t\tvalue loss:80.79880698786842 \t\tpolicy loss:0.728090986278322 \t\tavg entropy:0.7072723391986045 \t\tstd entropy:0.5836149451271803\n",
            "Epoch: 8 \t\tvalue loss:87.13612286249797 \t\tpolicy loss:0.6564414385292265 \t\tavg entropy:0.6975900117060572 \t\tstd entropy:0.5896321763158392\n",
            "Epoch: 9 \t\tvalue loss:84.17132758564419 \t\tpolicy loss:0.7901161978642146 \t\tavg entropy:0.7298155588251899 \t\tstd entropy:0.599504516796544\n",
            "Epoch: 10 \t\tvalue loss:71.40471266640557 \t\tpolicy loss:0.7766160200039546 \t\tavg entropy:0.7586256686958475 \t\tstd entropy:0.6149224102388616\n",
            "Episode 147 finished after 200 timesteps - cumulative reward = -119.53446787329482\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:55.923919916152954 \t\tpolicy loss:0.7928248580959109 \t\tavg entropy:0.774742062554321 \t\tstd entropy:0.6214235924210313\n",
            "Epoch: 2 \t\tvalue loss:51.161729009946185 \t\tpolicy loss:0.7289685385094749 \t\tavg entropy:0.8058644269374873 \t\tstd entropy:0.6172847684433824\n",
            "Epoch: 3 \t\tvalue loss:51.427939860026044 \t\tpolicy loss:0.6858477671941121 \t\tavg entropy:0.7603058406333393 \t\tstd entropy:0.6035551720548388\n",
            "Epoch: 4 \t\tvalue loss:55.6885798242357 \t\tpolicy loss:0.6564580672317081 \t\tavg entropy:0.7224762053563638 \t\tstd entropy:0.5973971424810334\n",
            "Epoch: 5 \t\tvalue loss:57.67431604862213 \t\tpolicy loss:0.7252870625919766 \t\tavg entropy:0.7108932659050501 \t\tstd entropy:0.5930720354639434\n",
            "Epoch: 6 \t\tvalue loss:57.04717752403683 \t\tpolicy loss:0.8559151748816173 \t\tavg entropy:0.7632903720263133 \t\tstd entropy:0.6148933274982487\n",
            "Epoch: 7 \t\tvalue loss:54.867681683434384 \t\tpolicy loss:0.7649377604325612 \t\tavg entropy:0.7942693142135967 \t\tstd entropy:0.6274441020219397\n",
            "Epoch: 8 \t\tvalue loss:48.86483286751641 \t\tpolicy loss:0.637795384393798 \t\tavg entropy:0.7593138883339277 \t\tstd entropy:0.6269065923466604\n",
            "Epoch: 9 \t\tvalue loss:46.181626394059926 \t\tpolicy loss:0.6563008649481668 \t\tavg entropy:0.6984083195418587 \t\tstd entropy:0.6220401625752916\n",
            "Epoch: 10 \t\tvalue loss:48.74529700544145 \t\tpolicy loss:0.5821692834297816 \t\tavg entropy:0.6626206331109247 \t\tstd entropy:0.6132876881454582\n",
            "Episode 148 finished after 200 timesteps - cumulative reward = -1.895387878396817\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:53.46403119828966 \t\tpolicy loss:0.5947790725363625 \t\tavg entropy:0.6193828947370612 \t\tstd entropy:0.6021588530524061\n",
            "Epoch: 2 \t\tvalue loss:61.23160790867276 \t\tpolicy loss:0.6396874838405185 \t\tavg entropy:0.5788970846107321 \t\tstd entropy:0.5942151101557496\n",
            "Epoch: 3 \t\tvalue loss:64.61998187171088 \t\tpolicy loss:0.5630276223023732 \t\tavg entropy:0.5723855750393938 \t\tstd entropy:0.5848189573247939\n",
            "Epoch: 4 \t\tvalue loss:64.70282151169248 \t\tpolicy loss:0.7173726667960485 \t\tavg entropy:0.5673911687457707 \t\tstd entropy:0.568240766503762\n",
            "Epoch: 5 \t\tvalue loss:69.76084157096015 \t\tpolicy loss:0.6683495971891615 \t\tavg entropy:0.6405942479641167 \t\tstd entropy:0.5883781769069599\n",
            "Epoch: 6 \t\tvalue loss:63.478600687450836 \t\tpolicy loss:0.6290803303321203 \t\tavg entropy:0.673231120490405 \t\tstd entropy:0.5980193675234385\n",
            "Epoch: 7 \t\tvalue loss:54.75426749653286 \t\tpolicy loss:0.6715782552957534 \t\tavg entropy:0.6646166021140518 \t\tstd entropy:0.5950968849191836\n",
            "Epoch: 8 \t\tvalue loss:57.2970930284924 \t\tpolicy loss:0.5688876718282699 \t\tavg entropy:0.6552462097326065 \t\tstd entropy:0.5869156467990858\n",
            "Epoch: 9 \t\tvalue loss:64.58555386331346 \t\tpolicy loss:0.6682445132070117 \t\tavg entropy:0.6333445462291337 \t\tstd entropy:0.5948263155243012\n",
            "Epoch: 10 \t\tvalue loss:66.59704622957442 \t\tpolicy loss:0.5763326125012503 \t\tavg entropy:0.6327873708413758 \t\tstd entropy:0.5969113280376049\n",
            "Episode 149 finished after 200 timesteps - cumulative reward = -2.0766275426823877\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:48.81412554052141 \t\tpolicy loss:0.6777560671170553 \t\tavg entropy:0.6359208309437039 \t\tstd entropy:0.5837521626640758\n",
            "Epoch: 2 \t\tvalue loss:42.09764110644658 \t\tpolicy loss:0.60341404179732 \t\tavg entropy:0.6708428042192117 \t\tstd entropy:0.5801186004826112\n",
            "Epoch: 3 \t\tvalue loss:47.23392070266936 \t\tpolicy loss:0.7187731620338228 \t\tavg entropy:0.6635224921531842 \t\tstd entropy:0.5902993705039574\n",
            "Epoch: 4 \t\tvalue loss:64.4220330370797 \t\tpolicy loss:0.7315507173538208 \t\tavg entropy:0.6763477161004634 \t\tstd entropy:0.6334930384067812\n",
            "Epoch: 5 \t\tvalue loss:64.2358170721266 \t\tpolicy loss:0.5880697045061324 \t\tavg entropy:0.6726179752156844 \t\tstd entropy:0.6301844655685986\n",
            "Epoch: 6 \t\tvalue loss:45.51154795752631 \t\tpolicy loss:0.7042062766022152 \t\tavg entropy:0.631519627686085 \t\tstd entropy:0.5793171087955693\n",
            "Epoch: 7 \t\tvalue loss:50.656832814216614 \t\tpolicy loss:0.7118434260288874 \t\tavg entropy:0.7217332591308981 \t\tstd entropy:0.621611272320255\n",
            "Epoch: 8 \t\tvalue loss:57.92379624843598 \t\tpolicy loss:0.6647979597250621 \t\tavg entropy:0.6809147190181106 \t\tstd entropy:0.6267524061211965\n",
            "Epoch: 9 \t\tvalue loss:60.88920355372959 \t\tpolicy loss:0.6126687048210038 \t\tavg entropy:0.6263038283148388 \t\tstd entropy:0.6151473499901642\n",
            "Epoch: 10 \t\tvalue loss:50.89515076743232 \t\tpolicy loss:0.6558673514260186 \t\tavg entropy:0.6275507876150302 \t\tstd entropy:0.6069234994229334\n",
            "Episode 150 finished after 200 timesteps - cumulative reward = -119.69907980935325\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:42.44588694205651 \t\tpolicy loss:0.6837876779692513 \t\tavg entropy:0.6597198603097751 \t\tstd entropy:0.6228760619643966\n",
            "Epoch: 2 \t\tvalue loss:60.968839477706744 \t\tpolicy loss:0.7887099257537297 \t\tavg entropy:0.6915013085459072 \t\tstd entropy:0.6551304903466629\n",
            "Epoch: 3 \t\tvalue loss:74.81795736983582 \t\tpolicy loss:0.6242782889486669 \t\tavg entropy:0.6561335779542878 \t\tstd entropy:0.6416744451392326\n",
            "Epoch: 4 \t\tvalue loss:58.40190611042819 \t\tpolicy loss:0.9884649030454866 \t\tavg entropy:0.6337073801221115 \t\tstd entropy:0.597730340215261\n",
            "Epoch: 5 \t\tvalue loss:81.88561532261608 \t\tpolicy loss:1.0972894462910328 \t\tavg entropy:0.760783112191628 \t\tstd entropy:0.6362047503389072\n",
            "Epoch: 6 \t\tvalue loss:75.9443386046441 \t\tpolicy loss:1.0838620236941747 \t\tavg entropy:0.8571070533962445 \t\tstd entropy:0.6871971692816392\n",
            "Epoch: 7 \t\tvalue loss:83.17782650413093 \t\tpolicy loss:1.0247481203341222 \t\tavg entropy:0.915874040941126 \t\tstd entropy:0.6847963525074525\n",
            "Epoch: 8 \t\tvalue loss:93.50902031280182 \t\tpolicy loss:1.070979247708897 \t\tavg entropy:0.8960108951102557 \t\tstd entropy:0.6433626692320255\n",
            "Epoch: 9 \t\tvalue loss:92.65584291730609 \t\tpolicy loss:1.2887050751801377 \t\tavg entropy:0.9568816575530276 \t\tstd entropy:0.6681783590296908\n",
            "Epoch: 10 \t\tvalue loss:109.04003721803099 \t\tpolicy loss:1.508562290078991 \t\tavg entropy:1.0682093495384142 \t\tstd entropy:0.677565969692485\n",
            "Episode 151 finished after 200 timesteps - cumulative reward = -2.4816699690717665\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:160.39740732213954 \t\tpolicy loss:1.4239428364313567 \t\tavg entropy:1.1630669729085188 \t\tstd entropy:0.6584659213358202\n",
            "Epoch: 2 \t\tvalue loss:97.6063062961285 \t\tpolicy loss:1.1975372499162025 \t\tavg entropy:1.217285742269869 \t\tstd entropy:0.6345528502029547\n",
            "Epoch: 3 \t\tvalue loss:68.11025861593393 \t\tpolicy loss:1.029148253110739 \t\tavg entropy:1.166413584670126 \t\tstd entropy:0.6199771933537467\n",
            "Epoch: 4 \t\tvalue loss:55.940165281295776 \t\tpolicy loss:1.1948117358343942 \t\tavg entropy:1.1303178751956469 \t\tstd entropy:0.6096248869088335\n",
            "Epoch: 5 \t\tvalue loss:64.90487491953503 \t\tpolicy loss:1.216719462976351 \t\tavg entropy:1.17631442833016 \t\tstd entropy:0.6335311811388452\n",
            "Epoch: 6 \t\tvalue loss:78.56588485738733 \t\tpolicy loss:0.9868222055854378 \t\tavg entropy:1.138707513169306 \t\tstd entropy:0.659155694686148\n",
            "Epoch: 7 \t\tvalue loss:77.14291466723432 \t\tpolicy loss:0.8559315695867433 \t\tavg entropy:1.0556793123054673 \t\tstd entropy:0.6692538651619143\n",
            "Epoch: 8 \t\tvalue loss:62.59009168960236 \t\tpolicy loss:0.8668092718491187 \t\tavg entropy:0.9785878493674244 \t\tstd entropy:0.6482039066038575\n",
            "Epoch: 9 \t\tvalue loss:53.26259032972566 \t\tpolicy loss:0.735704685305501 \t\tavg entropy:0.9417360298555608 \t\tstd entropy:0.6238361341915473\n",
            "Epoch: 10 \t\tvalue loss:56.03793240379501 \t\tpolicy loss:0.855376137481941 \t\tavg entropy:0.8610293296341649 \t\tstd entropy:0.6236070066984244\n",
            "Episode 152 finished after 200 timesteps - cumulative reward = -124.27274244483972\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:73.4169182410607 \t\tpolicy loss:0.9087497749171414 \t\tavg entropy:0.8686858934015329 \t\tstd entropy:0.6533244380783088\n",
            "Epoch: 2 \t\tvalue loss:88.41419970858227 \t\tpolicy loss:0.7678447031712794 \t\tavg entropy:0.8710138431102694 \t\tstd entropy:0.6696547556011694\n",
            "Epoch: 3 \t\tvalue loss:70.2855200950916 \t\tpolicy loss:0.8007199338504246 \t\tavg entropy:0.8187060174687866 \t\tstd entropy:0.6398211355288621\n",
            "Epoch: 4 \t\tvalue loss:57.862582754302814 \t\tpolicy loss:0.7864897899575286 \t\tavg entropy:0.8096642557905204 \t\tstd entropy:0.6270307843152767\n",
            "Epoch: 5 \t\tvalue loss:65.40474569142519 \t\tpolicy loss:0.8671618012281564 \t\tavg entropy:0.841760115685101 \t\tstd entropy:0.6540973620216967\n",
            "Epoch: 6 \t\tvalue loss:78.73377670036568 \t\tpolicy loss:0.7628918922209478 \t\tavg entropy:0.84815633584084 \t\tstd entropy:0.6616656651248829\n",
            "Epoch: 7 \t\tvalue loss:63.40311415378864 \t\tpolicy loss:0.719379301909562 \t\tavg entropy:0.7800559431495092 \t\tstd entropy:0.6302339236743393\n",
            "Epoch: 8 \t\tvalue loss:52.85012277666029 \t\tpolicy loss:0.8377640836841458 \t\tavg entropy:0.7681702733178186 \t\tstd entropy:0.6326032506160569\n",
            "Epoch: 9 \t\tvalue loss:62.73874632342831 \t\tpolicy loss:0.7920606113397158 \t\tavg entropy:0.832319810037024 \t\tstd entropy:0.6652967377335692\n",
            "Epoch: 10 \t\tvalue loss:69.21136000130203 \t\tpolicy loss:0.690424369259195 \t\tavg entropy:0.8001535591634874 \t\tstd entropy:0.6599143302023363\n",
            "Episode 153 finished after 200 timesteps - cumulative reward = -126.41261293068793\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:49.88585078847277 \t\tpolicy loss:0.8237765532928508 \t\tavg entropy:0.7387157678782621 \t\tstd entropy:0.6431089756729508\n",
            "Epoch: 2 \t\tvalue loss:49.97392426611303 \t\tpolicy loss:0.7815343383904342 \t\tavg entropy:0.7649228768745086 \t\tstd entropy:0.6661190746316848\n",
            "Epoch: 3 \t\tvalue loss:64.19683049537323 \t\tpolicy loss:0.7146861939639836 \t\tavg entropy:0.8195458724170024 \t\tstd entropy:0.6835504839871105\n",
            "Epoch: 4 \t\tvalue loss:58.131641453439066 \t\tpolicy loss:0.6242810956069401 \t\tavg entropy:0.7132133017055938 \t\tstd entropy:0.6478274546885308\n",
            "Epoch: 5 \t\tvalue loss:45.76463846190945 \t\tpolicy loss:0.8018183213668865 \t\tavg entropy:0.6886199612528666 \t\tstd entropy:0.6513867512339411\n",
            "Epoch: 6 \t\tvalue loss:56.74290700011201 \t\tpolicy loss:0.7062077844863409 \t\tavg entropy:0.7853878590165809 \t\tstd entropy:0.6843476677335183\n",
            "Epoch: 7 \t\tvalue loss:64.78880862875299 \t\tpolicy loss:0.605122195495354 \t\tavg entropy:0.7171681125250674 \t\tstd entropy:0.6556795319198093\n",
            "Epoch: 8 \t\tvalue loss:45.024052640893956 \t\tpolicy loss:0.8446442753404051 \t\tavg entropy:0.6832818265652149 \t\tstd entropy:0.6465926118991467\n",
            "Epoch: 9 \t\tvalue loss:54.39655908123478 \t\tpolicy loss:0.809611615735096 \t\tavg entropy:0.7689848801015541 \t\tstd entropy:0.682174697442939\n",
            "Epoch: 10 \t\tvalue loss:76.6992718413636 \t\tpolicy loss:0.8204087431614215 \t\tavg entropy:0.8257422516890381 \t\tstd entropy:0.6813944650261511\n",
            "Episode 154 finished after 200 timesteps - cumulative reward = -123.16403491379087\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:49.425745947020395 \t\tpolicy loss:0.9011982253619603 \t\tavg entropy:0.7636493333003574 \t\tstd entropy:0.6539521415626096\n",
            "Epoch: 2 \t\tvalue loss:53.767546909196035 \t\tpolicy loss:0.9005802561948587 \t\tavg entropy:0.8137737986754845 \t\tstd entropy:0.6900041553043607\n",
            "Epoch: 3 \t\tvalue loss:79.96444453773918 \t\tpolicy loss:0.8990843093002235 \t\tavg entropy:0.9079398510871227 \t\tstd entropy:0.6975229296912876\n",
            "Epoch: 4 \t\tvalue loss:59.292053599934 \t\tpolicy loss:0.8220840158698323 \t\tavg entropy:0.8231802279970694 \t\tstd entropy:0.6645754921781907\n",
            "Epoch: 5 \t\tvalue loss:54.39633365777823 \t\tpolicy loss:0.9813014281975044 \t\tavg entropy:0.841901142169841 \t\tstd entropy:0.6725775815089348\n",
            "Epoch: 6 \t\tvalue loss:80.26741832691235 \t\tpolicy loss:0.9458442461359632 \t\tavg entropy:0.9568959450732896 \t\tstd entropy:0.6893337321118275\n",
            "Epoch: 7 \t\tvalue loss:76.8667665733086 \t\tpolicy loss:0.7875360188248394 \t\tavg entropy:0.8683238304555058 \t\tstd entropy:0.6716496680116677\n",
            "Epoch: 8 \t\tvalue loss:57.96196578361176 \t\tpolicy loss:1.051032039162877 \t\tavg entropy:0.8927721892993163 \t\tstd entropy:0.6403521217750564\n",
            "Epoch: 9 \t\tvalue loss:72.92088259707441 \t\tpolicy loss:1.0803035972537576 \t\tavg entropy:0.9640072739996498 \t\tstd entropy:0.6659414407561457\n",
            "Epoch: 10 \t\tvalue loss:90.53958024035444 \t\tpolicy loss:1.0081840354007678 \t\tavg entropy:1.0351974002680624 \t\tstd entropy:0.6736151143787107\n",
            "Episode 155 finished after 200 timesteps - cumulative reward = -126.78625230480651\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:64.64865369382112 \t\tpolicy loss:1.0223626915527426 \t\tavg entropy:0.9929273082581609 \t\tstd entropy:0.6412775766760769\n",
            "Epoch: 2 \t\tvalue loss:51.1315596440564 \t\tpolicy loss:0.9353642327630002 \t\tavg entropy:0.9841622013364701 \t\tstd entropy:0.6342321949971021\n",
            "Epoch: 3 \t\tvalue loss:61.02960226846778 \t\tpolicy loss:0.9667667736825736 \t\tavg entropy:0.9586144514254135 \t\tstd entropy:0.6535971911873343\n",
            "Epoch: 4 \t\tvalue loss:80.21013151562732 \t\tpolicy loss:0.9670385951581209 \t\tavg entropy:0.9816263794537551 \t\tstd entropy:0.6770606775512243\n",
            "Epoch: 5 \t\tvalue loss:74.04071546119192 \t\tpolicy loss:0.8654855962680734 \t\tavg entropy:0.9468917027038901 \t\tstd entropy:0.6746766398697599\n",
            "Epoch: 6 \t\tvalue loss:59.0442199823649 \t\tpolicy loss:0.9573470716891082 \t\tavg entropy:0.9389014829237018 \t\tstd entropy:0.664733332284284\n",
            "Epoch: 7 \t\tvalue loss:61.98512741016305 \t\tpolicy loss:0.9845482609842134 \t\tavg entropy:0.9529999984390477 \t\tstd entropy:0.6617558406682718\n",
            "Epoch: 8 \t\tvalue loss:64.57403439542522 \t\tpolicy loss:0.8772888164157453 \t\tavg entropy:0.9188469510088256 \t\tstd entropy:0.6632650219422749\n",
            "Epoch: 9 \t\tvalue loss:55.23260382983995 \t\tpolicy loss:0.8964065436435782 \t\tavg entropy:0.8882812758733531 \t\tstd entropy:0.6522121444197299\n",
            "Epoch: 10 \t\tvalue loss:52.4811572847159 \t\tpolicy loss:0.8349318734329679 \t\tavg entropy:0.8928560307267227 \t\tstd entropy:0.6512737630298082\n",
            "Episode 156 finished after 200 timesteps - cumulative reward = -234.76415065452986\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:52.61784499235775 \t\tpolicy loss:0.9577290746180908 \t\tavg entropy:0.8800874220841988 \t\tstd entropy:0.6711816668328219\n",
            "Epoch: 2 \t\tvalue loss:65.4932778970055 \t\tpolicy loss:0.8404415411793668 \t\tavg entropy:0.905025351381687 \t\tstd entropy:0.6880203146441248\n",
            "Epoch: 3 \t\tvalue loss:68.59602015951405 \t\tpolicy loss:0.8394793187794478 \t\tavg entropy:0.8330165531706067 \t\tstd entropy:0.6748680672344226\n",
            "Epoch: 4 \t\tvalue loss:65.41462915876637 \t\tpolicy loss:0.9396321196918902 \t\tavg entropy:0.857074001180649 \t\tstd entropy:0.6723765584728439\n",
            "Epoch: 5 \t\tvalue loss:62.412153291961424 \t\tpolicy loss:0.9208479070145151 \t\tavg entropy:0.8879243899328794 \t\tstd entropy:0.7026373342342845\n",
            "Epoch: 6 \t\tvalue loss:56.03456296091495 \t\tpolicy loss:0.8991834064540656 \t\tavg entropy:0.8915688349872717 \t\tstd entropy:0.7204290138703561\n",
            "Epoch: 7 \t\tvalue loss:45.24156991036042 \t\tpolicy loss:0.8958934749598089 \t\tavg entropy:0.9032001592690969 \t\tstd entropy:0.6900374840847623\n",
            "Epoch: 8 \t\tvalue loss:47.6463280112847 \t\tpolicy loss:0.8762923349504885 \t\tavg entropy:0.9058037611054489 \t\tstd entropy:0.6719272100912241\n",
            "Epoch: 9 \t\tvalue loss:62.72646442444428 \t\tpolicy loss:0.8378177816453187 \t\tavg entropy:0.8629791517843056 \t\tstd entropy:0.6878799530855831\n",
            "Epoch: 10 \t\tvalue loss:68.34708483841108 \t\tpolicy loss:0.6979634703501411 \t\tavg entropy:0.8054759317381129 \t\tstd entropy:0.687454490868135\n",
            "Episode 157 finished after 200 timesteps - cumulative reward = -116.37161679604691\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:55.227219252482705 \t\tpolicy loss:0.831107304148052 \t\tavg entropy:0.8014378910314125 \t\tstd entropy:0.6827871353156305\n",
            "Epoch: 2 \t\tvalue loss:43.19178286583527 \t\tpolicy loss:0.8621085062623024 \t\tavg entropy:0.82633864694691 \t\tstd entropy:0.6443475481283556\n",
            "Epoch: 3 \t\tvalue loss:56.92056496117426 \t\tpolicy loss:0.9572955703605777 \t\tavg entropy:0.880229639160144 \t\tstd entropy:0.6791046238791499\n",
            "Epoch: 4 \t\tvalue loss:72.31911004107931 \t\tpolicy loss:0.7706736267908759 \t\tavg entropy:0.8687236039677063 \t\tstd entropy:0.7015115893197568\n",
            "Epoch: 5 \t\tvalue loss:62.95852078043896 \t\tpolicy loss:0.8120557209071906 \t\tavg entropy:0.8153597392487062 \t\tstd entropy:0.6829713856184033\n",
            "Epoch: 6 \t\tvalue loss:38.97303024582241 \t\tpolicy loss:1.0037657463032266 \t\tavg entropy:0.8517015531967627 \t\tstd entropy:0.65019334013723\n",
            "Epoch: 7 \t\tvalue loss:43.90999904534091 \t\tpolicy loss:0.9908072822119879 \t\tavg entropy:0.9612336167754835 \t\tstd entropy:0.6787187564493069\n",
            "Epoch: 8 \t\tvalue loss:61.59695043641588 \t\tpolicy loss:0.8261812562527864 \t\tavg entropy:0.9225464745495942 \t\tstd entropy:0.7162998151433884\n",
            "Epoch: 9 \t\tvalue loss:68.99752427702364 \t\tpolicy loss:0.7333157846461171 \t\tavg entropy:0.8299998011562961 \t\tstd entropy:0.7084353564729636\n",
            "Epoch: 10 \t\tvalue loss:49.40005427598953 \t\tpolicy loss:0.868578653620637 \t\tavg entropy:0.8149240063726497 \t\tstd entropy:0.6834145500657425\n",
            "Episode 158 finished after 200 timesteps - cumulative reward = -345.99158117524\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:41.47563282821489 \t\tpolicy loss:0.8362137668806574 \t\tavg entropy:0.8664477751723048 \t\tstd entropy:0.658098255308878\n",
            "Epoch: 2 \t\tvalue loss:50.446713726157725 \t\tpolicy loss:0.9207854662900385 \t\tavg entropy:0.9083724266081105 \t\tstd entropy:0.6955416526479495\n",
            "Epoch: 3 \t\tvalue loss:62.99167532765347 \t\tpolicy loss:0.7539875057080517 \t\tavg entropy:0.8581412367243444 \t\tstd entropy:0.7101455566429269\n",
            "Epoch: 4 \t\tvalue loss:56.48276773224706 \t\tpolicy loss:0.7673361427758051 \t\tavg entropy:0.7990023874049968 \t\tstd entropy:0.7076994016276824\n",
            "Epoch: 5 \t\tvalue loss:36.786100552133895 \t\tpolicy loss:1.0150717020682667 \t\tavg entropy:0.8452815340478147 \t\tstd entropy:0.6876124837616074\n",
            "Epoch: 6 \t\tvalue loss:43.937119656282924 \t\tpolicy loss:0.9808660447597504 \t\tavg entropy:0.9533598818408372 \t\tstd entropy:0.7048266298295162\n",
            "Epoch: 7 \t\tvalue loss:59.411184056945466 \t\tpolicy loss:0.7978231278450593 \t\tavg entropy:0.9049236420661669 \t\tstd entropy:0.7268518565727744\n",
            "Epoch: 8 \t\tvalue loss:57.60200457210126 \t\tpolicy loss:0.7996450377547223 \t\tavg entropy:0.8248098394736209 \t\tstd entropy:0.7314275450902059\n",
            "Epoch: 9 \t\tvalue loss:42.897579102412514 \t\tpolicy loss:0.8604913558001104 \t\tavg entropy:0.8428884215901644 \t\tstd entropy:0.7221035071810653\n",
            "Epoch: 10 \t\tvalue loss:42.01021914378457 \t\tpolicy loss:0.8597806708320327 \t\tavg entropy:0.8929156488978627 \t\tstd entropy:0.7255721693688651\n",
            "Episode 159 finished after 200 timesteps - cumulative reward = -235.6594066523391\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:51.074276238679886 \t\tpolicy loss:0.7888630502249884 \t\tavg entropy:0.8889977235446564 \t\tstd entropy:0.7435204851883799\n",
            "Epoch: 2 \t\tvalue loss:54.75665554015533 \t\tpolicy loss:0.7600166059058645 \t\tavg entropy:0.8273187358143259 \t\tstd entropy:0.7476227714598846\n",
            "Epoch: 3 \t\tvalue loss:35.889843760625176 \t\tpolicy loss:0.9780049061645633 \t\tavg entropy:0.8376014824217929 \t\tstd entropy:0.7414050994867782\n",
            "Epoch: 4 \t\tvalue loss:45.41812676709631 \t\tpolicy loss:0.894492221267327 \t\tavg entropy:0.9269220520333832 \t\tstd entropy:0.7529513457230398\n",
            "Epoch: 5 \t\tvalue loss:51.531452554723494 \t\tpolicy loss:0.8793937612486922 \t\tavg entropy:0.8591588151720336 \t\tstd entropy:0.7517898244293703\n",
            "Epoch: 6 \t\tvalue loss:50.508366944997206 \t\tpolicy loss:0.8377529687207678 \t\tavg entropy:0.8553056716542421 \t\tstd entropy:0.768794557038609\n",
            "Epoch: 7 \t\tvalue loss:43.800402120403625 \t\tpolicy loss:0.8360402169434921 \t\tavg entropy:0.8556191203973235 \t\tstd entropy:0.7622529449884745\n",
            "Epoch: 8 \t\tvalue loss:44.356561862904094 \t\tpolicy loss:0.8699238420180653 \t\tavg entropy:0.8844486643043431 \t\tstd entropy:0.7680251266440399\n",
            "Epoch: 9 \t\tvalue loss:61.27769384176835 \t\tpolicy loss:0.8576932789190955 \t\tavg entropy:0.8943740610787847 \t\tstd entropy:0.7681835501981272\n",
            "Epoch: 10 \t\tvalue loss:38.260174227797464 \t\tpolicy loss:1.0135408526529437 \t\tavg entropy:0.8592392754178328 \t\tstd entropy:0.7538963103253257\n",
            "Episode 160 finished after 200 timesteps - cumulative reward = -119.67076127515921\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:42.63324254302568 \t\tpolicy loss:1.00692512976226 \t\tavg entropy:0.9693297257944109 \t\tstd entropy:0.7597387852781071\n",
            "Epoch: 2 \t\tvalue loss:59.28551516737989 \t\tpolicy loss:0.8827721893146474 \t\tavg entropy:0.9483810126317072 \t\tstd entropy:0.7701543467095102\n",
            "Epoch: 3 \t\tvalue loss:54.20556992100131 \t\tpolicy loss:0.9285724006032431 \t\tavg entropy:0.8689774116409797 \t\tstd entropy:0.7822352743738397\n",
            "Epoch: 4 \t\tvalue loss:46.39634624988802 \t\tpolicy loss:0.9156182725583354 \t\tavg entropy:0.8980506179565584 \t\tstd entropy:0.772791765734266\n",
            "Epoch: 5 \t\tvalue loss:42.50088124890481 \t\tpolicy loss:0.9808028340339661 \t\tavg entropy:0.9047404885932521 \t\tstd entropy:0.7755988090948084\n",
            "Epoch: 6 \t\tvalue loss:59.408276847613756 \t\tpolicy loss:1.1607834498087566 \t\tavg entropy:1.012660288416165 \t\tstd entropy:0.7710348827855071\n",
            "Epoch: 7 \t\tvalue loss:77.53947445141372 \t\tpolicy loss:1.0111152484852781 \t\tavg entropy:1.0174789367219013 \t\tstd entropy:0.7431334012752877\n",
            "Epoch: 8 \t\tvalue loss:39.084351539611816 \t\tpolicy loss:1.206645550586844 \t\tavg entropy:0.9469543403369752 \t\tstd entropy:0.758461176137589\n",
            "Epoch: 9 \t\tvalue loss:40.945139790094025 \t\tpolicy loss:1.1343607069343649 \t\tavg entropy:1.061362317804739 \t\tstd entropy:0.7725381532452531\n",
            "Epoch: 10 \t\tvalue loss:67.34459414020661 \t\tpolicy loss:1.0045106737844405 \t\tavg entropy:1.0357512424769142 \t\tstd entropy:0.7662060337460518\n",
            "Episode 161 finished after 200 timesteps - cumulative reward = -236.1310168070054\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:98.84481757174257 \t\tpolicy loss:0.9412708759948771 \t\tavg entropy:0.9078608201882823 \t\tstd entropy:0.7513364534296187\n",
            "Epoch: 2 \t\tvalue loss:73.33964826214698 \t\tpolicy loss:1.3112615237953842 \t\tavg entropy:0.9220625718100902 \t\tstd entropy:0.7032724624288978\n",
            "Epoch: 3 \t\tvalue loss:89.41798488555416 \t\tpolicy loss:1.2781186405048575 \t\tavg entropy:1.0413299288847624 \t\tstd entropy:0.690397531133392\n",
            "Epoch: 4 \t\tvalue loss:87.3667537525136 \t\tpolicy loss:1.0259284460416405 \t\tavg entropy:1.0575572631493748 \t\tstd entropy:0.706951900990399\n",
            "Epoch: 5 \t\tvalue loss:54.65168147958735 \t\tpolicy loss:1.1488951931717575 \t\tavg entropy:1.0418563810802515 \t\tstd entropy:0.7208027894469159\n",
            "Epoch: 6 \t\tvalue loss:39.6976247320893 \t\tpolicy loss:1.1054458723914238 \t\tavg entropy:1.059711056406581 \t\tstd entropy:0.7249479722846369\n",
            "Epoch: 7 \t\tvalue loss:48.860595468551885 \t\tpolicy loss:1.2181680420393586 \t\tavg entropy:1.1680899405786243 \t\tstd entropy:0.7373483660044642\n",
            "Epoch: 8 \t\tvalue loss:65.31323209629264 \t\tpolicy loss:0.9707337380737386 \t\tavg entropy:1.1580565240483724 \t\tstd entropy:0.7513837422202035\n",
            "Epoch: 9 \t\tvalue loss:61.014669123516285 \t\tpolicy loss:0.8893943377720412 \t\tavg entropy:1.0209863242046913 \t\tstd entropy:0.7535429047378875\n",
            "Epoch: 10 \t\tvalue loss:46.697354152638425 \t\tpolicy loss:0.9374775200761775 \t\tavg entropy:0.9644633727744999 \t\tstd entropy:0.7381566717767243\n",
            "Episode 162 finished after 200 timesteps - cumulative reward = -238.91125356872553\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:42.3200556847357 \t\tpolicy loss:0.852399879565803 \t\tavg entropy:0.9262921895886609 \t\tstd entropy:0.7412926341919513\n",
            "Epoch: 2 \t\tvalue loss:44.83190477919835 \t\tpolicy loss:0.83305827840682 \t\tavg entropy:0.9175088129423278 \t\tstd entropy:0.7383944439576828\n",
            "Epoch: 3 \t\tvalue loss:54.60675214823856 \t\tpolicy loss:0.8423133184832912 \t\tavg entropy:0.9225737882052704 \t\tstd entropy:0.7450310337417122\n",
            "Epoch: 4 \t\tvalue loss:56.13590154852918 \t\tpolicy loss:0.8754787646955059 \t\tavg entropy:0.8704950443151523 \t\tstd entropy:0.7442835360081809\n",
            "Epoch: 5 \t\tvalue loss:39.34900919596354 \t\tpolicy loss:0.8341629409020946 \t\tavg entropy:0.8613215779900271 \t\tstd entropy:0.732267731221943\n",
            "Epoch: 6 \t\tvalue loss:36.29188080885077 \t\tpolicy loss:0.7602985395539191 \t\tavg entropy:0.8847712233478597 \t\tstd entropy:0.7411980060353006\n",
            "Epoch: 7 \t\tvalue loss:44.9330556854125 \t\tpolicy loss:0.7633116235656123 \t\tavg entropy:0.8744445327201646 \t\tstd entropy:0.7401690267096647\n",
            "Epoch: 8 \t\tvalue loss:46.74321132577876 \t\tpolicy loss:0.791314160631549 \t\tavg entropy:0.8183944310151491 \t\tstd entropy:0.7422732218552106\n",
            "Epoch: 9 \t\tvalue loss:36.741621072574326 \t\tpolicy loss:0.8152235908213482 \t\tavg entropy:0.796487802119156 \t\tstd entropy:0.7406150258473859\n",
            "Epoch: 10 \t\tvalue loss:40.361002100411284 \t\tpolicy loss:0.7901444412687774 \t\tavg entropy:0.8372163352109607 \t\tstd entropy:0.7475941802216971\n",
            "Episode 163 finished after 200 timesteps - cumulative reward = -118.15383263595147\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:47.22332034572478 \t\tpolicy loss:0.7788034709551002 \t\tavg entropy:0.8117943079109285 \t\tstd entropy:0.7453315388917371\n",
            "Epoch: 2 \t\tvalue loss:39.042919353772234 \t\tpolicy loss:0.894456462834471 \t\tavg entropy:0.8070721410596926 \t\tstd entropy:0.745754794786858\n",
            "Epoch: 3 \t\tvalue loss:39.76193784001053 \t\tpolicy loss:0.7688826806442712 \t\tavg entropy:0.8486629549446459 \t\tstd entropy:0.7427610351572264\n",
            "Epoch: 4 \t\tvalue loss:45.24110306975662 \t\tpolicy loss:0.8233171066930217 \t\tavg entropy:0.8448743972994187 \t\tstd entropy:0.7292642713811334\n",
            "Epoch: 5 \t\tvalue loss:38.696872899609225 \t\tpolicy loss:0.8669231844845638 \t\tavg entropy:0.8213068722040765 \t\tstd entropy:0.725831843900839\n",
            "Epoch: 6 \t\tvalue loss:41.28135565019423 \t\tpolicy loss:0.7932396489445881 \t\tavg entropy:0.8602726061910521 \t\tstd entropy:0.7246569850025903\n",
            "Epoch: 7 \t\tvalue loss:46.258199220062586 \t\tpolicy loss:0.9337378073764103 \t\tavg entropy:0.8926686252821476 \t\tstd entropy:0.7256555267014752\n",
            "Epoch: 8 \t\tvalue loss:36.72757105801695 \t\tpolicy loss:0.9307108774620999 \t\tavg entropy:0.8471403789995641 \t\tstd entropy:0.720222230433365\n",
            "Epoch: 9 \t\tvalue loss:43.01929948406835 \t\tpolicy loss:0.8829686859602569 \t\tavg entropy:0.9137345872803712 \t\tstd entropy:0.7469723930136264\n",
            "Epoch: 10 \t\tvalue loss:54.47587971020771 \t\tpolicy loss:1.1400250571389352 \t\tavg entropy:0.9578995005919838 \t\tstd entropy:0.7286905218213139\n",
            "Episode 164 finished after 200 timesteps - cumulative reward = -122.7103618218624\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:44.45964409459022 \t\tpolicy loss:1.0748756604809915 \t\tavg entropy:0.915459010878974 \t\tstd entropy:0.7232187226910487\n",
            "Epoch: 2 \t\tvalue loss:42.630148651779336 \t\tpolicy loss:0.9274811074938826 \t\tavg entropy:0.9587203337927912 \t\tstd entropy:0.7680916399890907\n",
            "Epoch: 3 \t\tvalue loss:51.00186065191863 \t\tpolicy loss:1.0958367143907854 \t\tavg entropy:0.9849666902438734 \t\tstd entropy:0.7578586110235878\n",
            "Epoch: 4 \t\tvalue loss:59.38977472243771 \t\tpolicy loss:0.9620581339764339 \t\tavg entropy:0.9942669807758097 \t\tstd entropy:0.758296209134114\n",
            "Epoch: 5 \t\tvalue loss:37.63207804003069 \t\tpolicy loss:1.0058915326672215 \t\tavg entropy:0.9040555726098365 \t\tstd entropy:0.7528787511861826\n",
            "Epoch: 6 \t\tvalue loss:44.173706252087825 \t\tpolicy loss:0.9692788633608049 \t\tavg entropy:0.9870805660999591 \t\tstd entropy:0.7842280603381301\n",
            "Epoch: 7 \t\tvalue loss:56.633427589170395 \t\tpolicy loss:0.9489457463064501 \t\tavg entropy:0.9867046991402267 \t\tstd entropy:0.7779580068843143\n",
            "Epoch: 8 \t\tvalue loss:57.25090027624561 \t\tpolicy loss:0.9132788520987316 \t\tavg entropy:0.9472160256235774 \t\tstd entropy:0.7568705911729361\n",
            "Epoch: 9 \t\tvalue loss:35.879299313791336 \t\tpolicy loss:0.9056028505807282 \t\tavg entropy:0.9002073281715726 \t\tstd entropy:0.7412637582715196\n",
            "Epoch: 10 \t\tvalue loss:34.17311197070665 \t\tpolicy loss:0.8512879289606566 \t\tavg entropy:0.9328818564711634 \t\tstd entropy:0.7543004729082963\n",
            "Episode 165 finished after 200 timesteps - cumulative reward = -234.24240304328254\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:48.69800535288263 \t\tpolicy loss:0.9169473793912442 \t\tavg entropy:0.9600503908304912 \t\tstd entropy:0.7679431713340318\n",
            "Epoch: 2 \t\tvalue loss:56.119369293781034 \t\tpolicy loss:0.8448422516280032 \t\tavg entropy:0.8989258397667561 \t\tstd entropy:0.7466298830448576\n",
            "Epoch: 3 \t\tvalue loss:36.992819931912926 \t\tpolicy loss:0.8684929945367448 \t\tavg entropy:0.8654863850498129 \t\tstd entropy:0.7317753729259058\n",
            "Epoch: 4 \t\tvalue loss:30.59727784927855 \t\tpolicy loss:0.778793676736507 \t\tavg entropy:0.8774967969183599 \t\tstd entropy:0.7259447850886365\n",
            "Epoch: 5 \t\tvalue loss:42.28315687686839 \t\tpolicy loss:0.8357314540350691 \t\tavg entropy:0.902334617234036 \t\tstd entropy:0.7480973063314514\n",
            "Epoch: 6 \t\tvalue loss:50.52461587114537 \t\tpolicy loss:0.8208390705763026 \t\tavg entropy:0.8543887984930421 \t\tstd entropy:0.7298192544467909\n",
            "Epoch: 7 \t\tvalue loss:36.577324744234694 \t\tpolicy loss:0.8257395523659726 \t\tavg entropy:0.815435552246296 \t\tstd entropy:0.7152818530417213\n",
            "Epoch: 8 \t\tvalue loss:31.401195453836564 \t\tpolicy loss:0.7493182959708762 \t\tavg entropy:0.8223542288013287 \t\tstd entropy:0.7037266920142025\n",
            "Epoch: 9 \t\tvalue loss:41.54833453386388 \t\tpolicy loss:0.8329711493025435 \t\tavg entropy:0.8448253172463248 \t\tstd entropy:0.7275184756582445\n",
            "Epoch: 10 \t\tvalue loss:49.936848295495864 \t\tpolicy loss:0.8407195223138687 \t\tavg entropy:0.8431439712100399 \t\tstd entropy:0.7256726256302592\n",
            "Episode 166 finished after 200 timesteps - cumulative reward = -309.84120465053235\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:34.51862822568163 \t\tpolicy loss:0.8840549670635386 \t\tavg entropy:0.8173570033367843 \t\tstd entropy:0.700846557671157\n",
            "Epoch: 2 \t\tvalue loss:35.379810149365284 \t\tpolicy loss:0.8474068857253866 \t\tavg entropy:0.8457250065288351 \t\tstd entropy:0.7225270995770104\n",
            "Epoch: 3 \t\tvalue loss:45.51459899100851 \t\tpolicy loss:0.9887313538409294 \t\tavg entropy:0.8869246976439741 \t\tstd entropy:0.734685527316806\n",
            "Epoch: 4 \t\tvalue loss:55.1522189175829 \t\tpolicy loss:0.900008147384258 \t\tavg entropy:0.8866054951565675 \t\tstd entropy:0.7570649632210991\n",
            "Epoch: 5 \t\tvalue loss:41.10293163771325 \t\tpolicy loss:1.0332354446674914 \t\tavg entropy:0.8914552005856269 \t\tstd entropy:0.7325281800558424\n",
            "Epoch: 6 \t\tvalue loss:41.73951845853887 \t\tpolicy loss:0.7777912401138468 \t\tavg entropy:0.8886210802361603 \t\tstd entropy:0.7184430843249219\n",
            "Epoch: 7 \t\tvalue loss:45.687662198188455 \t\tpolicy loss:0.9819445977819726 \t\tavg entropy:0.9062660715245257 \t\tstd entropy:0.7243674760152279\n",
            "Epoch: 8 \t\tvalue loss:41.95352822288554 \t\tpolicy loss:0.9810857354326451 \t\tavg entropy:0.8953456792993607 \t\tstd entropy:0.7297442466605291\n",
            "Epoch: 9 \t\tvalue loss:44.14345286247578 \t\tpolicy loss:0.9775227273398257 \t\tavg entropy:0.9036211095401644 \t\tstd entropy:0.7381583559463489\n",
            "Epoch: 10 \t\tvalue loss:51.33720674920589 \t\tpolicy loss:1.2871809722261225 \t\tavg entropy:1.0060376647733853 \t\tstd entropy:0.7271552750036301\n",
            "Episode 167 finished after 200 timesteps - cumulative reward = -121.86141880507002\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:59.056122191408846 \t\tpolicy loss:1.0472970820487815 \t\tavg entropy:1.0655523189587959 \t\tstd entropy:0.7398894821814095\n",
            "Epoch: 2 \t\tvalue loss:44.26134899068386 \t\tpolicy loss:1.1503215295837281 \t\tavg entropy:1.034335517756926 \t\tstd entropy:0.7429069928886392\n",
            "Epoch: 3 \t\tvalue loss:40.50767125981919 \t\tpolicy loss:0.9169959831745067 \t\tavg entropy:0.9722968341859827 \t\tstd entropy:0.7377564085567544\n",
            "Epoch: 4 \t\tvalue loss:43.9226252347865 \t\tpolicy loss:1.1888655018299183 \t\tavg entropy:1.0354369873963303 \t\tstd entropy:0.7336430406234893\n",
            "Epoch: 5 \t\tvalue loss:64.93702702319368 \t\tpolicy loss:1.063270110399165 \t\tavg entropy:1.081713342360165 \t\tstd entropy:0.735783957423802\n",
            "Epoch: 6 \t\tvalue loss:63.413569490960306 \t\tpolicy loss:1.4218771571808673 \t\tavg entropy:1.0437694777512394 \t\tstd entropy:0.7477223585387144\n",
            "Epoch: 7 \t\tvalue loss:82.52621015589288 \t\tpolicy loss:1.3701334760544148 \t\tavg entropy:1.1507261621004068 \t\tstd entropy:0.7133196131818714\n",
            "Epoch: 8 \t\tvalue loss:87.90450827618862 \t\tpolicy loss:1.2222953045621832 \t\tavg entropy:1.1268899100161314 \t\tstd entropy:0.6920985855294205\n",
            "Epoch: 9 \t\tvalue loss:114.89468261028858 \t\tpolicy loss:1.450539380311966 \t\tavg entropy:1.1683646895563382 \t\tstd entropy:0.6997338419340527\n",
            "Epoch: 10 \t\tvalue loss:88.86238825067561 \t\tpolicy loss:1.275183323850023 \t\tavg entropy:1.2210186494772648 \t\tstd entropy:0.6923328580516505\n",
            "Episode 168 finished after 200 timesteps - cumulative reward = -240.04225293636793\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:75.29949577311253 \t\tpolicy loss:1.0741276645914037 \t\tavg entropy:1.1806383239767595 \t\tstd entropy:0.6817273688659395\n",
            "Epoch: 2 \t\tvalue loss:65.98216133675676 \t\tpolicy loss:1.1549733490385907 \t\tavg entropy:1.1191981577351668 \t\tstd entropy:0.6886031490240507\n",
            "Epoch: 3 \t\tvalue loss:60.98159266279099 \t\tpolicy loss:1.137262670283622 \t\tavg entropy:1.1049630959637102 \t\tstd entropy:0.6834260049206322\n",
            "Epoch: 4 \t\tvalue loss:55.37623771961699 \t\tpolicy loss:1.3455513785494135 \t\tavg entropy:1.1599000919077431 \t\tstd entropy:0.6509946634487299\n",
            "Epoch: 5 \t\tvalue loss:58.45940243436935 \t\tpolicy loss:1.4151893156640074 \t\tavg entropy:1.2468362071576826 \t\tstd entropy:0.6408678472364869\n",
            "Epoch: 6 \t\tvalue loss:103.34942890735383 \t\tpolicy loss:1.6108619019072106 \t\tavg entropy:1.3221412556219487 \t\tstd entropy:0.6257360679332918\n",
            "Epoch: 7 \t\tvalue loss:197.77598172045768 \t\tpolicy loss:1.5671734854261925 \t\tavg entropy:1.3898870273547541 \t\tstd entropy:0.6449998145617416\n",
            "Epoch: 8 \t\tvalue loss:199.5573152988515 \t\tpolicy loss:1.5912272010711914 \t\tavg entropy:1.3203316310852138 \t\tstd entropy:0.7140529968078899\n",
            "Epoch: 9 \t\tvalue loss:161.7946812244172 \t\tpolicy loss:1.283288598060608 \t\tavg entropy:1.38131395575791 \t\tstd entropy:0.6249484372157793\n",
            "Epoch: 10 \t\tvalue loss:123.55920951924425 \t\tpolicy loss:1.1188105275022222 \t\tavg entropy:1.244915600275186 \t\tstd entropy:0.5817596172602281\n",
            "Episode 169 finished after 200 timesteps - cumulative reward = -2.252870018495083\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:96.73800909772832 \t\tpolicy loss:1.2517831572826872 \t\tavg entropy:1.1513834054163024 \t\tstd entropy:0.6428847103363192\n",
            "Epoch: 2 \t\tvalue loss:86.78855738741287 \t\tpolicy loss:1.141536628312253 \t\tavg entropy:1.1676918602726878 \t\tstd entropy:0.6967844342107765\n",
            "Epoch: 3 \t\tvalue loss:94.21589645172688 \t\tpolicy loss:1.0210843473038775 \t\tavg entropy:1.170468078600434 \t\tstd entropy:0.710750937009072\n",
            "Epoch: 4 \t\tvalue loss:86.6401995572638 \t\tpolicy loss:1.029611756826969 \t\tavg entropy:1.1365882373996856 \t\tstd entropy:0.7152438194343218\n",
            "Epoch: 5 \t\tvalue loss:74.70829042221638 \t\tpolicy loss:1.0420789192331599 \t\tavg entropy:1.112610446686387 \t\tstd entropy:0.695089508917055\n",
            "Epoch: 6 \t\tvalue loss:66.17349414115256 \t\tpolicy loss:0.8983592150059152 \t\tavg entropy:1.0531718981024447 \t\tstd entropy:0.6661013761276089\n",
            "Epoch: 7 \t\tvalue loss:57.478178376847126 \t\tpolicy loss:0.9634946159859921 \t\tavg entropy:1.0145828868634603 \t\tstd entropy:0.6469399558375603\n",
            "Epoch: 8 \t\tvalue loss:53.656595471057486 \t\tpolicy loss:1.0366260491787118 \t\tavg entropy:1.0386880055919847 \t\tstd entropy:0.6838028714567287\n",
            "Epoch: 9 \t\tvalue loss:52.80975766131218 \t\tpolicy loss:0.9490390717983246 \t\tavg entropy:1.0118425077897075 \t\tstd entropy:0.7223426805101891\n",
            "Epoch: 10 \t\tvalue loss:53.46751778810582 \t\tpolicy loss:0.9055632562079328 \t\tavg entropy:1.02633199842037 \t\tstd entropy:0.7226791955704844\n",
            "Episode 170 finished after 200 timesteps - cumulative reward = -1.528003232592867\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:54.305689713829445 \t\tpolicy loss:0.8427958927656475 \t\tavg entropy:0.9967324950518123 \t\tstd entropy:0.7149193526832023\n",
            "Epoch: 2 \t\tvalue loss:52.31574208861903 \t\tpolicy loss:0.9566038712074882 \t\tavg entropy:0.9212350009746614 \t\tstd entropy:0.7163090810655862\n",
            "Epoch: 3 \t\tvalue loss:48.67776051947945 \t\tpolicy loss:0.9695123481123071 \t\tavg entropy:0.9185283566620445 \t\tstd entropy:0.7120688052439788\n",
            "Epoch: 4 \t\tvalue loss:45.973535276714124 \t\tpolicy loss:0.8700482493952701 \t\tavg entropy:0.9110931989596122 \t\tstd entropy:0.6961622416379966\n",
            "Epoch: 5 \t\tvalue loss:44.80953782232184 \t\tpolicy loss:0.8359411026302137 \t\tavg entropy:0.8864575649533437 \t\tstd entropy:0.7070989877041629\n",
            "Epoch: 6 \t\tvalue loss:43.4703431725502 \t\tpolicy loss:0.7799705389298891 \t\tavg entropy:0.8659609155584426 \t\tstd entropy:0.71566037942482\n",
            "Epoch: 7 \t\tvalue loss:43.47279068796258 \t\tpolicy loss:0.7869110273687463 \t\tavg entropy:0.8704881166358635 \t\tstd entropy:0.6984404173047946\n",
            "Epoch: 8 \t\tvalue loss:44.46981093256097 \t\tpolicy loss:0.7528013954037114 \t\tavg entropy:0.8182257137419732 \t\tstd entropy:0.6752808552940339\n",
            "Epoch: 9 \t\tvalue loss:42.34407022313068 \t\tpolicy loss:0.8794680579712516 \t\tavg entropy:0.7896008745484081 \t\tstd entropy:0.7023453823750394\n",
            "Epoch: 10 \t\tvalue loss:40.381649993595325 \t\tpolicy loss:0.8156053885033256 \t\tavg entropy:0.8228921021555988 \t\tstd entropy:0.7108014941750003\n",
            "Episode 171 finished after 200 timesteps - cumulative reward = -1.8930466675230033\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:39.07500053142247 \t\tpolicy loss:0.7632191877616079 \t\tavg entropy:0.8357120202460637 \t\tstd entropy:0.6964320235196623\n",
            "Epoch: 2 \t\tvalue loss:38.16832588597348 \t\tpolicy loss:0.72345525590997 \t\tavg entropy:0.817841526039356 \t\tstd entropy:0.6802573746538108\n",
            "Epoch: 3 \t\tvalue loss:38.726087786021985 \t\tpolicy loss:0.6851295122974798 \t\tavg entropy:0.8045814827177029 \t\tstd entropy:0.6693670226276447\n",
            "Epoch: 4 \t\tvalue loss:39.099836827579296 \t\tpolicy loss:0.7071681113619553 \t\tavg entropy:0.7510589043575052 \t\tstd entropy:0.654576090952983\n",
            "Epoch: 5 \t\tvalue loss:38.163522404746004 \t\tpolicy loss:0.7384820053451939 \t\tavg entropy:0.7506432596065472 \t\tstd entropy:0.6802650160625542\n",
            "Epoch: 6 \t\tvalue loss:37.15867874120411 \t\tpolicy loss:0.631894574667278 \t\tavg entropy:0.7537080246865077 \t\tstd entropy:0.6722008606200395\n",
            "Epoch: 7 \t\tvalue loss:37.511866942204925 \t\tpolicy loss:0.5771849782843339 \t\tavg entropy:0.7112982230324443 \t\tstd entropy:0.6167597142714782\n",
            "Epoch: 8 \t\tvalue loss:38.23799885323173 \t\tpolicy loss:0.5924423269535366 \t\tavg entropy:0.6627195216722017 \t\tstd entropy:0.587495110435768\n",
            "Epoch: 9 \t\tvalue loss:36.98590747682672 \t\tpolicy loss:0.6376641458586643 \t\tavg entropy:0.6517500341139393 \t\tstd entropy:0.5937712020330548\n",
            "Epoch: 10 \t\tvalue loss:35.33914412824731 \t\tpolicy loss:0.5819865985920555 \t\tavg entropy:0.6518107640339007 \t\tstd entropy:0.586074431030756\n",
            "Episode 172 finished after 200 timesteps - cumulative reward = -126.73270804905376\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:35.43207143858859 \t\tpolicy loss:0.5585200413277275 \t\tavg entropy:0.6348659536186637 \t\tstd entropy:0.5675120209116626\n",
            "Epoch: 2 \t\tvalue loss:36.55387158895794 \t\tpolicy loss:0.5807998986620652 \t\tavg entropy:0.6280035582868695 \t\tstd entropy:0.5703188586478931\n",
            "Epoch: 3 \t\tvalue loss:35.974892844651876 \t\tpolicy loss:0.5979626525389521 \t\tavg entropy:0.6351380888326529 \t\tstd entropy:0.5782192274447924\n",
            "Epoch: 4 \t\tvalue loss:35.15346791995199 \t\tpolicy loss:0.5818732834176014 \t\tavg entropy:0.6366264114691489 \t\tstd entropy:0.5747155876774418\n",
            "Epoch: 5 \t\tvalue loss:35.34151878607901 \t\tpolicy loss:0.5910502810227244 \t\tavg entropy:0.6347569604753613 \t\tstd entropy:0.5748431655719455\n",
            "Epoch: 6 \t\tvalue loss:34.98946531697324 \t\tpolicy loss:0.5925175031549053 \t\tavg entropy:0.6366706302403832 \t\tstd entropy:0.5812908482301893\n",
            "Epoch: 7 \t\tvalue loss:34.601795742386265 \t\tpolicy loss:0.5788795118269168 \t\tavg entropy:0.6347289053570114 \t\tstd entropy:0.5763046115434286\n",
            "Epoch: 8 \t\tvalue loss:34.31498601185648 \t\tpolicy loss:0.5957768673959531 \t\tavg entropy:0.6327230427176702 \t\tstd entropy:0.569652783410607\n",
            "Epoch: 9 \t\tvalue loss:35.466984996042754 \t\tpolicy loss:0.6142249946531496 \t\tavg entropy:0.6294299198283205 \t\tstd entropy:0.5795243829072653\n",
            "Epoch: 10 \t\tvalue loss:36.9618420450311 \t\tpolicy loss:0.6279176503419877 \t\tavg entropy:0.6376238500831559 \t\tstd entropy:0.5799446899336602\n",
            "Episode 173 finished after 200 timesteps - cumulative reward = -235.648421122673\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:32.78215817903217 \t\tpolicy loss:0.8241194107030567 \t\tavg entropy:0.6877283469865944 \t\tstd entropy:0.5875618201596701\n",
            "Epoch: 2 \t\tvalue loss:44.63235149383545 \t\tpolicy loss:0.9938944700517153 \t\tavg entropy:0.7168677303647085 \t\tstd entropy:0.6104456357214113\n",
            "Epoch: 3 \t\tvalue loss:49.64669374164782 \t\tpolicy loss:1.383570058722245 \t\tavg entropy:0.819231996577636 \t\tstd entropy:0.6747117326315718\n",
            "Epoch: 4 \t\tvalue loss:71.40114947871157 \t\tpolicy loss:1.2305506916422593 \t\tavg entropy:0.877436386533154 \t\tstd entropy:0.6842328680861695\n",
            "Epoch: 5 \t\tvalue loss:141.11686905308773 \t\tpolicy loss:1.9268589371129086 \t\tavg entropy:1.0750534317041518 \t\tstd entropy:0.7034923435065611\n",
            "Epoch: 6 \t\tvalue loss:136.89746700086093 \t\tpolicy loss:1.7270000545602096 \t\tavg entropy:1.2329102915742818 \t\tstd entropy:0.6459015617228934\n",
            "Epoch: 7 \t\tvalue loss:140.87961064388878 \t\tpolicy loss:1.570835562756187 \t\tavg entropy:1.2632584785262835 \t\tstd entropy:0.653422385104646\n",
            "Epoch: 8 \t\tvalue loss:176.33292545519376 \t\tpolicy loss:1.7220482562717638 \t\tavg entropy:1.3679839566678353 \t\tstd entropy:0.6223028581799381\n",
            "Epoch: 9 \t\tvalue loss:193.70450320996736 \t\tpolicy loss:1.9576537182456568 \t\tavg entropy:1.4288049952721298 \t\tstd entropy:0.5892309587039899\n",
            "Epoch: 10 \t\tvalue loss:204.91165558664423 \t\tpolicy loss:1.450988397472783 \t\tavg entropy:1.5119148530231254 \t\tstd entropy:0.5865357906542071\n",
            "Episode 174 finished after 200 timesteps - cumulative reward = -247.0559396769719\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:132.58367034510562 \t\tpolicy loss:1.2057458708160802 \t\tavg entropy:1.2393263860122155 \t\tstd entropy:0.6509351850566956\n",
            "Epoch: 2 \t\tvalue loss:95.67475718448037 \t\tpolicy loss:1.3382087751438743 \t\tavg entropy:1.2626181478241654 \t\tstd entropy:0.6588401301950462\n",
            "Epoch: 3 \t\tvalue loss:101.05379604540373 \t\tpolicy loss:1.2444591999053956 \t\tavg entropy:1.2819379626434984 \t\tstd entropy:0.6719244770869879\n",
            "Epoch: 4 \t\tvalue loss:120.93086345070287 \t\tpolicy loss:1.3529828397851242 \t\tavg entropy:1.2542517945515874 \t\tstd entropy:0.6900892222812127\n",
            "Epoch: 5 \t\tvalue loss:134.82434363114206 \t\tpolicy loss:1.4193795041034096 \t\tavg entropy:1.2862747584187524 \t\tstd entropy:0.6417197383472225\n",
            "Epoch: 6 \t\tvalue loss:165.0626148625424 \t\tpolicy loss:1.6246470068630419 \t\tavg entropy:1.3487921535269995 \t\tstd entropy:0.5752058020377703\n",
            "Epoch: 7 \t\tvalue loss:153.43963615015934 \t\tpolicy loss:1.3725348089870653 \t\tavg entropy:1.330311636725519 \t\tstd entropy:0.6025209773318347\n",
            "Epoch: 8 \t\tvalue loss:166.78215376201427 \t\tpolicy loss:1.2237500579733598 \t\tavg entropy:1.247845494049415 \t\tstd entropy:0.579260490689556\n",
            "Epoch: 9 \t\tvalue loss:201.55052259344805 \t\tpolicy loss:1.0927026641996282 \t\tavg entropy:1.1855183943161252 \t\tstd entropy:0.5261480650128084\n",
            "Epoch: 10 \t\tvalue loss:128.11472292448346 \t\tpolicy loss:1.0054650294153313 \t\tavg entropy:1.0454247399563068 \t\tstd entropy:0.502473595747841\n",
            "Episode 175 finished after 200 timesteps - cumulative reward = -334.4178783659638\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:78.30082962413628 \t\tpolicy loss:0.9318797842909893 \t\tavg entropy:1.0110201354477626 \t\tstd entropy:0.5096636495864333\n",
            "Epoch: 2 \t\tvalue loss:57.90591956923405 \t\tpolicy loss:0.7843276594455043 \t\tavg entropy:0.9331095944382172 \t\tstd entropy:0.5288078367876125\n",
            "Epoch: 3 \t\tvalue loss:49.50731185823679 \t\tpolicy loss:0.7331102890893817 \t\tavg entropy:0.9018978407497258 \t\tstd entropy:0.5502380903506421\n",
            "Epoch: 4 \t\tvalue loss:50.11442748705546 \t\tpolicy loss:0.6943737004573146 \t\tavg entropy:0.8742419383715117 \t\tstd entropy:0.5659154484245972\n",
            "Epoch: 5 \t\tvalue loss:56.48719472438097 \t\tpolicy loss:0.6718367779006561 \t\tavg entropy:0.8362547925093134 \t\tstd entropy:0.5952151334034932\n",
            "Epoch: 6 \t\tvalue loss:60.519584914048515 \t\tpolicy loss:0.6332434543098012 \t\tavg entropy:0.7895164238005469 \t\tstd entropy:0.6131102884128137\n",
            "Epoch: 7 \t\tvalue loss:56.06592376778523 \t\tpolicy loss:0.5928832165276011 \t\tavg entropy:0.7442954344354925 \t\tstd entropy:0.6084009323125411\n",
            "Epoch: 8 \t\tvalue loss:50.720929227769375 \t\tpolicy loss:0.5872752324988445 \t\tavg entropy:0.7151882082721507 \t\tstd entropy:0.6041247461457231\n",
            "Epoch: 9 \t\tvalue loss:48.50203518321117 \t\tpolicy loss:0.5885927947238088 \t\tavg entropy:0.6949775586059099 \t\tstd entropy:0.6018938817093763\n",
            "Epoch: 10 \t\tvalue loss:47.12076853339871 \t\tpolicy loss:0.6021185154095292 \t\tavg entropy:0.6708442317161252 \t\tstd entropy:0.6038729662877634\n",
            "Episode 176 finished after 200 timesteps - cumulative reward = -125.25375188447485\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:45.24132871131102 \t\tpolicy loss:0.6033874371399482 \t\tavg entropy:0.6481025277460865 \t\tstd entropy:0.6055859680944816\n",
            "Epoch: 2 \t\tvalue loss:43.30238984525204 \t\tpolicy loss:0.5611075041815639 \t\tavg entropy:0.6425805651344344 \t\tstd entropy:0.6006811901007285\n",
            "Epoch: 3 \t\tvalue loss:41.10154482970635 \t\tpolicy loss:0.5219975694393119 \t\tavg entropy:0.6150676306957394 \t\tstd entropy:0.5822297514010546\n",
            "Epoch: 4 \t\tvalue loss:39.852494029949106 \t\tpolicy loss:0.5249116386597356 \t\tavg entropy:0.5927407812725505 \t\tstd entropy:0.5652314514421938\n",
            "Epoch: 5 \t\tvalue loss:40.58669672285517 \t\tpolicy loss:0.5366376571667691 \t\tavg entropy:0.5872252369379919 \t\tstd entropy:0.5598161400911255\n",
            "Epoch: 6 \t\tvalue loss:40.480182092636824 \t\tpolicy loss:0.5253134894495209 \t\tavg entropy:0.5830112276433647 \t\tstd entropy:0.5575507010021533\n",
            "Epoch: 7 \t\tvalue loss:39.131377533078194 \t\tpolicy loss:0.5214773239567876 \t\tavg entropy:0.5870172409781649 \t\tstd entropy:0.553605116158408\n",
            "Epoch: 8 \t\tvalue loss:39.519876984258495 \t\tpolicy loss:0.5231191997105876 \t\tavg entropy:0.589296345488973 \t\tstd entropy:0.5478737378337245\n",
            "Epoch: 9 \t\tvalue loss:39.93165911113223 \t\tpolicy loss:0.5052191214635968 \t\tavg entropy:0.5736158995942662 \t\tstd entropy:0.5424771849846536\n",
            "Epoch: 10 \t\tvalue loss:37.96628432596723 \t\tpolicy loss:0.49878203760211665 \t\tavg entropy:0.5606437878381791 \t\tstd entropy:0.5386720370240569\n",
            "Episode 177 finished after 200 timesteps - cumulative reward = -126.62502223015534\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:37.02588520695766 \t\tpolicy loss:0.5070853759534657 \t\tavg entropy:0.5600237442662637 \t\tstd entropy:0.5400798343195686\n",
            "Epoch: 2 \t\tvalue loss:37.72820052256187 \t\tpolicy loss:0.5023794518783689 \t\tavg entropy:0.5487453189120616 \t\tstd entropy:0.5388129251929905\n",
            "Epoch: 3 \t\tvalue loss:36.52087007214626 \t\tpolicy loss:0.512427422683686 \t\tavg entropy:0.5322890287970145 \t\tstd entropy:0.5360444550749358\n",
            "Epoch: 4 \t\tvalue loss:36.208669263869524 \t\tpolicy loss:0.5523768706868092 \t\tavg entropy:0.5393174408237101 \t\tstd entropy:0.5400715338575429\n",
            "Epoch: 5 \t\tvalue loss:36.73173565914234 \t\tpolicy loss:0.5400051836234828 \t\tavg entropy:0.5423032622206684 \t\tstd entropy:0.552920394765964\n",
            "Epoch: 6 \t\tvalue loss:35.4685280546546 \t\tpolicy loss:0.5434932809633514 \t\tavg entropy:0.5511431249379336 \t\tstd entropy:0.5715899742073556\n",
            "Epoch: 7 \t\tvalue loss:35.981415130198 \t\tpolicy loss:0.5584302643934885 \t\tavg entropy:0.5715904616925087 \t\tstd entropy:0.587529971994356\n",
            "Epoch: 8 \t\tvalue loss:35.692801459381975 \t\tpolicy loss:0.528007386252284 \t\tavg entropy:0.5757161894931914 \t\tstd entropy:0.6095126246132876\n",
            "Epoch: 9 \t\tvalue loss:34.58280750364065 \t\tpolicy loss:0.5310253314673901 \t\tavg entropy:0.5810311724111104 \t\tstd entropy:0.6143170752513479\n",
            "Epoch: 10 \t\tvalue loss:35.08993140235543 \t\tpolicy loss:0.5196716176966826 \t\tavg entropy:0.5784310714590933 \t\tstd entropy:0.6141882065534691\n",
            "Episode 178 finished after 200 timesteps - cumulative reward = -2.2006285953744014\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:33.984218003849186 \t\tpolicy loss:0.5298022283241153 \t\tavg entropy:0.5739140691579081 \t\tstd entropy:0.6158176551595956\n",
            "Epoch: 2 \t\tvalue loss:34.86642091969649 \t\tpolicy loss:0.5655018819185594 \t\tavg entropy:0.5912326043205252 \t\tstd entropy:0.6165430134953209\n",
            "Epoch: 3 \t\tvalue loss:33.68147099018097 \t\tpolicy loss:0.5861381458428999 \t\tavg entropy:0.5875804715406358 \t\tstd entropy:0.6149420854435332\n",
            "Epoch: 4 \t\tvalue loss:35.61314224700133 \t\tpolicy loss:0.6246519763953984 \t\tavg entropy:0.6228108081461892 \t\tstd entropy:0.6084550927000981\n",
            "Epoch: 5 \t\tvalue loss:35.19407753398021 \t\tpolicy loss:0.6695906721676389 \t\tavg entropy:0.6262169938170653 \t\tstd entropy:0.6222358556175418\n",
            "Epoch: 6 \t\tvalue loss:36.14779528230429 \t\tpolicy loss:0.7601024457253516 \t\tavg entropy:0.6751564123038994 \t\tstd entropy:0.6179326691308803\n",
            "Epoch: 7 \t\tvalue loss:36.65905564030012 \t\tpolicy loss:0.8195238698584338 \t\tavg entropy:0.6984861891107296 \t\tstd entropy:0.6528409651759705\n",
            "Epoch: 8 \t\tvalue loss:51.56529555966457 \t\tpolicy loss:1.1185397741695244 \t\tavg entropy:0.7711683061398424 \t\tstd entropy:0.6301329055784183\n",
            "Epoch: 9 \t\tvalue loss:49.06327072779337 \t\tpolicy loss:1.110958540191253 \t\tavg entropy:0.8303646601014438 \t\tstd entropy:0.6740080963159268\n",
            "Epoch: 10 \t\tvalue loss:55.83868380387624 \t\tpolicy loss:1.3887431435286999 \t\tavg entropy:0.9838882120816309 \t\tstd entropy:0.6388766880442572\n",
            "Episode 179 finished after 200 timesteps - cumulative reward = -230.3561099222766\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:74.68371993303299 \t\tpolicy loss:1.1263598681737979 \t\tavg entropy:1.0153045927092583 \t\tstd entropy:0.6860863136196462\n",
            "Epoch: 2 \t\tvalue loss:77.59351929028828 \t\tpolicy loss:0.9859086945652962 \t\tavg entropy:0.9494037895736595 \t\tstd entropy:0.6600743013104228\n",
            "Epoch: 3 \t\tvalue loss:68.48744591077168 \t\tpolicy loss:1.020890881928305 \t\tavg entropy:0.928539278031037 \t\tstd entropy:0.6755911196410853\n",
            "Epoch: 4 \t\tvalue loss:50.30040496091048 \t\tpolicy loss:1.1607773937284946 \t\tavg entropy:0.9387055891879015 \t\tstd entropy:0.7034744332328781\n",
            "Epoch: 5 \t\tvalue loss:58.6144136240085 \t\tpolicy loss:1.0229523181915283 \t\tavg entropy:0.9962218132467824 \t\tstd entropy:0.6608741180001302\n",
            "Epoch: 6 \t\tvalue loss:77.58110502362251 \t\tpolicy loss:1.2566675984611113 \t\tavg entropy:1.087525921943503 \t\tstd entropy:0.6630507596290849\n",
            "Epoch: 7 \t\tvalue loss:64.21624126285315 \t\tpolicy loss:1.045248598481218 \t\tavg entropy:0.9785759773079772 \t\tstd entropy:0.6861097760912286\n",
            "Epoch: 8 \t\tvalue loss:48.3111028795441 \t\tpolicy loss:0.9659950351342559 \t\tavg entropy:0.9621681034091477 \t\tstd entropy:0.6688343820781248\n",
            "Epoch: 9 \t\tvalue loss:56.78938387955228 \t\tpolicy loss:1.0963981784880161 \t\tavg entropy:1.036473590689376 \t\tstd entropy:0.6742145441429451\n",
            "Epoch: 10 \t\tvalue loss:52.24542383104563 \t\tpolicy loss:0.9844981885204712 \t\tavg entropy:1.0214150014879184 \t\tstd entropy:0.6929059564560446\n",
            "Episode 180 finished after 200 timesteps - cumulative reward = -2.483845593951956\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:39.212406285030326 \t\tpolicy loss:0.8704305493954531 \t\tavg entropy:0.9409556120468326 \t\tstd entropy:0.7015494425833457\n",
            "Epoch: 2 \t\tvalue loss:41.246752211728044 \t\tpolicy loss:0.8512429548907525 \t\tavg entropy:0.9309152066437042 \t\tstd entropy:0.6759574681580302\n",
            "Epoch: 3 \t\tvalue loss:46.06314548634991 \t\tpolicy loss:0.8687128781043377 \t\tavg entropy:0.9846004083904335 \t\tstd entropy:0.6620802269293432\n",
            "Epoch: 4 \t\tvalue loss:43.06505045694174 \t\tpolicy loss:0.7684647059932197 \t\tavg entropy:0.8832642336425676 \t\tstd entropy:0.6758026510150811\n",
            "Epoch: 5 \t\tvalue loss:33.64436629506731 \t\tpolicy loss:0.7154552795223355 \t\tavg entropy:0.8100975088528144 \t\tstd entropy:0.6624542845203301\n",
            "Epoch: 6 \t\tvalue loss:35.14186216383865 \t\tpolicy loss:0.7792614090073969 \t\tavg entropy:0.8507862212690692 \t\tstd entropy:0.6256346796215272\n",
            "Epoch: 7 \t\tvalue loss:36.89463670597863 \t\tpolicy loss:0.6535425296763784 \t\tavg entropy:0.8254583722429384 \t\tstd entropy:0.6216325006645376\n",
            "Epoch: 8 \t\tvalue loss:33.202847711818734 \t\tpolicy loss:0.6596010519364446 \t\tavg entropy:0.7315467566814076 \t\tstd entropy:0.6167819514808743\n",
            "Epoch: 9 \t\tvalue loss:31.391254772845002 \t\tpolicy loss:0.6840846397213101 \t\tavg entropy:0.742584672613557 \t\tstd entropy:0.5998185988342265\n",
            "Epoch: 10 \t\tvalue loss:33.21710482086103 \t\tpolicy loss:0.6997210792045003 \t\tavg entropy:0.7712885601081966 \t\tstd entropy:0.5999695793232641\n",
            "Episode 181 finished after 200 timesteps - cumulative reward = -115.40846685235292\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:32.883201102620546 \t\tpolicy loss:0.6490871574460846 \t\tavg entropy:0.7245852017867247 \t\tstd entropy:0.6090534632016096\n",
            "Epoch: 2 \t\tvalue loss:29.67913401987135 \t\tpolicy loss:0.6726977900438702 \t\tavg entropy:0.7185305953350763 \t\tstd entropy:0.6031773135118735\n",
            "Epoch: 3 \t\tvalue loss:31.687789403286178 \t\tpolicy loss:0.689151620741972 \t\tavg entropy:0.7485589737659701 \t\tstd entropy:0.6118232336334108\n",
            "Epoch: 4 \t\tvalue loss:30.26276510032182 \t\tpolicy loss:0.6364663212569719 \t\tavg entropy:0.7154077493592516 \t\tstd entropy:0.6181102670512935\n",
            "Epoch: 5 \t\tvalue loss:28.782492790025533 \t\tpolicy loss:0.686046520920144 \t\tavg entropy:0.7224048565925927 \t\tstd entropy:0.6110571146435304\n",
            "Epoch: 6 \t\tvalue loss:30.760561864400646 \t\tpolicy loss:0.655786750857363 \t\tavg entropy:0.7360865314028365 \t\tstd entropy:0.6188985169042939\n",
            "Epoch: 7 \t\tvalue loss:27.606037144808425 \t\tpolicy loss:0.631577914369475 \t\tavg entropy:0.6993510992267741 \t\tstd entropy:0.6090093552649689\n",
            "Epoch: 8 \t\tvalue loss:28.876533038837394 \t\tpolicy loss:0.6851263318172435 \t\tavg entropy:0.7202408774992188 \t\tstd entropy:0.6025700849185873\n",
            "Epoch: 9 \t\tvalue loss:28.3996720953086 \t\tpolicy loss:0.6278073050312161 \t\tavg entropy:0.6942318604742455 \t\tstd entropy:0.6044209730013536\n",
            "Epoch: 10 \t\tvalue loss:26.701409108859977 \t\tpolicy loss:0.6687247075678147 \t\tavg entropy:0.6943195844730936 \t\tstd entropy:0.592251211184418\n",
            "Episode 182 finished after 200 timesteps - cumulative reward = -249.64474551572314\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:30.42135805936204 \t\tpolicy loss:0.6618570341277368 \t\tavg entropy:0.7036476208121637 \t\tstd entropy:0.6006793455429675\n",
            "Epoch: 2 \t\tvalue loss:25.29535395828719 \t\tpolicy loss:0.6776606034986752 \t\tavg entropy:0.6827681742262897 \t\tstd entropy:0.5895599725835403\n",
            "Epoch: 3 \t\tvalue loss:33.57787417873894 \t\tpolicy loss:0.7570437696791187 \t\tavg entropy:0.7151360271857775 \t\tstd entropy:0.5970534092678184\n",
            "Epoch: 4 \t\tvalue loss:24.086161387335395 \t\tpolicy loss:0.7943370334573627 \t\tavg entropy:0.6956044700900578 \t\tstd entropy:0.5965211474309788\n",
            "Epoch: 5 \t\tvalue loss:47.091102034775254 \t\tpolicy loss:1.0675298485559286 \t\tavg entropy:0.7828381121376468 \t\tstd entropy:0.6249112658725269\n",
            "Epoch: 6 \t\tvalue loss:26.509584478496276 \t\tpolicy loss:1.1022498079181946 \t\tavg entropy:0.7572619784357855 \t\tstd entropy:0.635547954800291\n",
            "Epoch: 7 \t\tvalue loss:49.939126388313845 \t\tpolicy loss:1.0203213313805688 \t\tavg entropy:0.8964011721129315 \t\tstd entropy:0.6548184683762013\n",
            "Epoch: 8 \t\tvalue loss:31.405491991141407 \t\tpolicy loss:0.9991419296903709 \t\tavg entropy:0.8650122162651638 \t\tstd entropy:0.6501387127086529\n",
            "Epoch: 9 \t\tvalue loss:58.44912835986344 \t\tpolicy loss:1.198447564828027 \t\tavg entropy:0.9693428953215585 \t\tstd entropy:0.6256167411543795\n",
            "Epoch: 10 \t\tvalue loss:55.70044461968019 \t\tpolicy loss:1.3133872489339298 \t\tavg entropy:1.0108915470419986 \t\tstd entropy:0.673838347634793\n",
            "Episode 183 finished after 200 timesteps - cumulative reward = -1.5045305344873259\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:34.0620296075172 \t\tpolicy loss:1.250386435346505 \t\tavg entropy:1.0264719270948286 \t\tstd entropy:0.6683655825803542\n",
            "Epoch: 2 \t\tvalue loss:52.72020338491066 \t\tpolicy loss:1.17744776084251 \t\tavg entropy:1.0901814822306783 \t\tstd entropy:0.6443163548879277\n",
            "Epoch: 3 \t\tvalue loss:84.70940051127955 \t\tpolicy loss:1.1401814830671881 \t\tavg entropy:1.149521766778221 \t\tstd entropy:0.6746695413759158\n",
            "Epoch: 4 \t\tvalue loss:55.418495802535226 \t\tpolicy loss:1.2419751919421953 \t\tavg entropy:1.0896586943081843 \t\tstd entropy:0.6793776523064394\n",
            "Epoch: 5 \t\tvalue loss:54.25477893082137 \t\tpolicy loss:1.0921263442826026 \t\tavg entropy:1.114413635612606 \t\tstd entropy:0.631617548022733\n",
            "Epoch: 6 \t\tvalue loss:59.45391391478863 \t\tpolicy loss:1.1843615094411004 \t\tavg entropy:1.1354799214156521 \t\tstd entropy:0.650181078887685\n",
            "Epoch: 7 \t\tvalue loss:40.03188142334063 \t\tpolicy loss:1.180256202663343 \t\tavg entropy:1.1065801123644068 \t\tstd entropy:0.6779350742983731\n",
            "Epoch: 8 \t\tvalue loss:31.099297882355366 \t\tpolicy loss:1.0866873098402907 \t\tavg entropy:1.1550037036819154 \t\tstd entropy:0.6477983554275355\n",
            "Epoch: 9 \t\tvalue loss:40.34056419932965 \t\tpolicy loss:0.9662749232705107 \t\tavg entropy:1.1311056224082432 \t\tstd entropy:0.6190053757544839\n",
            "Epoch: 10 \t\tvalue loss:55.91140342496105 \t\tpolicy loss:1.142099132242891 \t\tavg entropy:1.0438214725312323 \t\tstd entropy:0.6608531370098404\n",
            "Episode 184 finished after 200 timesteps - cumulative reward = -117.27312100734252\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:47.59253623313511 \t\tpolicy loss:0.9345564522694066 \t\tavg entropy:1.0026241011420414 \t\tstd entropy:0.7042698618810821\n",
            "Epoch: 2 \t\tvalue loss:31.156008558175 \t\tpolicy loss:1.0373070753112281 \t\tavg entropy:0.9882148005519957 \t\tstd entropy:0.6872538391088955\n",
            "Epoch: 3 \t\tvalue loss:32.1391124749921 \t\tpolicy loss:0.8295610820509723 \t\tavg entropy:1.0004728146466852 \t\tstd entropy:0.6551942191898179\n",
            "Epoch: 4 \t\tvalue loss:43.378135408322834 \t\tpolicy loss:0.8504413412403815 \t\tavg entropy:0.9593808708098254 \t\tstd entropy:0.6458126310061111\n",
            "Epoch: 5 \t\tvalue loss:41.921140370909704 \t\tpolicy loss:0.8572501890438119 \t\tavg entropy:0.9114002314613672 \t\tstd entropy:0.6721560238400203\n",
            "Epoch: 6 \t\tvalue loss:28.066874103447827 \t\tpolicy loss:0.9243138278882528 \t\tavg entropy:0.8952330047188353 \t\tstd entropy:0.6573546834998967\n",
            "Epoch: 7 \t\tvalue loss:31.822619499619474 \t\tpolicy loss:0.8277460023299935 \t\tavg entropy:0.9444311810747058 \t\tstd entropy:0.6140644913113498\n",
            "Epoch: 8 \t\tvalue loss:40.5045560192816 \t\tpolicy loss:0.7898616179363015 \t\tavg entropy:0.9037937089811285 \t\tstd entropy:0.6088682526911935\n",
            "Epoch: 9 \t\tvalue loss:31.105368737092952 \t\tpolicy loss:0.7649715324652564 \t\tavg entropy:0.8265892593994354 \t\tstd entropy:0.626165648902494\n",
            "Epoch: 10 \t\tvalue loss:26.556605833092917 \t\tpolicy loss:0.7578839410826103 \t\tavg entropy:0.8347705959812345 \t\tstd entropy:0.58346016311332\n",
            "Episode 185 finished after 200 timesteps - cumulative reward = -290.9934481021326\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:34.7631398074481 \t\tpolicy loss:0.771780748452459 \t\tavg entropy:0.8322406118556996 \t\tstd entropy:0.5829779199292502\n",
            "Epoch: 2 \t\tvalue loss:34.449531156189586 \t\tpolicy loss:0.8581553335700717 \t\tavg entropy:0.8085222581146662 \t\tstd entropy:0.6005342499827033\n",
            "Epoch: 3 \t\tvalue loss:24.705243473150293 \t\tpolicy loss:0.9685317953022159 \t\tavg entropy:0.8455773360801111 \t\tstd entropy:0.6122388511464228\n",
            "Epoch: 4 \t\tvalue loss:32.050301279340474 \t\tpolicy loss:0.9136868459837777 \t\tavg entropy:0.9069444818902831 \t\tstd entropy:0.6049739314231406\n",
            "Epoch: 5 \t\tvalue loss:40.681220655538596 \t\tpolicy loss:0.8298749832474456 \t\tavg entropy:0.9108163286656452 \t\tstd entropy:0.6245215103636168\n",
            "Epoch: 6 \t\tvalue loss:25.008244468241323 \t\tpolicy loss:0.929569205155178 \t\tavg entropy:0.8641172033246449 \t\tstd entropy:0.6192332350370456\n",
            "Epoch: 7 \t\tvalue loss:30.087742014807098 \t\tpolicy loss:0.8342498142500313 \t\tavg entropy:0.9122602025767393 \t\tstd entropy:0.5826763079027092\n",
            "Epoch: 8 \t\tvalue loss:39.89095023700169 \t\tpolicy loss:0.8096699687291165 \t\tavg entropy:0.8658131607847107 \t\tstd entropy:0.610388271889441\n",
            "Epoch: 9 \t\tvalue loss:29.309725883055705 \t\tpolicy loss:1.112864237050621 \t\tavg entropy:0.8849562498631721 \t\tstd entropy:0.6263531055208322\n",
            "Epoch: 10 \t\tvalue loss:28.46013610217036 \t\tpolicy loss:1.073416796265816 \t\tavg entropy:0.9293032639885286 \t\tstd entropy:0.646020590529723\n",
            "Episode 186 finished after 200 timesteps - cumulative reward = -230.7048794004048\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:40.91238259296028 \t\tpolicy loss:1.047247690813882 \t\tavg entropy:1.0214141971498583 \t\tstd entropy:0.654710935526127\n",
            "Epoch: 2 \t\tvalue loss:35.28764537645846 \t\tpolicy loss:0.7743050145859621 \t\tavg entropy:0.9438078723391803 \t\tstd entropy:0.6456805265638472\n",
            "Epoch: 3 \t\tvalue loss:23.82457716610967 \t\tpolicy loss:0.8773321497197054 \t\tavg entropy:0.8599000055168412 \t\tstd entropy:0.6049762620070517\n",
            "Epoch: 4 \t\tvalue loss:32.45930132817249 \t\tpolicy loss:0.9257429415474132 \t\tavg entropy:0.9448679991609111 \t\tstd entropy:0.5973634621794868\n",
            "Epoch: 5 \t\tvalue loss:35.525711229869295 \t\tpolicy loss:0.7846919240391984 \t\tavg entropy:0.8763093810227376 \t\tstd entropy:0.6110201079320696\n",
            "Epoch: 6 \t\tvalue loss:25.465794001306808 \t\tpolicy loss:1.0015835740736552 \t\tavg entropy:0.8859591379139486 \t\tstd entropy:0.6034575081739788\n",
            "Epoch: 7 \t\tvalue loss:33.206181243974335 \t\tpolicy loss:1.0941159165635401 \t\tavg entropy:0.9334710720541127 \t\tstd entropy:0.6178098422166455\n",
            "Epoch: 8 \t\tvalue loss:43.78513231082838 \t\tpolicy loss:1.007726055018756 \t\tavg entropy:0.959588425990579 \t\tstd entropy:0.649309543846111\n",
            "Epoch: 9 \t\tvalue loss:26.86816570953447 \t\tpolicy loss:0.9801471516185877 \t\tavg entropy:0.9427951217232151 \t\tstd entropy:0.6400622609484429\n",
            "Epoch: 10 \t\tvalue loss:30.471287751684383 \t\tpolicy loss:0.8288538948613771 \t\tavg entropy:0.919157776214356 \t\tstd entropy:0.6179442881333918\n",
            "Episode 187 finished after 200 timesteps - cumulative reward = -348.5224051887091\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:36.51165603131664 \t\tpolicy loss:0.9010275185716395 \t\tavg entropy:0.9370245761200898 \t\tstd entropy:0.6152923989121983\n",
            "Epoch: 2 \t\tvalue loss:24.32506865871196 \t\tpolicy loss:0.83577198367946 \t\tavg entropy:0.8769896420190678 \t\tstd entropy:0.5907786510439909\n",
            "Epoch: 3 \t\tvalue loss:27.186399999929932 \t\tpolicy loss:0.8658054726464408 \t\tavg entropy:0.8984330031976147 \t\tstd entropy:0.5731230658047227\n",
            "Epoch: 4 \t\tvalue loss:34.95983682846536 \t\tpolicy loss:0.8444216725777607 \t\tavg entropy:0.8829365550677859 \t\tstd entropy:0.5826298413570064\n",
            "Epoch: 5 \t\tvalue loss:29.432992891389496 \t\tpolicy loss:1.1378872482752314 \t\tavg entropy:0.8984927931049851 \t\tstd entropy:0.6009188353928043\n",
            "Epoch: 6 \t\tvalue loss:30.36828224999564 \t\tpolicy loss:1.1140553251821168 \t\tavg entropy:0.9576202527798785 \t\tstd entropy:0.6298370081803192\n",
            "Epoch: 7 \t\tvalue loss:43.88091186601289 \t\tpolicy loss:1.040794201651398 \t\tavg entropy:0.9933521724570223 \t\tstd entropy:0.6214333977749985\n",
            "Epoch: 8 \t\tvalue loss:38.842417174456074 \t\tpolicy loss:1.0480710806287066 \t\tavg entropy:1.0128338394192686 \t\tstd entropy:0.6436094335254838\n",
            "Epoch: 9 \t\tvalue loss:26.289054848709885 \t\tpolicy loss:1.0443004099081974 \t\tavg entropy:0.9670177694481712 \t\tstd entropy:0.6277543094947281\n",
            "Epoch: 10 \t\tvalue loss:35.926048449107576 \t\tpolicy loss:1.0554812489723673 \t\tavg entropy:1.0160951035754917 \t\tstd entropy:0.608007556328169\n",
            "Episode 188 finished after 200 timesteps - cumulative reward = -127.0894709529183\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:39.61849475880059 \t\tpolicy loss:0.8758062318879731 \t\tavg entropy:0.992781615457313 \t\tstd entropy:0.6172981040120553\n",
            "Epoch: 2 \t\tvalue loss:27.649288260206884 \t\tpolicy loss:0.9291453096939593 \t\tavg entropy:0.9074443389094787 \t\tstd entropy:0.6043798100754489\n",
            "Epoch: 3 \t\tvalue loss:29.452347310221924 \t\tpolicy loss:0.9147787954734297 \t\tavg entropy:0.9616208580363518 \t\tstd entropy:0.5858932437809914\n",
            "Epoch: 4 \t\tvalue loss:32.858717555902444 \t\tpolicy loss:1.0666481086186 \t\tavg entropy:0.9333732630683192 \t\tstd entropy:0.6210672801285378\n",
            "Epoch: 5 \t\tvalue loss:46.09097666156535 \t\tpolicy loss:1.142204002458222 \t\tavg entropy:1.0211270112636264 \t\tstd entropy:0.624989652640117\n",
            "Epoch: 6 \t\tvalue loss:31.73592575228944 \t\tpolicy loss:0.9485656789370945 \t\tavg entropy:1.0051586794648995 \t\tstd entropy:0.6195254720221726\n",
            "Epoch: 7 \t\tvalue loss:25.157483553399846 \t\tpolicy loss:0.8777003841740745 \t\tavg entropy:0.9485172733828386 \t\tstd entropy:0.6033915621214918\n",
            "Epoch: 8 \t\tvalue loss:31.983373483833002 \t\tpolicy loss:0.8517892950651597 \t\tavg entropy:0.9498806541930409 \t\tstd entropy:0.591073299755575\n",
            "Epoch: 9 \t\tvalue loss:32.023328216708435 \t\tpolicy loss:0.6944928068895729 \t\tavg entropy:0.8731971020806414 \t\tstd entropy:0.5792519888355817\n",
            "Epoch: 10 \t\tvalue loss:25.070191475809835 \t\tpolicy loss:0.8740204320270188 \t\tavg entropy:0.8292403008350243 \t\tstd entropy:0.5561429537070856\n",
            "Episode 189 finished after 200 timesteps - cumulative reward = -240.22668021186198\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:25.078896775537608 \t\tpolicy loss:0.8735828217195005 \t\tavg entropy:0.8556516207853441 \t\tstd entropy:0.5589907018248006\n",
            "Epoch: 2 \t\tvalue loss:32.80608095928114 \t\tpolicy loss:0.8818741203570852 \t\tavg entropy:0.8767764387577517 \t\tstd entropy:0.5931265891698975\n",
            "Epoch: 3 \t\tvalue loss:36.22265340843979 \t\tpolicy loss:1.0469688630225706 \t\tavg entropy:0.9286392613791496 \t\tstd entropy:0.5981688987848248\n",
            "Epoch: 4 \t\tvalue loss:25.96406317730339 \t\tpolicy loss:1.0505720154971492 \t\tavg entropy:0.9277994363787522 \t\tstd entropy:0.6162658550738455\n",
            "Epoch: 5 \t\tvalue loss:28.47970963497551 \t\tpolicy loss:0.8360683072586449 \t\tavg entropy:0.9084726594177515 \t\tstd entropy:0.6317551744947716\n",
            "Epoch: 6 \t\tvalue loss:37.64156563914552 \t\tpolicy loss:0.9142276842375191 \t\tavg entropy:0.9555219315916231 \t\tstd entropy:0.6095636812943863\n",
            "Epoch: 7 \t\tvalue loss:25.73369899574591 \t\tpolicy loss:0.8319282851048878 \t\tavg entropy:0.8903808320277642 \t\tstd entropy:0.6020714983636714\n",
            "Epoch: 8 \t\tvalue loss:23.702635570448273 \t\tpolicy loss:0.8824713424760469 \t\tavg entropy:0.8843551078367974 \t\tstd entropy:0.5883520681467379\n",
            "Epoch: 9 \t\tvalue loss:29.96869935308184 \t\tpolicy loss:0.8764998791169147 \t\tavg entropy:0.9094166746320219 \t\tstd entropy:0.5942586802664412\n",
            "Epoch: 10 \t\tvalue loss:32.49083945702533 \t\tpolicy loss:0.8817832327010681 \t\tavg entropy:0.8908290612786757 \t\tstd entropy:0.5926434839514989\n",
            "Episode 190 finished after 200 timesteps - cumulative reward = -255.59238203590027\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:26.184972647464637 \t\tpolicy loss:1.0265003680580793 \t\tavg entropy:0.8906799541111606 \t\tstd entropy:0.5863321502497951\n",
            "Epoch: 2 \t\tvalue loss:26.45346917046441 \t\tpolicy loss:0.8776397118062684 \t\tavg entropy:0.8674290505620551 \t\tstd entropy:0.6092961121132454\n",
            "Epoch: 3 \t\tvalue loss:41.9294553910843 \t\tpolicy loss:0.9842333131366305 \t\tavg entropy:0.9629137890970817 \t\tstd entropy:0.6185668954185809\n",
            "Epoch: 4 \t\tvalue loss:28.184802753756745 \t\tpolicy loss:0.9307670274166145 \t\tavg entropy:0.9102249936539578 \t\tstd entropy:0.6216149022870112\n",
            "Epoch: 5 \t\tvalue loss:21.903945190737947 \t\tpolicy loss:0.8765997531438114 \t\tavg entropy:0.8842885358255392 \t\tstd entropy:0.6171425278703913\n",
            "Epoch: 6 \t\tvalue loss:28.876776950527923 \t\tpolicy loss:0.8424451519744565 \t\tavg entropy:0.9101977566019787 \t\tstd entropy:0.6092696481113257\n",
            "Epoch: 7 \t\tvalue loss:33.937666888188836 \t\tpolicy loss:0.8521365520327983 \t\tavg entropy:0.8762601115027542 \t\tstd entropy:0.5786085629332007\n",
            "Epoch: 8 \t\tvalue loss:25.87669940909954 \t\tpolicy loss:1.024095154169834 \t\tavg entropy:0.8754966814496873 \t\tstd entropy:0.5989502688335212\n",
            "Epoch: 9 \t\tvalue loss:24.89126689024646 \t\tpolicy loss:0.9066631279208444 \t\tavg entropy:0.8757159846412673 \t\tstd entropy:0.6142204388306068\n",
            "Epoch: 10 \t\tvalue loss:43.32693559954865 \t\tpolicy loss:1.0993345169105915 \t\tavg entropy:0.9753368309734961 \t\tstd entropy:0.6300003783026478\n",
            "Episode 191 finished after 200 timesteps - cumulative reward = -120.40228832355238\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:34.214660779394286 \t\tpolicy loss:1.0220334307111876 \t\tavg entropy:0.9613346712570986 \t\tstd entropy:0.6491055518558159\n",
            "Epoch: 2 \t\tvalue loss:23.126578061267583 \t\tpolicy loss:0.9887092285084002 \t\tavg entropy:0.9351635186936542 \t\tstd entropy:0.6427327493393258\n",
            "Epoch: 3 \t\tvalue loss:25.906959430135863 \t\tpolicy loss:0.8896244273643301 \t\tavg entropy:0.9378257938017711 \t\tstd entropy:0.6353079002126598\n",
            "Epoch: 4 \t\tvalue loss:38.68119420427264 \t\tpolicy loss:0.9294326997766591 \t\tavg entropy:0.9599574903470072 \t\tstd entropy:0.6180922050433413\n",
            "Epoch: 5 \t\tvalue loss:29.586349405423558 \t\tpolicy loss:0.9893991619047492 \t\tavg entropy:0.9253016484461699 \t\tstd entropy:0.6253480997675979\n",
            "Epoch: 6 \t\tvalue loss:26.26842484329686 \t\tpolicy loss:0.9362529383765327 \t\tavg entropy:0.9349478115261212 \t\tstd entropy:0.6261335173647516\n",
            "Epoch: 7 \t\tvalue loss:31.7206557302764 \t\tpolicy loss:0.9816094558648388 \t\tavg entropy:0.9237667917107402 \t\tstd entropy:0.6151265734890907\n",
            "Epoch: 8 \t\tvalue loss:59.82978405615296 \t\tpolicy loss:1.3148785823523397 \t\tavg entropy:1.041327813482316 \t\tstd entropy:0.6347981113482054\n",
            "Epoch: 9 \t\tvalue loss:46.65814093869142 \t\tpolicy loss:1.098266475730472 \t\tavg entropy:1.0230978625848608 \t\tstd entropy:0.6847876701639949\n",
            "Epoch: 10 \t\tvalue loss:39.565155255674114 \t\tpolicy loss:1.0088736824315003 \t\tavg entropy:1.0142879076055853 \t\tstd entropy:0.6511616613998339\n",
            "Episode 192 finished after 200 timesteps - cumulative reward = -125.47644400849845\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:43.56674247317844 \t\tpolicy loss:1.3038460383511552 \t\tavg entropy:1.077879144199616 \t\tstd entropy:0.6291315308583186\n",
            "Epoch: 2 \t\tvalue loss:38.968695792284876 \t\tpolicy loss:1.0647401156449559 \t\tavg entropy:1.0705144365384192 \t\tstd entropy:0.6285939576827475\n",
            "Epoch: 3 \t\tvalue loss:45.33702165911896 \t\tpolicy loss:1.2361309871529087 \t\tavg entropy:1.0873815984846777 \t\tstd entropy:0.6174211594850016\n",
            "Epoch: 4 \t\tvalue loss:61.734006221848304 \t\tpolicy loss:1.2920755292430068 \t\tavg entropy:1.1616353930870607 \t\tstd entropy:0.6499113307029117\n",
            "Epoch: 5 \t\tvalue loss:49.050705671310425 \t\tpolicy loss:1.0202264382381632 \t\tavg entropy:1.1528243507760574 \t\tstd entropy:0.6637934128148205\n",
            "Epoch: 6 \t\tvalue loss:33.75160428731128 \t\tpolicy loss:0.9464407271206982 \t\tavg entropy:1.0118571922266912 \t\tstd entropy:0.6580671107811317\n",
            "Epoch: 7 \t\tvalue loss:33.6495194868608 \t\tpolicy loss:0.9835786620775858 \t\tavg entropy:1.0163448397484542 \t\tstd entropy:0.6471495617154249\n",
            "Epoch: 8 \t\tvalue loss:35.077307455467455 \t\tpolicy loss:0.9124838541252445 \t\tavg entropy:1.0116767112930578 \t\tstd entropy:0.6577012981340671\n",
            "Epoch: 9 \t\tvalue loss:29.58600459917627 \t\tpolicy loss:0.8472416587550231 \t\tavg entropy:0.9575566456127125 \t\tstd entropy:0.6526913759156227\n",
            "Epoch: 10 \t\tvalue loss:26.459332514290857 \t\tpolicy loss:0.7635319479788193 \t\tavg entropy:0.9023618445304659 \t\tstd entropy:0.6230057164236548\n",
            "Episode 193 finished after 200 timesteps - cumulative reward = -241.59391597394014\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:29.088130108033766 \t\tpolicy loss:0.870756497888854 \t\tavg entropy:0.9205352998072734 \t\tstd entropy:0.6063333182558508\n",
            "Epoch: 2 \t\tvalue loss:28.598508680709685 \t\tpolicy loss:0.8123054492353189 \t\tavg entropy:0.9026538110920422 \t\tstd entropy:0.6096432249792348\n",
            "Epoch: 3 \t\tvalue loss:24.325698183040426 \t\tpolicy loss:0.7474969514090606 \t\tavg entropy:0.8471758618038975 \t\tstd entropy:0.5937327162885283\n",
            "Epoch: 4 \t\tvalue loss:27.2731456130442 \t\tpolicy loss:0.8106287543219749 \t\tavg entropy:0.8530349758615164 \t\tstd entropy:0.5619324789521172\n",
            "Epoch: 5 \t\tvalue loss:29.58328391566421 \t\tpolicy loss:0.7897457835650203 \t\tavg entropy:0.8508390273278881 \t\tstd entropy:0.5674096399193298\n",
            "Epoch: 6 \t\tvalue loss:24.44267169875328 \t\tpolicy loss:0.7546601181078438 \t\tavg entropy:0.7969950739426074 \t\tstd entropy:0.562922247309798\n",
            "Epoch: 7 \t\tvalue loss:24.732399270992087 \t\tpolicy loss:0.7254298630386892 \t\tavg entropy:0.7885129791589548 \t\tstd entropy:0.5476820646791638\n",
            "Epoch: 8 \t\tvalue loss:27.23715598173816 \t\tpolicy loss:0.7054326865408156 \t\tavg entropy:0.7654495118224102 \t\tstd entropy:0.5553689557072784\n",
            "Epoch: 9 \t\tvalue loss:27.400400267706978 \t\tpolicy loss:0.8242551864999713 \t\tavg entropy:0.7861881497699686 \t\tstd entropy:0.5602461284612615\n",
            "Epoch: 10 \t\tvalue loss:25.55643933710426 \t\tpolicy loss:0.903881660615555 \t\tavg entropy:0.8236588627561802 \t\tstd entropy:0.5899728042738001\n",
            "Episode 194 finished after 200 timesteps - cumulative reward = -302.5626081791073\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:24.81733415102718 \t\tpolicy loss:0.8254657222165002 \t\tavg entropy:0.8186166315877867 \t\tstd entropy:0.5858914505033023\n",
            "Epoch: 2 \t\tvalue loss:37.41215154859755 \t\tpolicy loss:1.1078039636515609 \t\tavg entropy:0.9102803648766478 \t\tstd entropy:0.5996551355749093\n",
            "Epoch: 3 \t\tvalue loss:30.248862179842863 \t\tpolicy loss:1.036357373902292 \t\tavg entropy:0.917401942556731 \t\tstd entropy:0.6457626108388569\n",
            "Epoch: 4 \t\tvalue loss:22.041809737080275 \t\tpolicy loss:0.8861055419300542 \t\tavg entropy:0.9339519606614944 \t\tstd entropy:0.6391740681659244\n",
            "Epoch: 5 \t\tvalue loss:28.161233723765672 \t\tpolicy loss:0.9053838915295072 \t\tavg entropy:0.9175323254158744 \t\tstd entropy:0.62182582423788\n",
            "Epoch: 6 \t\tvalue loss:42.23721265792847 \t\tpolicy loss:1.1124022247815373 \t\tavg entropy:0.9559427200030592 \t\tstd entropy:0.6439133532372149\n",
            "Epoch: 7 \t\tvalue loss:26.02556091848046 \t\tpolicy loss:0.9479612870649858 \t\tavg entropy:0.9709455873826512 \t\tstd entropy:0.6446566293611229\n",
            "Epoch: 8 \t\tvalue loss:22.97598123550415 \t\tpolicy loss:0.8204074405660533 \t\tavg entropy:0.9434118328077173 \t\tstd entropy:0.6316117905067165\n",
            "Epoch: 9 \t\tvalue loss:28.2703071555706 \t\tpolicy loss:0.7558380681456942 \t\tavg entropy:0.8639893619694672 \t\tstd entropy:0.6056415965805764\n",
            "Epoch: 10 \t\tvalue loss:35.07629984075373 \t\tpolicy loss:0.9008793231814799 \t\tavg entropy:0.8606436459036595 \t\tstd entropy:0.5907291036608444\n",
            "Episode 195 finished after 200 timesteps - cumulative reward = -123.65076202295825\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:29.80749493598938 \t\tpolicy loss:1.1309591656923295 \t\tavg entropy:0.9043309467538285 \t\tstd entropy:0.6217583806922112\n",
            "Epoch: 2 \t\tvalue loss:25.147695121765135 \t\tpolicy loss:0.9512854263186454 \t\tavg entropy:0.9665302605454581 \t\tstd entropy:0.640220565263284\n",
            "Epoch: 3 \t\tvalue loss:33.429545965194706 \t\tpolicy loss:0.9942211872339248 \t\tavg entropy:0.958442303913675 \t\tstd entropy:0.6052832104635949\n",
            "Epoch: 4 \t\tvalue loss:57.98948728561401 \t\tpolicy loss:1.5047116059064864 \t\tavg entropy:1.0377972675491636 \t\tstd entropy:0.6391295926447639\n",
            "Epoch: 5 \t\tvalue loss:34.245869691371915 \t\tpolicy loss:1.1116697829961777 \t\tavg entropy:1.0972950947345477 \t\tstd entropy:0.6630408070274646\n",
            "Epoch: 6 \t\tvalue loss:26.7664390039444 \t\tpolicy loss:0.9419334679841995 \t\tavg entropy:1.02490363815398 \t\tstd entropy:0.635358252685336\n",
            "Epoch: 7 \t\tvalue loss:29.7507954454422 \t\tpolicy loss:0.8426603588461876 \t\tavg entropy:0.9539016027368592 \t\tstd entropy:0.6308467371567932\n",
            "Epoch: 8 \t\tvalue loss:33.431421306133274 \t\tpolicy loss:0.8604447847604751 \t\tavg entropy:0.9127821760410558 \t\tstd entropy:0.6279673154593303\n",
            "Epoch: 9 \t\tvalue loss:27.491793236732484 \t\tpolicy loss:0.8361497321724891 \t\tavg entropy:0.8771565362152544 \t\tstd entropy:0.6372625867025209\n",
            "Epoch: 10 \t\tvalue loss:26.176459591388703 \t\tpolicy loss:0.9791591918468475 \t\tavg entropy:0.9207151555094817 \t\tstd entropy:0.6155954835860779\n",
            "Episode 196 finished after 200 timesteps - cumulative reward = -124.50905523196106\n",
            "Agent test30 learning\n",
            "Epoch: 1 \t\tvalue loss:29.52609724521637 \t\tpolicy loss:0.8172914630174637 \t\tavg entropy:0.9141444480431972 \t\tstd entropy:0.601323749220897\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}